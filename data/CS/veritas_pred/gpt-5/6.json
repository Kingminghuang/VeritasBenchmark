{
  "question": "\nGiven the two primary paradigms of AI in academic writing\u2014semi-automatic assistance and full automation\u2014how do the specific collaborative functionalities across the three phases of semi-automatic writing (preparation, writing, and revision) fundamentally contrast with the self-refining, multi-agent methodologies of full-automatic systems in terms of their reliance on human oversight, their current limitations in areas like citation and contextual accuracy, and their ultimate implications for the evolving habits and integrity of scholarly manuscript creation?\n",
  "research_plan": "<research_outline>\n(1) Clarify scope and research questions\n(a) Define paradigms and boundaries\n    - Semi-automatic academic writing: human-in-the-loop collaboration across preparation, writing, and revision phases; humans make acceptance/rejection decisions and verify sources.\n    - Full-automatic systems: end-to-end generation with no human interventions during generation/revision; a human may evaluate only post hoc. Often multi-agent (planner/retriever/drafter/critic/controller) though single-model pipelines exist.\n(b) Operationalize oversight and provenance\n    - Oversight density metrics: human interventions per 1,000 words; percent AI text accepted; percent claims with human-verified sources; time-on-task.\n    - Provenance requirements: per-claim source links, retrieval snapshots with timestamps, tool/model/version logs.\n(c) Core comparative questions\n    - How do semi-automatic phase-specific collaborative functions contrast with full-automatic self-refining loops?\n    - How does reliance on human oversight differ (checkpoints, auditability, accountability)?\n    - What are current limitations in citation integrity and contextual accuracy for each paradigm?\n    - What are the implications for scholarly habits, authorship norms, and research integrity?\n\n(2) Semi-automatic paradigm: functionalities by phase and collaboration patterns\n(a) Preparation phase (human steers, AI assists)\n    - Literature discovery: query expansion, concept graphs, related work surfacing.\n    - Prompt/outlining: query reformulation, outline ideation, scope delimitation.\n    - Evidence triage: tagging relevance, extracting key claims, building annotated notes.\n    - Citation harvesting: preliminary bibliography, de-duplication, metadata normalization.\n    - Risks/controls: confirmation bias; missing counterevidence; embedding noise; institute human verification checkpoints and source linking.\n(b) Writing/drafting phase (co-authoring with guardrails)\n    - Section-level drafting from human-approved outlines; paraphrase/translation; style adherence.\n    - Method/results assistance from structured inputs (code, tables, figures) and non-textual evidence handling (equations, graphs).\n    - Inline citation suggestions and placeholders tied to retrieved evidence with page/section anchors.\n    - Risks/controls: subtle content drift; invented or mismatched citations; require in-text grounding to sources and human acceptance gates.\n(c) Revision phase (editorial and factual refinement)\n    - Grammar/style/clarity passes; structural coherence improvements.\n    - Fact-check prompts; terminology, units, and equation consistency; statistical sanity checks (effect sizes, multiple comparisons, units/dimensions).\n    - Reference formatting; identifier (DOI/URL/PMID/arXiv) validation; figure/table caption refinement.\n    - Reviewer emulation for critique and rebuttal drafting support.\n    - Risks/controls: over-smoothing that alters meaning; false positives in checks; require source-linked justifications and change tracking.\n\n(3) Full-automatic systems: self-refining methodologies and agent roles\n(a) Architectural components and loops\n    - Planner: topic selection, outline, section goals.\n    - Retriever/citation agent: source gathering, ranking, grounding artifacts with page/section anchors.\n    - Drafter: section composition with template adherence.\n    - Critics/verifiers: logical, factual, and citation checks; iterative critique\u2013rewrite cycles.\n    - Controller: manages iterations, stopping criteria, acceptance thresholds, and failure handling (timeouts, degraded modes, rate-limit retries).\n(b) Self-refinement mechanics and auditability\n    - Multi-round plan\u2013write\u2013critique\u2013revise cycles; uncertainty signaling; detection/correction of propagation errors.\n    - Tool-augmented verification: identifier resolvability, quote overlap, passage-level grounding checks.\n    - Full agent trace logging: planner/retriever/drafter/critic tool calls with timestamps; retrieval snapshots hashed for reproducibility (with decoding configs and seeds where applicable).\n(c) Typical failure modes without human oversight\n    - Compounded citation errors (fabricated references, misattribution, outdated/mismatched sources; status/version errors such as preprint vs published).\n    - Contextual misinterpretation of domain nuances, statistics, or methods; over-optimization for fluency over truth.\n    - Weak provenance and calibration; vulnerability to prompt-injection/poisoned pages; OCR/parsing/anchor fidelity issues.\n\n(4) Comparative framework: reliance on human oversight and control surfaces\n(a) Semi-automatic\n    - Dense human checkpoints at all phases; granular acceptance/rejection of AI suggestions.\n    - Provenance maintained via human-curated notes, source links, bibliographic tooling, and tracked edits.\n    - Clear accountability remains with authors; AI is assistive apparatus.\n(b) Full-automatic\n    - Sparse or optional human touchpoints (e.g., pre-run outline gate, post-run review only).\n    - Provenance depends on retrieval quality and enforced validation/audit policies; requires immutable logs.\n    - Accountability diffusion risk; needs explicit oversight protocols, sign-offs, and disclosure.\n\n(5) Evaluation criteria and metrics for limitations (citation and contextual accuracy)\n(a) Citation integrity\n    - Metrics: citation precision; identifier/URL resolvability rate; in-text\u2013reference consistency; fabricated/misattributed reference rate.\n    - Status/version checks: retractions/errata flags (e.g., Crossref/Retraction Watch), venue quality filters, preprint\u2013published mapping, DOI/PMID/arXiv crosswalk accuracy, staleness/recency.\n    - Source policy: prioritize primary sources for key claims; penalize citation-by-proxy.\n    - Parsing/anchor fidelity: page/line-level quote overlap; PDF/OCR reliability tests; table/figure/equation anchoring quality.\n(b) Contextual faithfulness and factuality\n    - Metrics: expert-judged claim correctness; method appropriateness; contradiction detection; domain-specific factual QA.\n    - Statistical/method rigor checks: effect sizes, multiple comparisons controls, unit/dimension consistency.\n    - Cross-lingual retrieval/citation correctness for non-English sources.\n(c) Provenance, transparency, and security\n    - Completeness of evidence trails; hashed retrieval snapshots; confidence/uncertainty annotations; reproducibility of retrieval.\n    - RAG security: allowlists, content hashing/sandboxing, prompt-injection defenses.\n(d) Writing quality (secondary to truthfulness)\n    - Readability, coherence, and meaning-preserving edits vs. semantic drift.\n\n(6) Methodology to study contrasts empirically\n(a) Corpus and task design\n    - Balanced topics across disciplines (methods-heavy, math-heavy, empirical, qualitative; multilingual coverage where feasible).\n    - Standardized prompts/outlines; matched length and citation targets.\n(b) Experimental conditions and baselines\n    - Semi-auto: instrument human edits, decision points, oversight density, and time-on-task.\n    - Full-auto: fix agent configuration, iteration limits, retrieval settings; log critique/rewrite cycles end-to-end.\n    - Baselines: human-only and simple RAG baseline systems.\n(c) Measurement protocols and human factors\n    - Pre-registered metrics; blinded expert review for factuality and citation correctness; rater calibration with IRR thresholds (e.g., Cohen\u2019s kappa/Krippendorff\u2019s alpha) and adjudication.\n    - Cognitive load and attention: NASA-TLX; deep reading vs skimming measures; screen-capture for provenance of decisions (where ethical).\n(d) Statistical analysis and power\n    - Power analysis; multiple-comparisons corrections (e.g., BH/FDR); mixed-effects models to analyze oversight density and topic difficulty interactions.\n(e) Threats to validity\n    - Generalizability across disciplines/languages; user expertise interaction effects; version drift of tools/indexes; environmental/cost constraints; benchmark leakage/blinding controls.\n\n(7) Integrity, authorship, and evolving scholarly habits\n(a) Authorship and disclosure\n    - Contribution taxonomy (CRediT); disclosure of AI roles and checkpoints; alignment with COPE/ICMJE and journal policies.\n(b) Habit formation and skill dynamics\n    - Semi-auto: supports skill retention if verification remains central.\n    - Full-auto: risk of overreliance and reduced deep reading/methodological rigor; introduce scaffolds to keep humans in the evaluative loop.\n(c) Plagiarism, paper mills, and text reuse\n    - Patchwriting risks; similarity checks; citation manipulation (stacking, self-citation) monitoring; ghostwriting/paper-mill detection heuristics.\n(d) Equity and access\n    - Tool availability disparities; multilingual and discipline coverage gaps; bias amplification; paywall access concerns.\n(e) Governance and compliance\n    - Verification checklists; provenance appendices; privacy/copyright constraints for sensitive domains; auditor roles and escalation paths.\n\n(8) Case studies and failure analyses\n(a) Side-by-side manuscripts on identical topics under both paradigms; annotate citation/context errors and provenance quality.\n(b) Domain-specific edge cases: complex statistics, specialized nomenclature, ambiguous or evolving literature; cross-lingual topics.\n(c) Adversarial scenarios: contradictory sources, noisy PDFs/OCR, injected false claims; test controller failure-handling policies.\n(d) High-stakes workflows: systematic reviews/meta-analyses (e.g., PRISMA/CONSORT) with domain expert adjudication.\n(e) Retracted keystone-source test cases and post-mortems of flawed/retracted AI-assisted drafts.\n\n(9) Synthesis and recommendations\n(a) Prefer semi-automatic workflows for high-stakes content; mandate human gates if using full-automatic systems.\n(b) Design principles for safer automation: strict grounding with page/section anchors; identifier resolvability checks; uncertainty reporting; immutable audit logs; hashed retrieval snapshots; full agent trace logging.\n(c) Best-practice checklists per phase (prep, writing, revision) emphasizing claim\u2013evidence alignment and citation verification; author attestation/disclosure templates.\n(d) Open research gaps: robust citation agents; domain-aware verification; provenance UX; metrics for meaning-preserving edits; cross-lingual verification; RAG security benchmarks.\n\n(10) Deliverables and timeline\n(a) Datasets and benchmarks: gold-standard claim\u2013evidence pairs with page anchors, retraction/status metadata, and severity labels; multilingual subsets.\n(b) Tooling and artifacts: benchmark tasks and evaluation scripts; containerized pipelines; retrieval snapshots with hashes; model/run cards.\n(c) Reports and policy kits: comparative report with metrics, error typologies, case studies, and recommendations; reviewer/editor kits; oversight-density rubric.\n(d) Cost and sustainability: compute/wall-clock/energy tracking; cost per correct citation/claim analyses; equity/access considerations.\n</research_outline>",
  "references": []
}