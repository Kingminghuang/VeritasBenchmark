# Calibrated Recommendations: Survey and Future Directions

DIEGO CORRE A DA SILVA, University of Klagenfurt, Austria  DIETMAR JANNACH, University of Klagenfurt, Austria

The idea of calibrated recommendations is that the properties of the items that are suggested to users should match the distribution of their individual past preferences. Calibration techniques are therefore helpful to ensure that the recommendations provided to a user are not limited to a certain subset of the user's interests. Over the past few years, we have observed an increasing number of research works that use calibration for different purposes, including questions of diversity, biases, and fairness. In this work, we provide a survey on the recent developments in the area of calibrated recommendations. We both review existing technical approaches for calibration and provide an overview on empirical and analytical studies on the effectiveness of calibration for different use cases. Furthermore, we discuss limitations and common challenges when implementing calibration in practice.

CCS Concepts:  $\cdot$  Information systems  $\rightarrow$  Recommender systems;  $\cdot$  Computing methodologies  $\rightarrow$  Machine learning.

Additional Key Words and Phrases: Recommender Systems, Calibration, Survey

# ACM Reference Format:

Diego Correa da Silva and Dietmar Jannach. 2025. Calibrated Recommendations: Survey and Future Directions. ACM Trans. Recomm. Syst. 1, 1, Article 1 (January 2025), 27 pages. https://doi.org/10.1145/nnnnnnnn.nnnnnnn

# 1 Introduction

Recommender systems are nowadays widely used on various online platforms to help users discover relevant content. Commonly, such systems make personalized recommendations based both on an individual user's past observed interests and based on preference patterns from a larger user community [25, 52]. Historically, most published research on recommender systems from the last decades focuses on designing machine learning models that accurately place the assumedly most relevant items at the top of the recommendation list. However, already many years ago it was observed that "being accurate is not enough" [43], and that focusing solely on prediction or ranking accuracy may be actually detrimental for the user experience of a recommender system, for example because the resulting recommendations may be boring and lacking diversity or novelty [28].

In this broader context of such "beyond- accuracy" [21] considerations, Steck [60] coined the term "calibrated recommendations" in a paper from 2018. He used the following intuitive example to illustrate this idea: "When a user has watched, say, 70 romance movies and 30 action movies, then it is reasonable to expect the personalized list of recommended movies to be comprised of about  $70\%$  romance and  $30\%$  action movies as well." The general idea of calibration in this example in the domain of movie recommendations is thus to match the distribution of movie genres in the set of recommendations with the distribution of the genres in the individual user's past preference profile. As a result, if a user had diverse preferences in the past, this diversity will be reflected in

the recommendations after calibration, thereby avoiding monothematic recommendations. Notably, calibration is different from many previous approaches to increasing recommendation diversity like [8, 72] in that the diversification process is personalized to the individual user.

Technically, calibration approaches are commonly implemented as reranking techniques. This means that the outputs of any underlying recommendation algorithm are post- processed to ensure that the distributions of the item properties in the recommendation list and the user profile are aligned. The level of alignment of the distributions can be gauged with the help of measures from information theory like Kullback- Leibler divergence [15]. Besides item features like movie genres, other properties of the items can be the target of the calibration process, for example an item's general popularity, which often is a relevant factor in the context of fairness considerations of recommender systems [3, 4].

However, while the idea of calibration as described by Steck is intuitive and useful for many use cases, it may also face challenges and limitations in certain situations. In real- world settings, for example, not all past preferences may be equally important, and older preferences may no longer be relevant and should thus not be considered for calibration. Also, at the level of individual users it may also happen that calibration actually reduces diversity in case a user had a monothematic interest profile in the past. But there are also challenges from a practical perspective. Calibration by design creates a trade- off situation, where recommendation accuracy has to be balanced with a calibration target. Finding the right balance between these competing objectives can be challenging in practice. Also, when calibration is used to match a user's preferences in terms of item popularity it is a non- trivial process to determine appropriate thresholds values that are used to discriminate between different levels of popularity [32].

Given these challenges, a variety of calibration- based approaches have been proposed over the past few years, and the impact of calibrated recommendations has been studied in a growing number of research works. With this present work, our goal is to provide a structured survey of these developments in an area of high practical relevance. To that purpose, we have systematically scanned the literature and identified 53 relevant papers, which we discuss and categorize in this work. The paper is organized as follows. Next, in Section 2, we elaborate on our research methodology. We then provide an overview existing technical approaches for calibration in Section 3. Afterwards, in Section 4, we focus on the evaluation of calibration approaches and we review studies which have analyzed the effects of recommendations in offline and online settings. The paper continues with a discussion of existing solutions and open challenges in Section 5 and ends with an outlook on future developments.

# 2 Research Methodology and General Statistics

Identification of Relevant Papers. We followed a systematic approach [30] to identify relevant research works. Specifically, we queried two bibliographic databases, namely the ACM Digital Library (DL) and Google Scholar with a pre- defined set of keywords. The ACM DL was chosen because many important publication venues for recommender systems research are related to ACM, e.g., the SIGIR, WWW, or RecSys conference series. Google Scholar was used to complement the search because its index is very comprehensive. The following query was used to identify papers that had relevant terms in the full text.

("calibrated recommendations") OR ("calibrated recommenders") OR ("calibrated recommendation") OR ("calibrated recommendation system") OR ("calibrated recommender system") OR ("calibrated recommender") OR ("calibrated recommendation systems") OR ("calibrated recommender systems")

We limited the search to papers appearing since 2018, i.e., the year when Steck's paper was published. The search, when executed in May 2025, yielded 57 papers in the ACM DL and 356 papers from Google Scholar. In terms of inclusion criteria, we only retained works that build on Steck's notion of calibration, e.g., by proposing a new calibration method or by evaluating and analyzing the impact of calibrated recommendations. Correspondingly, we excluded works that only discuss calibration as related work or cite such works somewhere in the text, i.e., works without a specific contribution to calibration research. Furthermore, we did not consider preprints, e.g., from arxiv.org or ssrn.com, and Master's and PhD theses. Papers that were not written in English were also excluded. Papers published in conferences and later expanded in journals were considered as one paper.

Applying these inclusion and exclusion criteria left us with 51 research papers. In addition, going through the references of these papers, we found two earlier works that propose ideas that are very similar to Steck's calibration approach, but which were not using the term calibration. These works were published in 2011 and 2017 by Oh et al. [48] and by Jugovac et al. [27], where the latter work is based on the first one. Oh et al. propose a method called Personal Popularity Tendency Matching (PPTM), where the goal is to increase the novelty of the recommendations by aligning the popularity of the recommended items with the user's past popularity preferences. Like Steck, they propose a reranking procedure for this alignment. Differently from Steck, Oh et al. use the Earth Mover's Distance as a measure for miscalibration. Jugovac et al. then later propose an extension to this work in the context of music recommendations, in which multiple optimization goals can be considered in parallel. Adding the approaches presented in [27] and [48] left us with 53 papers that were finally considered in our analyses.

We note that calibration techniques are also related to early intent- aware recommender systems (IARS). IARS are designed to take time- varying user intents into account when making recommendations [24]. Early approaches in this area [63, 64] implemented intent- awareness (or: "intent- orientation") by considering the past interest of users in certain item features, which could for example be movie genres. To cover most of the possible user intents or interests, these approaches then work by diversifying the recommendations by ensuring that the recommendations cover many of the past interests, e.g., movie genres. Technically, this is commonly implemented by a re- ranking procedure that considers both the relevance of the items and a diversity component. A deeper discussion and analysis of intent- aware and calibrated recommendations can be found in [29]. While some intent- aware approaches are technically related to calibration, we did not include intent- aware models in our survey as their main target is typically to being able to cover short- term user interests.

Statistics of Collected Papers. We retrieved the metadata of the 53 identified research works to analyze the number of publications per year, their publication venues, and the nature of their contributions. As shown in Figure 1, the number of papers in this area has consistently increased over the past eight years, with the most significant growth occurring in 2024. This trend indicates that the field is experiencing strong annual growth.

Figure 2a illustrates the distribution of publications in proceedings and journals. As we can see, more than  $80\%$  of the papers were published in conference or workshop proceedings and less than  $20\%$  in journals. Most of publications are presented at major conferences, with a notable concentration at the ACM Conference on Recommender Systems (RecSys)<sup>2</sup> with 12 publications. Other

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/d59e73c77ff96d61f9b6fb384e6e80b0e5aacc0a5d52d9e8194a5da4bb424250.jpg)  
Fig. 1. Number of publications per year on calibrated recommendations.

prominent venues include ACM UMAP $^3$  and the ACM Web Conference $^4$  conferences with five and four publications respectively. The journal with the most relevant publications is ACM Transactions on Intelligent Systems and Technology (TIST) $^5$ . Three papers appeared in the proceedings of a workshop on Advances in Bias and Fairness in Information Retrieval (BIAS) $^6$ . The venues with two publications on calibration are Expert Systems with Applications (ESWA) $^7$ , the ACM International Conference on Information & Knowledge Management (CIKM) $^8$ , Applied Intelligence (APIN) $^9$  and the International Conference on Enterprise Information Systems (ICEIS) $^{10}$ . The "Others" category in Figure 2b represents all venues with only one paper published on calibrated recommendations.

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/7f0a616bd45acf5cee9eb192d9d320e8a6862065acadf76df8b552027f4f31ad.jpg)  
Fig. 2. Publication venues.

Among the 53 research papers, we identified recurring patterns that allowed us to classify them into three distinct categories (Figure 3): technical proposals, impact analyses, and comparison works. The focus of each category is detailed below, and all relevant studies will be discussed in the following sections.

- Technical Proposal: Papers in this category introduce new technical contributions to the field, including modifications or improvements to existing calibration methods. The majority of the analyzed papers fall into this category, highlighting the research community's focus on expanding and refining calibration techniques.- Impact Analysis: These studies examine specific aspects of calibrated recommendations, such as end user perceptions, algorithm behavior and biases (by popularity, stereotypes, human gender, age, and country), or system performance, without proposing new calibration methods. This category represents the second-largest share of publications, indicating a strong interest in understanding how calibration or its absence impact the system's recommendations and the user experience.- Comparison Works: This category includes studies that compare different calibrated recommendation methods, evaluating their strengths and weaknesses. These comparisons often assess calibration effectiveness with and without additional post-processing techniques. It is the least common type of publication, suggesting that while new calibration methods are frequently introduced, fewer studies focus on systematically comparing their effects.

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/5a32626f82fcfbd1e0d81141723eb790f5f7e6550ac395084471a3655e4e839b.jpg)  
- Technical Proposal  $\cdot$  Comparison Works  $\cdot$  Impact Analysis  Fig. 3. Proportion of papers classified based on the type of contribution.

# 3 Technical Approaches to Calibration

In this section, we outline the general architecture of most calibration approaches, using Steck's proposal as a foundational reference. We then also summarize the various implementations of individual components within this architecture, relating to works with proposals for the component. This technical section formally develops the calibrated recommendation framework, with Table 1 summarizing the notation used throughout.

# 3.1 A General Architecture for Calibration Approaches

Calibration is commonly implemented as a post- processing approach that operates independently of an underlying base recommender algorithm. Given a ranked list of candidate items, the calibration

Table 1. Formal notation used in the paper.  

<table><tr><td>Symbol</td><td>Description</td></tr><tr><td>u∈U</td><td>u represents a unique user; U is the set of all users.</td></tr><tr><td>i∈I</td><td>i represents a unique item; I is the set of all items.</td></tr><tr><td>g∈G</td><td>g represents a unique genre; G is the set of all genres.</td></tr><tr><td>Hu</td><td>The preference profile of u derived from the user&#x27;s historical interactions with items.</td></tr><tr><td>Lu</td><td>A list for recommendations for u.</td></tr><tr><td>L*</td><td>A calibrated list of recommendation for u.</td></tr><tr><td>N</td><td>Size of the list of calibrated recommendations.</td></tr><tr><td>n</td><td>Temporary calibrated recommendation list size.</td></tr><tr><td>wu,i</td><td>The relevance value (e.g., a rating) given by u to item i.</td></tr><tr><td>^wu,i</td><td>The predicted relevance value by the recommendation algorithm.</td></tr><tr><td>λ</td><td>The trade-off weight value.</td></tr><tr><td>CL(·,·)</td><td>The calibration function used to measure the distance between two distributions.</td></tr><tr><td>REL(·)</td><td>A function that returns the relevance of a given list of items.</td></tr><tr><td>P(·)</td><td>A function to derive the target distribution from the user profile.</td></tr><tr><td>Q(·)</td><td>A function to derive a realized distribution from a recommendation list.</td></tr><tr><td>Pu</td><td>The target distribution for user u.</td></tr><tr><td>Qu</td><td>The realized distribution for user u.</td></tr><tr><td>p(g|u)</td><td>The distribution value of genre g in u&#x27;s profile.</td></tr><tr><td>q(g|u)</td><td>Represents the distribution value of genre g in u&#x27;s recommendation list.</td></tr></table>

process consists of re- ranking the list to achieve a given calibration goal. Most studies identified in our survey follow Steck's framework, which can be decomposed into several key steps for which different choices can be made [13]: Baseline Ranking, Candidate Selection, Measuring Aggregate Relevance, Determining Target and Realized Distributions, Choosing the Calibration Measure, Trade- off Balance Modeling, and Optimized Item Selection.

3.1.1 Baseline Ranking. In the Baseline Ranking step, any algorithm can be applied to determine a personalized item ranking for a given user u. Commonly, collaborative filtering techniques, e.g., based on matrix factorization, are applied that solely work on a given user- item interaction matrix. However, content- based or hybrid techniques can also be used, as long as these techniques provide a personalized ranking using some sort of relevance score. Notably, different baseline ranking techniques may lead to different levels of initial miscalibration of the recommendations, e.g., in case when one baseline techniques generates more diverse recommendations 'by design' than another one.

3.1.2 Candidate Selection. The next step in the process is Candidate Selection, i.e., the selection of items that should be considered in the subsequent calibration step. In all studied papers, this process simply consists of considering the first  $k$  elements recommended by the baseline ranker for each user. The most common number of items in the candidate set is 100. However, there are also works that use smaller candidate item sets (e.g., 10 or 20 in [27]) or larger ones (e.g., 1 000 in [23, 56]). Intuitively, when using a smaller candidate set from a relevance- ordered list, the potential compromise on relevance can be limited. At the same time, however, the options for calibrating the list are reduced as well.

3.1.3 Measuring Aggregate Relevance. The core of calibration is to balance the relevance of a set of recommended items and its alignment with past preference patterns. In a next step, we therefore

need to quantify the relevance of a given set of recommendations  $L_{u}$  through a relevance function  $\mathrm{REL}(L_u)$ . A basic and most frequently implemented approach is to simply rely on the relevance scores computed by the baseline ranker and use the sum of these scores as an aggregate relevance measure, more formally:

$$
\mathrm{REL}(L_u) = \sum_{i\in L_u}\hat{w}_{u,i}. \tag{1}
$$

In the equation, the relevance score for a given user  $u$  and an item  $i$  is denoted as  $\hat{w}_{u,i}$ . Depending on the use case, the relevance scores can be predicted user ratings, e.g., on a scale from 1- 5, or other forms of relevance predictions, e.g., estimated probabilities that a user will do a certain action on an item.

Steck argues that alternative metrics such as Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) can be used to assess recommendation relevance. While most studies rely on Steck's original proposal and implement Equation 1, some researchers have explored modifications to enhance the system. Sacharidis et al. [53], for example, introduce an alternative utility function inspired by the NDCG, aiming to improve ranking quality during the calibration step. da Silva and Durão [15] conducted a benchmark study to systematically compare the widely adopted summation- based approach with the NDCG, and their results indicate that both methods exhibit similar performance.

An alternative approach by Zhao et al. [69] shifts the focus to recommending users for items. In this work, the authors modify Steck's relevance function to better align with the task of recommending the most suitable users for each item. The relevance value for an item- user pair is not computed directly as in Steck's proposal, but rather inferred indirectly from the user's ranking position for the item. Expanding the previous idea, Zhao et al. [70] introduce a new relevance function in which an item's relevance decreases based on its position in the recommendation list. In this follow- up study, they propose TecRec, a method that incorporates calibration into a sequential recommendation setting. Further refinements have been made in the context of sequential recommendation also in other works. For example, Chen et al. [10, 11] incorporate the cross- entropy loss function as a relevance measure, although without detailed justification. Finally, in the context of top- n recommendation problems, Abdollahpouri et al. [5] formulate the calibrated recommendation task as a graph model, where the relevance scores are the edge weights. These relevance scores are incorporated into a maximum weight assignment problem, where the objective is to select a set of items for a recommendation list that maximizes the total relevance.

3.1.4 Determining Target and Realized Distributions. We recall that the goal of calibration is to ensure that the made recommendations match the user's past preference patterns in terms of their distribution of certain item features. As such, the item features of interest must be defined in a first step. Various calibration approaches are evaluated in the movie domain, and the movie genre is thus used as a calibration target. Other works consider item categories instead [47]. Furthermore, "meta- level" item attributes, such as their popularity were considered as well [4, 32, 33, 37, 38, 62]. Typically, exactly one item feature is used for calibration. An exception is the work by Jugovac et al. [27], who present an approach in which user preferences along multiple dimensions can be considered in parallel.

To match the distributions of the user preferences, e.g., in terms of the item genre, and the recommendations, a procedure has to be defined how these distributions are derived. In the case of movie genres, which we use for illustration purposes here, we note that a movie can belong to multiple genres. In such cases of multi- valued item features, the proportion of a movie belonging to a certain genre is usually computed first. The most commonly implemented approach is to simply

divide 1 by the number of genres that are associated with a given item. For instance, the proportion of a specific genre  $g$  of an item  $i$  with four genres is calculated as  $\frac{1}{4} = 0.25$ . In this approach, each genre assigned to an item has an equal weight. Let us denote the set of genres assigned to an item  $i$  with  $g(i)$ , and  $|g(i)|$  the size of this set. We can then define the proportion of a genre for a given item as follows:

$$
prop(g|i) = \frac{1}{|g(i)|}. \tag{2}
$$

Target Distribution. The Target Distribution, denoted as  $P(H_{u})$  following [60], represents the ideal distribution of item attributes (e.g., genres) based on the user's past interactions  $H_{u}$ . It captures the proportions in which different genres should appear in the recommendation list to best match the user's preferences. To compute  $P(H_{u})$ , we determine for each of the existing genres in the dataset, denoted as  $G$ , a value  $p(g|u)$  that represents the "importance" of this genre for this user. Specifically,  $p(g|u)$  is computed by considering all items in the user's past preference profile, and it both takes into account the genre proportion  $prop(g|i)$  of each item and the user's preference towards this item, e.g., the user's explicit item rating. Overall, the derivation of the target distribution can be formally described as follows<sup>11</sup>:

$$
\mathrm{P}(H_u) = (\forall g\in G)p(g|u) = \frac{\sum_{i}^{H_u}\mathbb{1}(g\in g(i))w_{u,i}\cdot prop(g|i)}{\sum_{i}^{H_u}\mathbb{1}(g\in g(i))w_{u,i}}. \tag{3}
$$

Realized Distribution. The Realized Distribution, denoted as  $Q(L_{u})$ , represents the actual distribution of item attributes in the generated recommendation list  $L_{u}$  for a user  $u$ . In our example, it is again computed based on the genres of the recommended items. The subsequent goal of the calibration technique is to narrow the gap between the realized and the target distributions, e.g., by re- ranking the recommendation list. Technically, the computation is identical to the one for the target distribution in Equation 3, with the difference that we are now considering the user- specific importance of each genre of an item in the recommendation list  $q(g|u)$  using the user's predicted rating  $\hat{w}_{u,i}$  for each item:

$$
\mathsf{Q}(L_u) = (\forall g\in G)q(g|u) = \frac{\sum_{i}^{L_u}\mathbb{1}(g\in g(i))\hat{w}_{u,i}\cdot prop(g|i)}{\sum_{i}^{L_u}\mathbb{1}(g\in g(i))\hat{w}_{u,i}}. \tag{4}
$$

We note that using this approach to compute  $Q(L_{u})$  may result in a tricky situation during calibration in cases where a certain genre is very infrequent in the user profile, and thus the target distribution. In such cases, the genre might not be considered at all during the calibration process, because adding an item of this genre might immediately lead to an increase in the level of miscalibration, which is caused by overrepresenting this genre in the realized distribution. To mitigate this issue, Steck proposes a smoothing re- balancing approach  $\tilde{Q} (L_{u})$  that ensures that no genre has a value of zero in the realized distribution.

$$
\tilde{Q} (L_u) = (\forall g\in G)\tilde{q} (g|u) = (1 - \alpha)\cdot q(g|u) + \alpha \cdot p(g|u). \tag{5}
$$

A commonly used value for  $\alpha$  is 0.01, which prevents extreme sparsity in the distribution while maintaining the overall proportional structure.

Alternatives to Modeling User Distributions. The core concept of calibration relies on the derivation of distributions, making the design of an effective distribution a critical research focus. Thus, several works propose alternative distribution formulations. Jeon et al. [26] introduce an additional hyperparameter weighting factor to refine the distribution derivation process for sequential recommendation contexts. Specifically, the weighting factor can be used to give more emphasis to more recent user interactions. Jiayi et al. [11] simplify Steck's approach by deriving distributions based on item popularity, distinguishing between head and long- tail items. Chen et al. [10] integrate the distribution derivation process directly into the recommender algorithm, applying a softmax function to the target distribution. Differently from these personalized approaches, Abdollahpouri et al. [1] propose to use a system- wide target distribution determined by domain experts, which is combined with the target distribution to balance both system and user interests. Zhao et al. [70] apply Steck's method but incorporate a Long Short- Term Memory (LSTM) module to predict the future user preference distribution based on the evolution of the historical distribution. Finally, da Silva and Durão [15] demonstrate that a simple normalization step, ensuring a properly normalized distribution, consistently improves both precision and calibration compared to Steck's proposal. Later on, da Silva et al. [16] incorporate entropy and temporal factors into the distribution derivation process, finding that entropy- based distributions can outperform even the normalized version.

3.1.5 Choosing the Calibration Measure. Given the goal of creating a recommendation list that leads to a realized distribution  $Q$  that is close to the target distribution  $P$ , i.e.,  $Q \approx P$ , a measure is needed to quantify the discrepancy between  $P$  and  $Q$ . Various measures exist in the literature to quantify the distance between distributions [9]. In his proposal, Steck relied on the Kullback- Leibler (KL) divergence, as this measure possesses three useful properties for calibration [60]: (a) it is zero when  $P = Q$ ; (b) it is sensitive to small differences between the distributions and (c) it favors uniform and less extreme distributions.

Given a target distribution  $P$  and a realized distribution  $Q$ , the KL divergence is defined as follows.

$$
\mathrm{CL}_{KL}(P_u,Q_u) = \sum p(g|u)\cdot \log \frac{p(g|u)}{q(g|u)}. \tag{6}
$$

Most studies follow Steck's approach and implement the Kullback- Leibler (KL) divergence as the primary calibration measure. Another widely adopted measure is the Jensen- Shannon (JS) divergence, which has been used in several works [1, 4, 15, 32, 33, 37, 38, 46, 47, 62, 65].

Beyond these commonly used approaches, some studies propose alternative distance measures. In an early work, Oh et al. [48] proposed to use the Earth- Mover's Distance (EMD) as a way to assess the distance between two distributions, where the EMD measure corresponds to the amount of 'work' needed to transform one distribution into the other. Sacharidis et al. [53] later introduce a measure called Normalized Fairness, which is the divergence between  $P$  and  $Q$  normalized by the highest possible upper bound. The rationale of the approach is however not further detailed in the paper. Seymen et al. [56] propose to use the Weighted Total Variation of  $P$  and  $Q$  due to the non- linearity of the KL- divergence measure. Chen et al. [12] integrate calibration directly into a neighbor- based recommender algorithm and develop an approach that applies the KL- divergence measure across three different "orders" of calibration. The first- order distance measures the target distribution distance between two users. The second order extends this by evaluating the target distribution distances among three users, using one of these users as a reference point to obtain the distance value. The third order measures the distance between the average genre ratings for two users. By combining these three orders, the approach enhances the neighbor selection process, ultimately improving the quality of the recommendations.

In principle, any distance measure can be used in this component. Following this idea, da Silva and Durão [15] conduct a benchmark study comparing 57 distance measures in a movie recommendation context. Their findings reveal that the same four measures, Viciś Emanon2, Kulczynski S, Kumar Johnson, and Additive Chi, consistently achieve the best performance across different datasets. This suggests that alternatives to KL and JS divergence may yield superior results.

3.1.6 Trade- off Balance Modeling. Calibration commonly involves a trade- off situation, where the aggregated relevance of the recommended items competes with the goal of covering past user preferences well. This trade- off must be modeled accordingly, using a formalism that quantifies the overall quality or utility of a (calibrated) recommendation list by taking into account both factors. The most common formulation in the literature to obtain an optimal calibrated list  $L_{u}^{*}$  is as follows:

$$
L_{u}^{*} = \arg \max_{L_{\cdot},|L| = N}(1 - \lambda)\cdot \mathrm{REL}(L_{u}) - \lambda \cdot \mathrm{CL}(P_{u},Q(L_{u})). \tag{7}
$$

Equation 7 consolidates all functions into a single utility value, which serves as the foundation for evaluating the quality of the calibrated recommendation list. While many studies adhere to this formulation, several alternative trade- off implementations have been proposed. Chen et al. [10] extend the original trade- off function by incorporating additional relevance and distribution values using an algorithm- dependent technique. A detailed explanation of the intuition behind the specific approach is however not given. Other variations include the work by Nagbiaei et al. [46], who adopt a Mixed- Integer Linear Programming (MILP) approach to optimize the trade- off function, leading to a slight performance improvement. da Silva and Durão [13] introduce a logarithmic trade- off function that modifies Steck's original proposal by incorporating a logarithmic transformation and personalized bias scores. The authors argue that using a logarithmic function helps to mitigate large discrepancies in the calibration level. Additionally, by incorporating user bias scores, the method prioritizes the selection of items whose predicted ratings are close to the user's average rating, thereby reducing discrepancies between user preferences and the recommended list.

Seymen et al. [56], in contrast, modify the trade- off calculation by computing a global trade- off value across all users, rather than applying it individually. This approach aims to improve the overall diversity of the recommended items. Instead of creating independent recommendation lists for each user, the method inserts one item into each user's list and then evaluates the system- wide level of miscalibration. This process is repeated incrementally- - adding another new item to all lists and re- evaluating- - allowing the system to progressively reduce miscalibration at a global level.

Coming back to the most frequently used trade- off balance approach as described in Equation 7, the trade- off weight  $\lambda$  is typically a constant value that is used for all users. A weight of 0.0 focuses solely on accuracy, effectively bypassing calibration. Conversely, a weight of 1.0 maximizes calibration, constructing the most calibrated list possible. A value of 0.5 considers both factors equally. Many studies evaluate this weight using varying values from 0.0 to 1.0 in increments of 0.1, aiming to identify an optimal balance of calibration and accuracy.

While most studies treat  $\lambda$  as a constant value that is predefined by domain experts, some works have explored personalized approaches to determine this trade- off weight dynamically. Wang et al. [67] integrate calibration into the recommender algorithm as part of a deconfounding approach to mitigate bias amplification. Their method personalizes  $\lambda$  by normalizing divergence values, using the minimum and maximum divergence values across all users. Lesota et al. [37] take a different approach by dynamically adjusting  $\lambda$  throughout the calibration step, seeking a balance between calibration and relevance that recomputes the  $\lambda$  value as items are added to the list. da Silva et al. [17] propose two methods for deriving a user- specific value for  $\lambda$ , arguing that individual users have different preferences regarding the degree of calibration in their recommendations. Different

studies show [13, 15, 17] that using personalized weights can lead to improvements, depending on the recommender algorithm.

3.1.7 Optimized Item Selection. Determining the optimal list  $L_{u}^{*}$  is an NP- hard combinatorial optimization problem, which does not scale well given the large item catalogs in many application settings. Usually, the search for an optimal (or sufficiently good) solution is thus limited to a smaller candidate set, as described above, e.g., the top- 100 most relevant items determined by the baseline recommendation technique.

Technically, often greedy re- ranking techniques are applied to speed up the search for goodenough solutions. Steck proposes to use the Surrogate Submodular Greedy algorithm, which achieves a  $(1 - 1 / e)$  optimality guarantee12 for optimal item selection. The algorithm starts with an empty list and iteratively adds items at each list position by selecting the next item that maximizes the utility of the list according to Equation 7. This process is continued until a list of the desired length is obtained. The algorithm's complexity increases with the number of candidate items and the size of the requested recommendation list, making it computationally expensive, especially when dealing with large item catalogs.

Several alternative algorithms have been proposed to improve accuracy, calibration, and computational efficiency. For instance, the Top- Z algorithm implemented in [69, 70] offers a smooth improvement over the Surrogate Submodular algorithm. Top- Z introduces two additional parameters: the number of unique genres in the system  $(|G|)$  and a constant  $Z$  defined by the system specialist. The product of these two parameters determines the number of candidate items to be re- ranked, effectively controlling the re- ranking pool size and improving the balance between computational efficiency and calibration performance as reported in paper results. Starychfojtu and Peska [58] introduce a fuzzy logic- based approach by implementing the Fuzzy D'Hondt algorithm for calibration re- ranking. They argue that the Fuzzy D'Hondt algorithm not only ensures a calibrated selection but also provides a fair ordering of the recommended items. Additionally, some studies frame item selection as a branch- and- bound problem [46, 56]. In particular, Seymen et al. [56] adopt this approach in their method, which fills one position across all users' recommendation lists simultaneously. As the algorithm inserts an item into a user's list, it immediately updates the system- wide miscalibration value before proceeding to the next insertion. This process aligns with the branch- and- bound workflow, enabling dynamic recalculations of the miscalibration across all lists during list construction. Naghiaei et al. [46] rely on the use of a branch- and- bound approach as a consequence of formulating the problem as a Mixed- Integer Linear Programming (MILP) task. Abdollahpouri et al. [5] observe that MILP cannot guarantee an optimal solution efficiently in practice. To address this limitation, they argue for the need to adopt a more computationally efficient algorithm. Based on this motivation, the authors reformulate the calibration recommendation problem as a maximum flow optimization task. They introduce a new method called Minimum- Cost Flow (MCF), which models the recommendation process using a graph structure, where costs are assigned to the edges between adjacent nodes. The objective of MCF is to determine the least- cost path through the graph that yields a well- calibrated recommendation list. In a subsequent study, Naghiaei et al. [45] also apply MILP and branch- and- bound techniques, aiming to assess the improvements of their proposal across a broader set of metrics beyond accuracy. Finally, Kleinberg et al. [31] propose a novel algorithm for generating an optimal calibrated recommendation list, though their work remains theoretical and lacks empirical validation.

Overall, four major algorithmic techniques have been investigated in calibrated recommendation: greedy algorithms, linear programming, branch- and- bound methods, and fuzzy logic. These approaches highlight the flexibility in solving the item selection problem, although trade- offs exist

between computational cost and the effectiveness of accuracy and calibration. Jeon et al. [26] present a time complexity analysis of various proposals, including those from[5, 56, 60], alongside their own approach, which is also based on a greedy strategy. They show that the MCF method introduced in [5] exhibits the highest time complexity, making it the slowest among the compared methods. The MILP- based approach proposed in [56] follows as the second slowest. In contrast, Steck's original approach [60] and Jeon et al.'s approach [26] demonstrate similar and significantly better time complexities. These findings are further supported by results reported in [44].

# 3.2 Alternative Implementation Approaches

While the majority of the studies in the literature rely on the general architecture described in the previous section, a number of alternative approaches can be found as well. Specifically, we have identified a number of works that support the parallel consideration of multiple optimization objectives. Furthermore, a few works can be found that do not rely on a post- processing (re- ranking) approach for calibration.

3.2.1 Supporting Multiple Optimization Objectives. Most calibration studies use a single categorical attribute- - such as the item's genre- - as the basis for deriving the target distribution and generating a recommendation list that aligns with it. However, some studies go further by incorporating multiple objectives simultaneously.

In a work preceding Steck's proposal, Jugovac et al. [27] propose a parameterizable re- ranking technique to balance multiple objectives like accuracy, diversity, and item popularity. In their approach, the level of miscalibration is individually measured for each of these objectives. During the calibration process, the effects of re- ranking an item are evaluated for each of these dimensions, and a potential re- ranking move is only applied in case it leads to an aggregated improvement when considering all dimensions.

In a more recent work, Gomez et al. [23] propose a system that simultaneously optimizes accuracy, provider fairness, and calibration. Their approach introduces adjustments at both the global level focused on provider fairness, and the individual level focused on calibration. The core intuition is that the post- processing phase should ensure that the recommendations satisfy all three objectives simultaneously. Similarly, Treuillier et al. [61] introduce a framework aimed at simultaneously optimizing accuracy, diversity, and calibration, ensuring that enhancements in diversity and calibration do not compromise recommendation accuracy. They argue that diversity and calibration are as important as accuracy and should not be treated as secondary objectives.

Sacilotti et al. [54] and Souza et al. [19] simultaneously investigate whether users prioritize genre calibration or popularity calibration and personalize the re- ranking process accordingly. Their approach adapts the recommendation list based on individual user preferences regarding these two calibration factors. In another study, Souza et al. [57] introduce a two- stage post- processing approach, where the list is first re- ranked based on popularity calibration, followed by a second re- ranking step that applies genre calibration. This sequential method aims to control both factors more effectively while maintaining recommendation quality. Finally, Naghiaei et al. [45] extend multi- objective recommendation further by optimizing accuracy, calibration, novelty, surprise, coverage, and redundancy simultaneously. They define redundancy as a measure of how many redundant categories a user has interacted with. Their approach integrates these objectives to generate a well- balanced recommendation list that satisfies diverse user and system constraints.

3.2.2 Alternatives to Re- ranking. Most calibration proposals rely on a post- processing step. However, some approaches integrate calibration directly into the recommender algorithm, eliminating the need for post- processing. These studies highlight the benefits of embedding calibration within

different recommendation paradigms, enhancing both efficiency and effectiveness while avoiding the computational overhead of a separate post- processing step.

Chen et al. [10] present the Decoupled- Aggregated Calibrated Sequential Recommendation (DACSR) method, which incorporates a divergence- based calibration measure directly into the loss function. To generate calibrated recommendation lists, the authors design a loss function that enforces consistency between the recommended sequence and the user's historical interaction sequence, ensuring that the calibration objective is considered during model training. In another work, Chen et al. [11] explore the intersection of calibration, long- tail popularity, and session- based recommendation. The authors use two categories to derive the distributions: tail items, representing less popular items, and head items, representing the most popular ones. The core idea is to balance these two categories according to the user's session provided as input.

Jeon et al. [26] also focus on sequential problems and introduce a novel method designed to improve recommendation quality while maintaining efficiency. Their approach consists of two main steps. The first one is backbone integration, where calibration is embedded within the recommender algorithm to align candidate items with the user's preference distribution while preserving accuracy. The second one is re- ranking for relevance, where the recommendation list is re- ranked to prioritize relevance without compromising calibration, seeking to include relevant item in the top and calibration in the bottom of the recommended list.

The integration of calibration into a neighborhood- based algorithm is proposed in [12], where the calibration trade- off is considered during neighbor selection. This approach utilizes the calibration distance measure to consider genre distribution differences between the target user and their nearest neighbors, as well as the deviation in average rating behavior.

A graph- based approach is presented by Mo et al. [44], who develop a calibrated recommendation system for Point of Interest (POI) suggestions. Their approach integrates calibration into a graph convolutional network (GCN), eliminating the need for post- processing by directly generating calibrated recommendations through the model itself. Wang et al. [67], finally, compare calibration with four types of post- processing approaches and introduce the Deconfounded Recommender System (DecRS) model. This model is designed to extend the calibration idea to alleviate bias amplification directly within the recommender algorithm. DecRS leverages a causal graph framework, employing backdoor adjustment to mitigate the influence of confounding factors and to thereby improve the quality of the recommendations.

3.2.3 Recommending Users For Items. Most calibrated recommendation systems follow the traditional approach of recommending items to users. In contrast, Zhao et al. [69], as discussed earlier, reverse this process by focusing on user- to- item recommendations, that is, determining which users are the most appropriate target audience for a given item, rather than which items should be recommended to a user. Their work addresses the Target Customer Distortion problem, where conventional recommender systems distort the true customer base of an item, recommending it to users whose profiles do not align with the item's actual audience based on historical data. To tackle this, the authors apply calibrated recommendation techniques within this reversed recommendation framework.

# 4 Evaluation of Calibrated Recommendations

Having discussed technical approaches to calibration, we will now turn our attention on how these proposals are evaluated in the literature, focusing on application domains and datasets, experimental designs, and evaluation metrics.

# 4.1 Application Domains and Datasets

4.1 Application Domains and DatasetsFigure 4 provides an overview on the studied application domains of calibrated recommendations. We observe a strong research focus on the movie domain, and almost every paper in our analysis considers a dataset from this domain in the evaluation. Among the other use cases, only music recommendation scenarios are considered to a notable extent. All other application domains are in the focus of less than five papers. The "Others" category in Figure 4 includes papers on food, recipes, games, podcasts, and dating.

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/eecb581f276b8d72dc05200260ac57a47fc484c019598a63ff8a14006d1be1c8.jpg)  
Fig. 4. Number of papers per application domain.

Like in the broader recommender systems literature, studies focusing on the movie recommendation scenario seem largely over- represented, and a majority of these papers are based on one of the MovieLens datasets.<sup>14</sup> Conversely, other domains remain underexplored, highlighting the need for greater diversity in evaluation datasets. This gap presents an opportunity for future research to expand calibration techniques into less- explored domains and compare their effectiveness with well- established areas like movies and music.

# 4.2 Evaluation Methodologies

Three primary evaluation methodologies are commonly used to assess the efficacy of recommendation systems: offline evaluation, user studies, and online experiments [68]. Figure 5 shows the distribution of the chosen evaluation approaches in the surveyed papers.

Offline evaluation approaches strongly dominate the research landscape in the area of calibrated recommendations. This aligns well with observations in other areas of recommender systems research, as reported in other recent surveys [18, 34]. Specifically, in our survey, we found that  $88\%$  of the analyzed studies employ an offline evaluation protocol, while only  $8\%$  conduct user studies, and  $2\%$  , i.e., one paper, perform online experiments. One paper does not provide any experimental results. Generally, although offline evaluations allow for extensive benchmarking on multiple datasets and with multiple accuracy and beyond- accuracy metrics, the limited number of studies with humans strongly restricts our understanding of how calibration impacts real user preferences and behaviors. Thus, our study points to a major research gap in research on calibrated recommendations.

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/15197b6972390451bac80b1b575dd2741b611a9e32499df68c73f95e60b49613.jpg)  
Fig. 5. Distribution of evaluation methodologies.

# 4.3 Offline Experimental Designs

Most papers rely on established offline evaluation setups, where the existing data is split into training, validation, and test data, and the splitting is either done randomly or based on time. Commonly, two or more datasets are used for the evaluation.<sup>15</sup>

Figure 6 provides an overview of the data splitting strategies used in the considered studies. Worryingly, we find that  $27.1\%$  of the papers do not report any information regarding their data partitioning, which limits reproducibility. Among those that do,  $25\%$  adopt a three- way split into training, validation, and test sets. According to Figure 6b, the most common configuration for this setup is  $60\%$  for training,  $20\%$  for validation, and  $20\%$  for testing (60:20:20). About half of the studies  $(47.9\%)$  use a simpler split into training and test sets, with the most frequent ratio being  $80\%$  for training and  $20\%$  for testing (80:20). These findings suggest that a large number of papers may rely on a weaker offline evaluation protocol using only training and test sets, rather than the more robust three- way split that includes a validation phase.

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/e59f2d79b7c7728ea22248dd7289dba27b5605b2c73d9ed2eb9fec2902754257.jpg)  
Fig. 6. Data splitting overview, splitting partitions and ratios.

Baseline Algorithms. The majority of the papers apply calibration as a post- processing step, as discussed above. Thus, arbitrary baseline rankers can be used in these cases. Considering the family of the used baseline ranking techniques, we find that collaborative filtering (CF) is the dominant approach and almost  $90\%$  of the analyzed articles report experiments with a CF- based algorithm. Experiments using content- based filtering techniques are rather rare, and can only be found in about  $10\%$  of the studies. Other recommendation techniques (e.g., context- aware and hybrid models) have yet to be explored within the field of calibration.

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/93a225b9c67f5cf3fe8258760e99e4ce8a687390f363c4ac1364a246abfbe8f1.jpg)  
Fig. 7. Frequency distribution of baseline algorithms. The "Others" category comprises 38 algorithms that were used fewer than three times.

In terms of CF- based approaches, a rich variety of techniques were considered in the literature. The three most frequently used individual models are BPR [50], ItemKNN [55] and UserKNN [51] (Figure 7). Across all surveyed papers, more than two dozen other algorithms were considered in the experiment, including both various matrix factorization models and deep learning models. Experiments were also made both for sequential recommendation settings as well as for traditional top- n recommendation scenarios.

Calibration Targets. In most studies, item genres are used as the target category/feature for deriving user distributions [12, 14, 17, 56, 58, 60, 71]. However, alternative categorical attributes are used as well. For instance, Abdollahpouri et al. [4] introduce Calibration Popularity (CP), a variation of Steck's original method that replaces genres with item popularity levels. In this approach, items are categorized into three groups (Head, Middle, and Tail) based on their popularity. The user's preference distribution is then derived from the proportions of interactions with items from each of these groups. Following Abdollahpouri, several other studies [11, 19, 32, 33, 37, 54, 57, 62] focus on item popularity. In [69], in contrast, the user's gender and age are used as categories to derive the distribution, because in this study, users are recommended for items. Wang et al. [66] apply a quality- aware approach to calibration, where the quality of the items can be high or normal. Lin et al. [40] use the video tag manually set by a specialist as a feature. Also inspired by work on intent- aware recommendation, Kaya and Bridge [29] instead of item features, they implement an idea called user subprofile as a feature, where a subprofile is a set of items that capture one of the user's interests. The authors combine this subprofile idea with calibrated recommendations based on their previous studies, where they develop a method called Subprofile- Aware Diversification (SPAD).

Metrics. In terms of evaluation metrics, we recall that calibration is considered a trade- off problem between accuracy and other quality factors. Thus, every paper in our survey that relies on offline evaluation uses at least one metric to assess recommendation accuracy. The three most popular list- based metrics are NDCG, Precision, and Recall. Other metrics such as MRR or MAP are used less frequently; the prediction error metric MAE is considered in one single paper [35].

In terms of other quality factors, a rich variety of metrics is used. We group them into the following categories: Beyond Accuracy, Popularity, and Fairness. An overview of the use of different metrics and the corresponding optimization is shown in Figure 8.

![](https://cdn-mineru.openxlab.org.cn/extract/ba66fd76-0006-4865-98c0-4456ffdd878f/0e6a5e5ec18fff2cf3181727d78837ea89c42e11f0aeb38e17b79b146fe81d46.jpg)  
Fig. 8. Focus of the calibration process in offline evaluations.

The group of Beyond Accuracy metrics include aspects such as recommendation novelty, as studied in [14, 22, 45], diversity [4, 29, 46, 61], coverage [1, 4, 11, 14, 19, 22, 45- 47, 49, 54, 56, 66], or unexpectedness and serendipity [14]. The tendency of algorithms to focus on popular items before and after calibration (Popularity) is assessed in a variety of ways in the studied literature, including Average Recommendation Popularity (ARP) [33, 45, 56], Group Average Popularity (GAP) [3, 57], User and Item Popularity Deviation [4, 33], Popularity Lift [35], Bias Disparity [23], Average Percentage of Long Tail Items (APLT) and Average Coverage of Long Tail items (ACLT) [33]. A smaller number of works aim to assess the Fairness of the recommendations through corresponding metrics, e.g. [53, 56].

Generally, papers that are based on offline evaluations<sup>16</sup> implement some metric to quantify the level of miscalibration. Various ways are used in the literature to measure miscalibration, e.g., with the help of the Kullback- Leibler divergence [60], the Earth- Mover's Distance [48], or the Jensen- Shannon divergence [4], as discussed above. Besides these general methods to quantify the divergence of two distributions, a number of calibration- specific measures were proposed in the surveyed literature.

da Silva et al. [17], for example, propose the Mean Average Calibration Error (MACE) and the Mean Rank Miscalibration (MRMC) metrics. These normalized metrics were designed to enable a valid comparison of existing calibration approaches from the literature which were originally evaluated using different miscalibration measures, e.g., when one paper used the Kullback- Leibler divergence

and the other relied on the Jensen- Shannon divergence. In a subsequent related work, da Silva and Durão [13] introduce two more metrics, Coefficient of Miscalibration (CMC) and Coefficient of Calibration Error (CCE). These metrics represent further developments of the MACE and MRMC metrics, which consider the Mean Average Precision (MAP) in the equation, aiming to ensure that calibration quality is evaluated relative to ranking accuracy. Lower metric (error) values indicate a desirable balance between high MAP values and low miscalibration scores, representing an optimal trade- off. Finally, Chen et al. [12] introduce the Neighbor Average Calibration Error (NACE) and Neighbor Average Genre Rating Calibration Error (NAGRCE) metrics. Drawing on inspiration from [17], these metrics were designed for use in a neighborhood- based approach to calibration. NACE measures the average mismatch between the user's target distribution and those of neighboring users, assessing how well the recommendations align within a local user cluster. NAGRCE extends this concept by capturing differences in genre- based ratings, comparing the target user's ratings across genres with those of their neighbors.

# 4.4 User Studies and Field Tests

Although offline evaluations remain the most widely used approach in recommender system research, as noted by Zangerle et al. [68], they primarily offer a quick, reproducible, and controlled evaluation of system performance using well- defined metrics. Evaluations with humans, in contrast, can provide a reliable understanding of how real users perceive and interact with calibrated recommendations. Such evaluations with humans are commonly either implemented as controlled user studies, where study participants typically interact with a prototype system that was developed for the purpose of the study, or as field tests, where different versions of a system are tested in a real- world system in the form of an A/B test. In the case of A/B tests, a common approach is to first conduct offline experiments, select the most promising algorithms, and then validate their effectiveness online. Such a setup ensures that a calibration model not only performs well in controlled offline settings, but also translates to improved user experience in real- world applications.

User Studies. Our survey surfaced only five papers that involve studies with humans. Four of them report outcome of user studies [7, 38, 59, 62]. Starychfojtu et al. [59] address the problem of providing users with inspiration through recommendations in the context of cooking and food shopping. They develop a mobile app that generates recommendations based on past shopping lists of users, and they conduct a preliminary user study to assess the quality perceptions of three recommendation strategies: similarity- based, diversity- enhanced, and calibration- based. 32 respondents participated in the study and provided numerical ratings for recipe recommendations that were based on a set of food items in a given shopping cart. The results showed that pure similarity- based recommendations were perceived as being significantly worse than the diversity- enhanced and calibration- based ones. The calibrated recommendations were found to be slightly better than the diversity- enhanced ones, which can be attributed to the more sophisticated calibration approach based on the Fuzzy D'Hondt's algorithm that was implemented for the study.

In a more recent work, Alves et al. [7] aim to assess user perceptions when the system's recommendations are calibrated for fairness. In their study in the movie domain, the authors consider two situations that may be considered to lead to unfair situations. First, it might be considered unfair if a system mostly recommends already popular movies, reinforcing the system's popularity bias [34]. Second, one may consider it unfair if low- budget productions do not receive enough exposure by the system. For the user study, corresponding calibration techniques were implemented that ultimately lead to a more frequent recommendation of movies that are not blockbusters and were produced with a limited budget. An online study with 500 participants showed that a properly tuned calibration technique can be effective in guiding the choices of the participants without leading to

a decreased quality perception of the recommendations. Notably, Alves et al. [7] also observe that users only report noticing fairness improvements when explicitly alerted by the system through an explanation. This suggests that users should be explicitly signaled about this aspect within the user interface.

The problem of popularity bias in system- generated recommendations is also the focus of the study by Lesota et al. [38]. The authors conducted an online study in the music domain involving 56 participants. In their within- subjects experiment, the participants were shown five personalized recommendation lists of ten items, where each of them was created with a different algorithm. Among other aspects, the participants were then tasked to provide an assessment of the popularity bias of the list. These assessments were then correlated with the level of popularity miscalibration, as measured through the Jensen- Shannon Distance, to analyze if this computational measure can be used as a proxy for user perceptions. Differently from the findings reported earlier by [20] on perceived popularity bias, Lesota et al. found "that users generally do perceive significant differences in terms of popularity bias between algorithms if this bias is framed as popularity miscalibration."

Popularity- calibrated recommendations in the music domain were also the focus of the user study by Ungruh et al. [62]. In their study, the authors- - like in the work by Alves et al. [7]- - aimed to assess user perceptions when presented with calibrated recommendations. They recruited 40 participants with an active Spotify account, who then interacted with a tool that was developed for the study, in which they completed listening sessions and were then asked to choose some of the recommended items to add to their playlist. Different recommendation algorithms and calibration techniques were explored in the study, which produced recommendations that differed in their popularity bias and which measurably influenced the choices of the participants. In terms of user perceptions, the authors found that the varying levels of popularity bias did not significantly affect the users' satisfaction with the recommendations. For some approaches, an effect on the users' familiarity with the recommendations was observed. The authors argue that this represents an opportunity for discovery, which may ultimately lead to higher satisfaction by promoting less popular items. In terms of the perception of popularity bias, Ungruh et al. [62] found that users noticed differences in popularity only when using one of the two explored calibration techniques. Overall, combining the results by Lesota et al. [38] and Ungruh et al. [62] on user perceptions of popularity changes indicates that more research and studies involving more participants seem required in this area in the future.

Field Tests. We could only find one single paper that reports on the outcome of field- testing a calibration approach. Klimashevskaia et al. [32] implement the Calibrated Popularity (CP) technique in a production system, evaluating its impact in a real- world setting. Their findings of an A/B test, which involved calibrated and non- calibrated recommendations, indicate that calibration can be used to effectively promote less- popular items without negatively impacting relevance, as measured through the Click- Through- Rate (CTR). This suggests that the common accuracy- calibration trade- off may not necessarily exist in real- world settings. The study however also revealed practical challenges when applying calibration in a real- world settings. One particular challenge lies in the question how to set a proper threshold that is used to distinguish popular from less popular content. Depending on the threshold setting, the "intervention" through the calibration technique may for example be too limited to lead to a measurable effect on user behavior. Furthermore, since A/B tests are often only run for a limited amount of time, e.g., a few weeks, it is difficult to predict long- term effects of calibrated recommendations on user behavior and satisfaction.

# 5 Discussion

5 DiscussionMost studies indicate that calibration can enhance recommendation systems by improving key performance metrics. However, this benefit typically comes at the expense of increased computational cost, primarily due to the additional re- ranking step involved in the calibration process. The effectiveness of calibration varies depending on the system architecture and evaluation methodology, resulting in a wide range of reported outcomes. Consequently, several open challenges and research directions remain, warranting further investigation in future studies.

# 5.1 Issues

Challenges of Calibrating the Calibration Process (Thresholds, Weights). A key challenge in calibration is determining the optimal settings to maximize performance. One such challenge is selecting the appropriate threshold for the number of items used in re- ranking. A higher threshold increases the computational cost but also expands the set of candidate items, potentially improving calibration. However, no standard threshold values have been widely established, leaving this issue unresolved. Another critical factor is the trade- off weight  $\lambda$  , which balances calibration and accuracy. Most studies evaluate a range of values for  $\lambda$  (e.g., 0.0 to 1.0), as performance fluctuates depending on the system. While personalized approaches to determine  $\lambda$  have been proposed, their effectiveness remains underexplored, limiting our understanding of their applicability. Overall, optimizing the selection of both the threshold and  $\lambda$  remains an open problem in the field. Further research is needed to evaluate these parameters systematically and develop adaptive strategies for improving the effectiveness of calibrated recommendations.

Computational Complexity and Scalability: Challenges. Calibrated recommendations are NP- hard, as generating the optimal recommendation list requires evaluating all possible combinations of candidate items across all possible positions in the list. This results in a combinatorial explosion, where the complexity increases exponentially with the number of items, genres, users, and the length of the recommendation list. Jeon et al. [26] conducted a time complexity analysis across various studies, highlighting that some calibration methods have high computational costs. Similarly, da Silva et al. [16] compiled runtime measurements from multiple experiments, showing that the time required to derive the distribution in calibrated recommendations scales with dataset size. These findings indicate that scalability is a major concern in the field. Most studies are conducted in controlled environments, with predefined numbers of users and items. However, real- world production systems operate under dynamic, large- scale conditions where computational efficiency is crucial. The trade- off between improved performance and system responsiveness may become impractical if a calibration approach requires excessive computation time. This challenge presents a research opportunity for new methods aimed at reducing computational costs in calibration while preserving its benefits in evaluation metrics. Future work should explore efficient approximation techniques, heuristics, and parallelization strategies to improve the scalability of calibrated recommendation algorithms.

Limitations of Calibration, possible undesired effects in practice. Numerous studies have shown that calibration can influence additional recommendation objectives such as coverage, diversity, popularity bias, and serendipity. These effects are generally positive, often leading to improved system performance. However, in some cases, particularly regarding accuracy, calibration has been observed to reduce performance. Beyond accuracy, other potential negative side effects of calibration remain underexplored in the literature. For example, in news recommendation systems, where recency and popularity are critical, calibration may have a negative impact by suppressing popular or trending content. Similarly, in domains with strong inherent bias, such as job- seeking

platforms where candidates are recommended to companies, calibration may either help mitigate the bias or inadvertently maintain it. $^{17}$  Understanding the context and underlying data distribution is essential to assess whether calibration leads to fairer recommendations or introduces unintended consequences. Moreover, calibration introduces financial and operational implications due to increased computational complexity. In real- world deployments, the benefits of calibration may be offset by the associated production costs, raising concerns about its scalability and feasibility in practice.

Calibration Can Lead to a Reduction of Performance for Individual Users. Evaluation in recommender systems typically relies on reporting the average performance across all users, resulting in a single aggregate metric. The calibration literature follows this same practice. However, such aggregated evaluations can obscure important user- level variations. For some users, the calibrated recommendation list may underperform, failing to reduce miscalibration, decreasing accuracy, or lacking novelty and diversity. As a result, the individual- level impact of calibration remains largely unexplored, highlighting a critical gap in current research.

User Perception. An issue reported by Alves et al. [7] and Lesota et al. [38] is that users only perceive a recommendation list as calibrated or fair when it is accompanied by a label or explanatory text indicating so. This suggests that the effects of calibration may be too subtle to be noticed by users on their own, or that the calibration does not meaningfully impact the user experience. Notably, this perception gap occurs despite observed improvements in objective performance metrics, indicating that the effectiveness of calibration from a system perspective does not necessarily translate into perceived value for the user. This raises questions about how calibration should be designed or communicated to be both effective and perceptible.

# 5.2 Future Work and Outlook

When to calibrate. Typically, calibration is applied uniformly across all users in the system, with the same calibration degree. However, a few studies, such as [13, 15, 17, 37, 67], have proposed personalized approaches to adjust the calibration level for each user. Additionally, Sacilotti et al. [54] introduce a method to personalize calibration based on the user's preferred attribute distribution (genre or popularity). Despite these efforts, an important open question remains: some users may prefer to receive popular or trending items rather than a fully calibrated list. Future research should explore when and for whom calibration is appropriate, identifying users who benefit from calibration and those who may prefer alternative recommendation strategies depending on their preferences and contextual factors.

Dynamic Calibration (Recent Events, Intent- Awareness). Calibration is typically implemented as a static method, deriving the user distribution from historical preferences, as discussed throughout this survey. However, current techniques do not adapt the distribution dynamically to account for recent changes in user behavior or emerging interests. There is limited research addressing this gap. Only one study to date, by Kaya and Bridge [29], explicitly explores the connection between calibration and intent- awareness, suggesting that combining these two approaches could lead to a deeper understanding of user intent and better interpretation of evolving user trends. Additionally, da Silva et al.[16] and Lin et al.[40] propose calibration strategies that incorporate temporal aspects of user preferences. While these approaches introduce time- aware elements, they still lack full dynamism and real- time adaptability to user preference shifts. Future work should focus on developing fully dynamic calibration methods capable of adjusting to user intent and evolving tastes in real time.

Underexplored Recommendation Techniques: Hybrid, Context- Aware, and Others. Collaborative filtering remains the most widely studied technique in the calibration literature, largely due to its ease of reproduction and the availability of numerous public datasets. This trend is evident after compiling all the papers, which illustrates the predominance of collaborative approaches in calibrated recommendation studies. However, this focus restricts the applicability and generalizability of calibration techniques. Future research should expand the scope by incorporating content- based methods and exploring hybrid or context- aware recommendation techniques. Investigating the impact of calibration within these less- explored paradigms can reveal new insights and broaden the effectiveness of calibrated recommendations across diverse system architectures.

New domain exploration must happen. The movie and music domains are the two most extensively explored areas in calibrated recommendation research. These domains benefit from the availability of rich datasets, such as MovieLens and Last.fm, which have become standard benchmarks in the field. In contrast, other domains remain either underexplored or entirely unexplored, with insufficient empirical results to support domain- specific inferences. This imbalance highlights an important research gap, signaling the need to expand calibration studies into diverse domains to evaluate their generalizability and effectiveness in different recommendation contexts.

More Field Studies. We analyzed 53 papers on calibrated recommendations. Most of these studies rely on offline evaluations, with only a few incorporating user or field studies. Notably, Klimashevskia et al. [32] stands out as the only work that applied calibration within a real- world production recommendation system. The limited application of calibration in practical environments restricts our understanding of its real- world impact on users. Therefore, future research must advance into field studies, enabling the community to gather more practical insights and assess how calibration is perceived and experienced by real users.

Cold Start Problem. As discussed earlier in this survey, calibration relies on past user interactions to estimate the user's preference distribution. However, a significant challenge arises in coldstart scenarios, where a user has little to no historical interaction data. In such cases, it remains unclear how calibration techniques should behave and how well they can generate a balanced recommendation list while simultaneously allowing the user to explore and define their preferences.

Benchmarking All Proposals. Approximately  $58\%$  of the 53 papers analyzed present a technical contribution to the field (Figure 3). Each proposal represents an advancement, introducing new strategies to generate calibrated recommendation lists. However, no study to date has comprehensively compared all—or even most—of these proposals within a unified evaluation framework. Future research should focus on benchmarking these approaches under consistent conditions, using the same dataset, preprocessing pipeline, and baseline recommender algorithm. This would enable a fair comparison across various performance metrics, such as precision, coverage, miscalibration, diversity, and popularity. Additionally, the reproducibility of these methods must be considered, especially given the growing concern within the recommender systems community regarding the replicability of published results.

# 6 Summary

In this survey, we analyzed 356 papers and selected 53 that specifically address the field of calibrated recommendations, plus two from previous similar ideas. Our analysis reveals a consistent growth in the number of published works over recent years, with the majority appearing in conference proceedings. Most of these contributions present technical proposals, complemented by studies focused on impact analysis and system comparisons. Calibrated recommendation systems are generally structured into several key components, and their specific implementations vary depending

on system objectives, ranging from genre or popularity- based distribution derivations to methods that integrate calibration either directly into the recommender algorithm or as a post- processing step. The field is predominantly explored in movie and music domains, which offer valuable insights into real- world applicability. However, the limited number of studies involving online systems highlights a significant gap in user- centered evaluation and practical validation. This shortcoming presents opportunities for future research, many of which have been outlined in this survey. Overall, calibrated recommendation remains a relatively recent but increasingly influential area of research, with strong potential to enhance both user satisfaction and system performance.

# Acknowledgments

This research was funded in whole or in part by the Austrian Science Fund (FWF) 10.55776/COE12. For open access purposes, the author has applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.

# References

[1] Himan Abdollahpouri, Jesse Anderton, Zahra Nazari, and Ben Carterette. 2021. Target- aware Aggregate Diversification in Recommendation. In *Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization* (Utrecht, Netherlands) (UMAP '21). 16–21. doi:10.1145/3450614.3461681[2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re- Ranking. In *Proceedings of the Thirty- Second International Florida Artificial Intelligence Research Society Conference*, Román Barták and Keith W. Brawner (Eds.). 413–418. https://aaai.org/ocs/index.php/FLAIRS/FLAIRS19/paper/view/18199[3] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness in Recommendation. In *Proceedings of the 14th ACM Conference on Recommender Systems* (Virtual Event, Brazil) (RecSys '20). 726–731. doi:10.1145/3383313.3418437[4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher, and Edward Malthouse. 2021. User- centered Evaluation of Popularity Bias in Recommender Systems. In *Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization* (Utrecht, Netherlands) (UMAP '21). 119–129. doi:10.1145/3450613.3456821[5] Himan Abdollahpouri, Zahra Nazari, Alex Gain, Clay Gibson, Maria Dimakopoulou, Jesse Anderton, Benjamin Carterette, Mounia Lalmas, and Tony Jebara. 2023. Calibrated Recommendations as a Minimum- Cost Flow Problem. In *Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining* (Singapore, Singapore) (WSDM '23). 571–579. doi:10.1145/3539597.3570402[6] Fabian Abel, Yashar Deldjoo, Mehdi Elahi, and Daniel Kohlsdorf. 2017. RecSys Challenge 2017: Offline and Online Evaluation. In *Proceedings of the Eleventh ACM Conference on Recommender Systems* (Como, Italy) (RecSys '17). 372–373. doi:10.1145/3109859.3109954[7] Gabrielle Alves, Dietmar Jannach, Rodrigo Ferrari De Souza, and Marcelo Garcia Manzato. 2024. User Perception of Fairness- Calibrated Recommendations. In *Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization* (Cagliari, Italy) (UMAP '24). 78–88. doi:10.1145/3627043.3659558[8] Keith Bradley and Ben Smyth. 2001. Improving Recommendation Diversity. In *Proceedings of the 12th National Conference in Artificial Intelligence and Cognitive Science*. 75–84. [9] Sung- Hyuk Cha. 2007. Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions. *International Journal of Mathematical Models and Methods in Applied Sciences* 1, 4 (04 2007), 300–307. http://www.fisica.edu.uy/~cris/teaching/Cha_pdf_distances_2007. pdf[10] Jiayi Chen, Wen Wu, Liye Shi, Yu Ji, Wenxin Hu, Xi Chen, Wei Zheng, and Liang He. 2022. DACSR: Decoupled- Aggregated End- to- End Calibrated Sequential Recommendation. *Applied Sciences* 12, 22 (2022). doi:10.3390/app122211765[11] Jiayi Chen, Wen Wu, Liye Shi, Wei Zheng, and Liang He. 2023. Long- tail session- based recommendation from calibration. *Applied Intelligence* 53, 4 (01 Feb 2023), 4685–4702. doi:10.1007/s10489- 022- 03718- 7[12] Zhuang Chen, Haitao Zou, Hualong Yu, Shang Zheng, and Shang Gao. 0. Relevance meets calibration: Triple calibration distance design for neighbour- based recommender systems. *Journal of Information Science* 0, 0 (0). doi:10.1177/01655515231182069[13] Diego Corrêa da Silva and Frederico Araújo Durão. 2023. Introducing a framework and a decision protocol to calibrated recommender systems. *Applied Intelligence* 53, 19 (June 2023), 22044–22072. doi:10.1007/s10489- 023- 04681- 7

[14] Diego Corrêa da Silva and Frederico Araújo Durão. 2023. How Novel and Unexpected the Calibrated Recommendations Are?, In American Conference on Information Systems. AMCIS 2023 6, 6. https://aisel.aisnet.org/amcis2023/sig_odis/sig_odis/6[15] Diego Corrêa da Silva and Frederico Araújo Durão. 2025. Benchmarking fairness measures for calibrated recommendation systems on movies domain. Expert Systems with Applications 269 (2025), 126380. doi:10.1016/j.eswa.2025.126380[16] Diego Corrêa da Silva, Dietmar Jannach, and Frederico Araújo Durão. 2025. Considering Time and Feature Entropy in Calibrated Recommendations. ACM Trans. Intell. Syst. Technol. 16, 3, Article 62 (May 2025), 29 pages. doi:10.1145/3716858[17] Diego Corrêa da Silva, Marcelo Garcia Manzato, and Frederico Araújo Durão. 2021. Exploiting Personalized Calibration and Metrics for Fairness Recommendation. Expert Systems with Applications (2021), 115112. doi:10.1016/j.eswa.2021.115112[18] Yashar Deldjoo, Dietmar Jannach, Alejandro Bologin, Alessandro Difonzo, and Dario Zanzonelli. 2023. Fairness in recommender systems: research landscape and future directions. User Modeling and User- Adapted Interaction 34, 1 (April 2023), 59- 108. doi:10.1007/s11257- 023- 09364- z[19] Rodrigo Ferrari de Souza and Marcelo Garcia Manzato. 2024. Enhancing Calibration and Reducing Popularity Bias in Recommender Systems. In Enterprise Information Systems, Joaquim Filipe, Michal Smialek, Alexander Brodsky, and Slimane Hammoudi (Eds.). 3- 24. doi:10.1007/978- 3- 031- 64755- 0_1[20] Bruce Ferwerda, Eveline Ingesson, Michaela Berndl, and Markus Schedl. 2023. I Don't Care How Popular You Are! Investigating Popularity Bias in Music Recommendations from a User's Perspective. In Proceedings of the 2023 Conference on Human Information Interaction and Retrieval (CHIIR '23). 357- 361. doi:10.1145/3576840.3578287[21] Mouzhi Ge, Carla Delgado- Battenfeld, and Dietmar Jannach. 2010. Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity. In Proceedings of the 4th ACM Conference on Recommender Systems (RecSys 2010). 257- 260. doi:10.1145/1864708.1864761[22] Elizabeth Gómez, David Contreras, Ludovico Boratto, and Maria Salamo. 2024. AMBAR: A dataset for Assessing Multiple Beyond- Accuracy Recommenders. In Proceedings of the 18th ACM Conference on Recommender Systems (Bari, Italy) (RecSys '24). 137- 147. doi:10.1145/3640457.3688067[23] Elizabeth Gómez, David Contreras, Ludovico Boratto, and Maria Salamó. 2024. MOReGIn: Multi- Objective Recommendation at the Global and Individual Levels. In Advances in Information Retrieval, Nazli Goharian, Nicola Tonellotto, Yulan He, Aldo Lipani, Graham McDonald, Craig Macdonald, and Iadh Ounis (Eds.). 21- 38. doi:10.1007/978- 3- 031- 56027- 9_2[24] Dietmar Jannach and Markus Zanker. 2024. A Survey on Intent- aware Recommender Systems. ACM Trans. Recomm. Syst. 3, 2, Article 23 (Dec. 2024), 32 pages. doi:10.1145/3700890[25] Dietmar Jannach, Markus Zanker, Alexander Felfernig, and Gerhard Friedrich. 2011. Recommender Systems - An Introduction. Cambridge University Press.[26] Hyunsik Jeon, Se- eun Yoon, and Julian McAuley. 2024. Calibration- Disentangled Learning and Relevance- Prioritized Reranking for Calibrated Sequential Recommendation. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (Boise, ID, USA) (CIKM '24). 973- 982. doi:10.1145/3627673.3679728[27] Michael Jugovac, Dietmar Jannach, and Lukas Lerche. 2017. Efficient optimization of multiple recommendation quality factors according to individual user tendencies. Expert Systems with Applications 81 (2017), 321- 331. doi:10.1016/j.eswa.2017.03.055[28] Marius Kaminskas and Derek Bridge. 2016. Diversity, Serendipity, Novelty, and Coverage: A Survey and Empirical Analysis of Beyond- Accuracy Objectives in Recommender Systems. ACM Trans. Interact. Intell. Syst. 7, 1, Article 2 (Dec. 2016), 42 pages. doi:10.1145/2926720[29] Mesut Kaya and Derek Bridge. 2019. A Comparison of Calibrated and Intent- Aware Recommendations. In Proceedings of the 13th ACM Conference on Recommender Systems (Copenhagen, Denmark) (RecSys '19). 151- 159. doi:10.1145/3298689.3347045[30] Barbara Kitchenham and Stuart Charters. 2007. Guidelines for performing systematic literature reviews in software engineering. Technical Report EBSE- 2007- 01. School of Computer Science and Mathematics, Keele University.[31] Jon Kleinberg, Emily Ryu, and Éva Tardos. 2024. Calibrated Recommendations for Users with Decaying Attention. In Algorithmic Game Theory: 17th International Symposium, SAGT 2024 (Amsterdam, The Netherlands). 443- 460. doi:10.1007/978- 3- 031- 71033- 0_25[32] Anastasiia Klimashevskaia, Mehdi Elahi, Dietmar Jannach, Lars Skjærven, Astrid Tessem, and Christoph Trattner. 2023. Evaluating The Effects of Calibrated Popularity Bias Mitigation: A Field Study. In Proceedings of the 17th ACM Conference on Recommender Systems (Singapore, Singapore) (RecSys '23). 1084- 1089. doi:10.1145/3604915.3610637[33] Anastasiia Klimashevskaia, Mehdi Elahi, Dietmar Jannach, Christoph Trattner, and Lars Skjærven. 2022. Mitigating Popularity Bias in Recommendation: Potential and Limits of Calibration Approaches. In Third International Workshop on Algorithmic Bias in Search and Recommendation (Bias 2022) at ECIR '22. Stavanger, Norway. doi:10.1007/978- 3- 031- 09316- 6_8

[34] Anastasiia Klimashevskaia, Dietmar Jannach, Mehdi Elahi, and Christoph Trattner. 2024. A survey on popularity bias in recommender systems. User Modeling and User- Adapted Interaction 34, 5 (July 2024), 1777- 1834. doi:10.1007/s11257- 024- 09406- 0[35] Dominik Kowald, Gregor Mayr, Markus Schedl, and Elisabeth Lex. 2023. A Study on Accuracy, Miscalibration, and Popularity Bias in Recommendations. In Advances in Bias and Fairness in Information Retrieval, Ludovico Boratto, Stefano Faralli, Mirko Marras, and Giovanni Stilo (Eds.). 1- 16. doi:10.1007/978- 1- 031- 37249- 0_1[36] Deepak Kumar, Tessa Grosz, Navid Rekabsaz, Elisabeth Greif, and Markus Schedl. 2023. Fairness of recommender systems in the recruitment domain: an analysis from technical and legal perspectives. Frontiers in Big Data Volume 6 - 2023 (2023). doi:10.3389/fdata.2023.1245198[37] Oleg Lesota, Stefan Brandl, Matthias Wenzel, Alessandro B Melchiorre, Elisabeth Lex, Navid Rekabsaz, and Markus Schedl. 2022. Exploring Cross- group Discrepancies in Calibrated Popularity for Accuracy/Fairness Trade- off Optimization. In MORS@RecSys.[38] Oleg Lesota, Gustavo Escobedo, Yashar Deldjoo, Bruce Ferwerda, Simone Kopelnik, Elisabeth Lex, Navid Rekabsaz, and Markus Schedl. 2023. Computational Versus Perceived Popularity Miscalibration in Recommender Systems. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR '23). 1889- 1893. doi:10.1145/3539618.3591964[39] Yunqi Li, Michiharu Yamashita, Hanxiong Chen, Dongwon Lee, and Yongfeng Zhang. 2023. Fairness in job recommendation under quantity constraints. In AAAI- 23 Workshop on AI for Web Advertising. https://pike.psu.edu/publications/aaai23- fair.pdf[40] Kun Lin, Masoud Mansoury, Farzad Eskandanian, Milad Sabouri, and Bamshad Mobasher. 2024. Beyond Static Calibration: The Impact of User Preference Dynamics on Calibrated Recommendation. In Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization (Cagliari, Italy) (UMAP Adjunct '24). 86- 91. doi:10.1145/3631700.3664869[41] Malte Ludewig, Michael Jugovac, and Dietmar Jannach. 2017. A Light- Weight Approach to Recipient Determination When Recommending New Items. In Proceedings of the Recommender Systems Challenge 2017 (RecSys Challenge '17). Article 3. doi:10.1145/3124791.3124795[42] Malte Ludewig, Michael Jugovac, and Dietmar Jannach. 2017. A Light- Weight Approach to Recipient Determination When Recommending New Items. In Proceedings of the Recommender Systems Challenge 2017 (Como, Italy) (RecSys Challenge '17). Article 3, 6 pages. doi:10.1145/3124791.3124795[43] Sean M. McNee, John Riedl, and Joseph A. Konstan. 2006. Being Accurate is Not Enough: How Accuracy Metrics Have Hurt Recommender Systems. In CHI '06 Extended Abstracts on Human Factors in Computing Systems (Montréal, Québec, Canada) (CHI EA '06). 1097- 1101. doi:10.1145/1125451.1125659[44] Fan Mo, Xin Fan, Chongxian Chen, Changhao Bai, and Hayato Yamana. 2024. Sampling- based epoch differentiation calibrated graph convolution network for point- of- interest recommendation. Neurocomputing 571 (2024), 127140. doi:10.1016/j.neucom.2023.127140[45] Mohammadmehdi Naghiaei, Mahdi Dehghan, Hossein A. Rahmani, Javad Azizi, and Mohammad Aliannejadi. 2024. Personalized Beyond- accuracy Calibration in Recommendation. In Proceedings of the 2024 ACM SIGIR International Conference on Theory of Information Retrieval (Washington DC, USA) (ICTIR '24). 107- 116. doi:10.1145/3664190.3672507[46] Mohammadmehdi Naghiaei, Hossein A. Rahmani, Mohammad Aliannejadi, and Nasim Sonboli. 2022. Towards Confidence- aware Calibrated Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22). 4344- 4348. doi:10.1145/3511808.3557713[47] Zahra Nazari, Praveen Chandar, Ghazal Fazelnia, Catherine M. Edwards, Benjamin Carterette, and Mounia Lalmas. 2022. Choice of Implicit Signal Matters: Accounting for User Aspirations in Podcast Recommendations. In Proceedings of the ACM Web Conference 2022 (WWW '22). 2433- 2441. doi:10.1145/3485447.3512115[48] Jinoh Oh, Sun Park, Hwanjo Yu, Min Song, and Seung- Taek Park. 2011. Novel Recommendation Based on Personal Popularity Tendency. In 2011 IEEE 11th International Conference on Data Mining. 307- 516. doi:10.1109/ICDM.2011.110[49] Gustavo Ortega, Rodrigo Sousa, and Marcelo Manzato. 2024. Evaluating Zero- Shot Large Language Models Recommenders on Popularity Bias and Unfairness: A Comparative Approach to Traditional Algorithms. In Anais Estendidos do XXX Simpósio Brasileiro de Sistemas Multimidia e Web (Juiz de Fora/MG). 45- 68. doi:10.5753/webmedia_estendido.2024.244310[50] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt- Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. 10 pages.[51] Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. 1994. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Proceedings of the 1994 ACM Conference on Computer Supported Cooperative Work. 175- 186. [52] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor. 2015. Recommender Systems Handbook (2nd ed.). Springer- Verlag.

[53] Dimitris Sacharidis, Kyriakos Mouratidis, and Dimitrios Kleftogiannis. 2019. A Common Approach for Consumer and Provider Fairness in Recommendations. In Proceedings of the 13th ACM Conference on Recommender Systems (RecSys '19). http://ceur- ws.org/Vol- 2431/paper1. pdf[54] Andre Sacilotti, Rodrigo Ferrari de Souza, and Marcelo Garcia Manzato. 2023. Counteracting popularity- bias and improving diversity through calibrated recommendations. In International Conference on Enterprise Information Systems - ICEIS. doi:10.5220/0011846000000467[55] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item- based collaborative filtering recommendation algorithms. In Proceedings of the 10th International Conference on World Wide Web (Hong Kong) (WWW'01). 285- 295. doi:10.1145/371920.372071[56] Sinan Seymen, Himan Abdollahpouri, and Edward C. Malthouse. 2021. A Constrained Optimization Approach for Calibrated Recommendations. In Fifteenth ACM Conference on Recommender Systems (Amsterdam, Netherlands) (RecSys '21). 607- 612. doi:10.1145/3460231.3478857[57] Rodrigo Souza and Marcelo Manzato. 2024. A Two- Stage Calibration Approach for Mitigating Bias and Fairness in Recommender Systems. In Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing (Avila, Spain) (SAC '24). 1659- 1661. doi:10.1145/3505098.3636092[58] Josef Starychfojtu and Ladislav Peska. 2020. SmartRecepies: Towards Cooking and Food Shopping Integration via Mobile Recipes Recommender System. In Proceedings of the 22nd International Conference on Information Integration and Web- Based Applications; Services (Chiang Mai, Thailand) (iiWAS '20). 144- 148. doi:10.1145/3428757.3429096[59] Josef Starychfojtu and Ladislav Peska. 2021. SmartRecepies: Towards Cooking and Food Shopping Integration via Mobile Recipes Recommender System. In Proceedings of the 22nd International Conference on Information Integration and Web- Based Applications & Services (Chiang Mai, Thailand) (iiWAS '20). 144- 148. doi:10.1145/3428757.3429096[60] Harald Steck. 2018. Calibrated Recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys '18). 154- 162. doi:10.1145/3240323.3240372[61] Célina Treuillier, Sylvain Castaignes, Ozlem Özgöbek, and Armelle Brun. 2024. Beyond Trade- offs: Unveiling Fairness- Constrained Diversity in News Recommender Systems. In Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization (Cagliari, Italy) (UMAP '24). 143- 148. doi:10.1145/3627043.3659571[62] Robin Ungruh, Karlijn Dinnissen, Anja Volk, Maria Soledad Pera, and Hanna Hauptmann. 2024. Putting Popularity Bias Mitigation to the Test: A User- Centric Evaluation in Music Recommenders. In Proceedings of the 18th ACM Conference on Recommender Systems (RecSys '24). 169- 178. doi:10.1145/3640457.3688102[63] Saul Vargas, Pablo Castells, and David Vallet. 2011. Intent- oriented diversity in recommender systems. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1211- 1212. doi:10.1145/2009916.2010124[64] Saul Vargas, Pablo Castells, and David Vallet. 2012. Explicit relevance models in intent- oriented information retrieval diversification. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). 75- 84. doi:10.1145/2348283.2348297[65] Sanne Vrijenhoek, Gabriel Béndict, Mateo Gutierrez Granada, Daan Odijk, and Maarten De Rijke. 2022. RADio - Rank- Aware Divergence Metrics to Measure Normative Diversity in News Recommendations. In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys '22). 208- 219. doi:10.1145/3523227.3546780[66] Chenyang Wang, Yankai Liu, Yuanqing Yu, Weizhi Ma, Min Zhang, Yiqun Liu, Haitao Zeng, Junlan Feng, and Chao Deng. 2023. Two- sided Calibration for Quality- aware Responsible Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems (Singapore, Singapore) (RecSys '23). 223- 233. doi:10.1145/3604915.3608799[67] Wenjie Wang, Fuli Feng, Xiangqun He, Xiang Wang, and Tat- Seng Chua. 2021. Decconfounded Recommendation for Alleviating Bias Amplification. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (Virtual Event, Singapore) (KDD '21). 1717- 1725. doi:10.1145/3447548.3467249[68] Eva Zangerle and Christine Bauer. 2022. Evaluating Recommender Systems: Survey and Framework. ACM Comput. Surv. 55, 8, Article 170 (Dec. 2022), 58 pages. doi:10.1145/3556556[69] Xing Zhao, Ziwei Zhu, Majid Alfifi, and James Caverlee. 2020. Addressing the Target Customer Distortion Problem in Recommender Systems. In Proceedings of The Web Conference 2020 (Taipei, Taiwan) (WWW '20). 2969- 2975. doi:10.1145/3366423.3380065[70] Xing Zhao, Ziwei Zhu, and James Caverlee. 2021. Rabbit Holes and Taste Distortion: Distribution- Aware Recommendation with Evolving Interests. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW '21). 888- 899. doi:10.1145/3442381.3450099[71] Yuying Zhao, Yu Wang, Yi Zhang, Pamela Wisniewski, Charu Aggarwal, and Tyler Derr. 2024. Leveraging opposite gender interaction ratio as a path towards fairness in online dating recommendations based on user sexual orientation. In Proceedings of the Thirty- Eighth AAAI Conference on Artificial Intelligence and Thirty- Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI'24/IAAI'24/EEAI'24). Article 2515, 9 pages. doi:10.1609/aaai.v38i20.30263

[72] Cai- Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005. Improving recommendation lists through topic diversification. In Proceedings of the 14th International Conference on World Wide Web (WWW '05). 22- 32. doi:10.1145/1060745.1060754