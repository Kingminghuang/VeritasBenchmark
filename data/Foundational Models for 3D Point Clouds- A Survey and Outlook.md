# Foundational Models for 3D Point Clouds: A Survey and Outlook

Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, Yunpeng Li

Abstract—The 3D point cloud representation plays a crucial role in preserving the geometric fidelity of the physical world, enabling more accurate understanding and interaction with complex 3D environments. While humans naturally comprehend the intricate relationships between objects, their spatial arrangements, and variations through a multisensory system, artificial intelligence (AI) systems have yet to fully replicate this capacity. To bridge this gap, it becomes essential to incorporate multiple modalities, such as images, text, audio, and point clouds. Models that can seamlessly integrate and reason across these modalities are known as foundation models (FMs). The development of FMs for 2D modalities, such as images and text, has seen significant progress, driven by the abundant availability of large- scale datasets. However, the 3D domain has lagged due to the scarcity of labelled data and high computational overheads. In response, recent research has begun to explore the potential of applying FMs to 3D tasks, overcoming these challenges by leveraging existing 2D knowledge. Additionally, language, with its capacity for abstract reasoning and description of the environment, offers a promising avenue for enhancing 3D understanding through large pre- trained language models (LLMs). Despite the rapid development and adoption of FMs for 3D vision tasks in recent years, there remains a gap in comprehensive and in- depth literature reviews. This article aims to address this gap by presenting a comprehensive overview of the state- of- the- art methods that utilize FMs for 3D visual understanding. We start by reviewing various strategies employed in the building of various 3D FMs. Then we categorize and summarize use of different FMs for tasks such as perception tasks. Finally, the article offers insights into future directions for research and development in this field. This survey is intended to serve as a structured guide for researchers and practitioners seeking to delve into this emerging area of study, providing both a summary of existing knowledge and a roadmap for future exploration. To complement this survey, we provide a curated list of relevant papers on the topic: https://github.com/vgtengane/Awesome- FMs- in- 3D

Index Terms—Point Clouds, 3D Vision, Foundational Models, Vision- Language Models, Large Language Models, Multi- Model Models.

# 1 INTRODUCTION

In the ongoing race to develop artificial intelligence (AI) systems that think and behave like humans, a crucial factor is the ability to navigate and understand the threedimensional (3D) world around us. For AI systems to be effectively deployed in real- world environments, they must possess a robust sense of 3D world [1]. The 3D world can be represented in various formats, including depth images, meshes, volumetric grids, and point clouds [2]. Among these, point clouds are commonly used, consisting of a collection of points in a 3D coordinate system [3].

3D point clouds represent a fundamental paradigm in the domain of spatial data representation [4]. They serve as a pivotal data structure in several fields, including computer vision, robotics, autonomous vehicles, augmented reality, and many more [5]. In computer vision, point clouds enable the precise modelling of real- world scenes, facilitating tasks such as object detection, scene understanding, and 3D reconstruction [2]. Similarly, point clouds play a crucial role in perception and navigation in robotics and autonomous vehicles, aiding in obstacle detection, environment mapping, and path planning [6]. Furthermore, in augmented reality applications, point clouds serve as the backbone to overlay virtual objects on a physical world, enhancing user experiences and interactions [7]. Overall, the versatility and richness of information encapsulated within point clouds make them indispensable tools for 3D understanding and interaction.

Despite the crucial role of point clouds, their utilisation poses significant challenges. First, the process of collecting 3D datasets incurs substantial costs and time investments due to the complexities involved in capturing spatial information [8]. Additionally, annotating point cloud data with ground truth labels for tasks such as object recognition, semantic segmentation, and reasoning is labour- intensive and requires specialised expertise [9]. Training large- scale models on extensive datasets demands considerable computational resources and infrastructure, often necessitating high- performance computing systems [10]. Moreover, point cloud datasets are inherently sparse, lacking semantic information about objects or scenes, despite capturing geometric details [11].

All these challenges have prompted researchers to explore crucial questions: Can we leverage other available data modalities, such as images, text, and audio, to enhance understanding of 3D data, using models that can extract their features? Furthermore, can we bridge the gaps of limited data, annotations, and semantic information without the need for extensive data acquisition and costly model training?

This inquiry led to foundational models (FMs). The term "Foundation Model", introduced in [12], refers to deep learning models trained on extensive datasets, often

using self- supervision at scale. These models exhibit unprecedented adaptability across diverse tasks and domains, characterised by pre- training [13], generalizability, adaptability through transfer learning [14], large scale in both model and data dimensions, and self- supervised learning nature.

Although the fundamental components of foundation models (FMs), such as neural networks and transfer learning, have existed for many years, their significant advancements have recently surged in natural language processing (NLP) with the emergence of large language models (LLMs) such as BERT and GPT- 3 [15, 16]. Following their success in NLP, similar progress has been made in computer vision (CV). Vision- language models (VLMs), such as CLIP [17], exemplify this trend, trained on large image- text datasets, demonstrates remarkable generalisability across various downstream tasks [18, 19]. Models like SAM [20] for segmentation tasks further extend this concept to applications such as class- agnostic segmentation, which can be adapted to various domains, including medical image segmentation [21] and 3D vision [22].

In the quest to comprehend 3D worlds more effectively, leveraging additional modalities such as images, text, and audio alongside FMs has driven the development of various methods 1. For example, one line of approaches involves constructing 3DFMs by utilising 2DFMs [23, 24]. Another avenue of research uses these 2DFMs for tasks, including point cloud classification [15, 18], tasks such as segmentation [26, 27], and object detection [28, 29]. Furthermore, with the advent of LLMs [30, 31, 32] becoming open source, several methodologies have emerged that use these models for 3D understanding at both object [33, 34] and scene levels [35, 36]. Although LLMs are inherently designed for text- based reasoning, they are adapted for 3D tasks through integration with vision- based models. For instance, embeddings generated by LLMs from textual descriptions or instructions can be aligned with features from 3D models, enabling tasks such as visual grounding [37], 3D captioning [38], and 3D question- answering [39].

Despite the rapid evolution and widespread adoption of 2DFMs in the realm of 3D vision tasks, the literature lacks an in- depth summary of available methodologies. To address this gap, we present a comprehensive and structured guide, with the aim of providing a definitive resource for researchers and practitioners alike.

Taxonomy This survey presents a detailed analysis of 2DFMs tailored for 3D point cloud understanding. Designed to benefit researchers including those new and experienced in the field, it offers a structured taxonomy to navigate essential concepts, illustrated in Fig. 1. We begin by laying a solid foundation through a detailed discussion of key topics, including point clouds, available datasets, uni- modal and multi- modal models, and downstream adaptation. Sec. 3 then delves into various initial efforts to build 3DFMs, using 2DFMs. Subsequently, in Sec. 4, we examine the applications of these 2D- and 3DFMs to address various 3D tasks such as classification, segmentation, and detection. Similarly, Sec. 5 explores the application of 2D- and 3DFMs together with LLMs for 3D tasks. Throughout, we summarise methods and provide insights into their performance on different datasets used, Sec. 6 offers an outlook on current limitations and future directions, concluding our review in Sec. 7.

![](https://cdn-mineru.openxlab.org.cn/extract/3430f59e-2fc1-4af3-894b-c60360873657/3edf43c91540d40cd756c653a5b31358ae029bc16143141579765d1510231e5e.jpg)  
Fig. 1. Taxonomy of foundational models for 3D point clouds.

Scope Our survey focusses on examining FMs specifically for 3D point clouds. These FMs encompass both unimodal models tailored for texts, commonly known as LLMs, such as LLaMa [30], GPT- 3 [16], and Vacuna [31], as well as their multi- modal counterparts such as CLIP [17], SAM [20], ImageBind [40] and their variants [41] and variants of LLMs such as LLaVa [42], MiniGPT- 4 [43]. We exclude methods that use 2DFMs for tasks such as generation, manipulation, or rendering, as they have been extensively covered in the existing literature. Furthermore, we do not delve into applications within specific domains such as medical imaging or remote sensing, as these warrant dedicated survey papers. Instead, we offer a broader overview of the grounding work present in the literature, which can be readily applied across various domains.

Related surveys We compare this work with existing surveys in the 3D literature. Guo et al. [44] present a comprehensive review of deep learning methods for 3D point clouds. Similarly, [5, 2, 45] provide detailed analyses but focus exclusively on models based on transformer architectures. Several works summarise methods for 3D object detection in autonomous driving only [6, 46, 47, 48] and do not cover general use cases. Additionally, these works are outdated, as there have been significant advancements utilising pre- trained large models for 3D understanding. Awaise et al. [49] summarise 2DFMs for computer vision tasks, but do not review 3D use cases. Other works such as [7, 50] have limited scopes; for instance, [7] only provides a summary of self- supervised methods for point clouds, and [70] focusses on label- efficient methods for point clouds. In

contrast, this survey aims to provide a comprehensive list of methods, to the best of our knowledge, that utilise 2D/3D FMs to solve various 3D downstream tasks.

Features This survey represents the first comprehensive exploration of the landscape of FMs for 3D point- cloud learning, filling a significant gap in the current literature and intended to serve as a starting guide for both newcomers and seasoned researchers in the domain. The key features of this survey include:

- Background on 3D Vision Tasks and Datasets: Essential background on point clouds and an overview of diverse datasets for training and evaluation, highlighting their key characteristics and challenges.- Discussion of FMs and Key Concepts: A concise explanation of FMs and essential terminology to ensure clarity in understanding their applications across various use cases.- Comprehensive Analysis of Methods: A detailed review of existing methods, including comparisons with alternatives, provides the reader with a clear understanding of their strengths, limitations, and applications.

Contributions The contributions of this work are as follows:

Comprehensive Background Explanation: We provide an introduction about point clouds, various available datasets for 3D point cloud understanding, and the FMs and important terminology. This background sets the stage for understanding the methodologies discussed in this survey.

Structured Taxonomy: We present a structured taxonomy that offers clarity and ease of understanding for both new researchers and those seeking deeper insights into current trends in the field. Our taxonomy groups methods based on different tasks, adaptation strategies, and other important factors, facilitating better organisation and comprehension of the surveyed literature.

Insightful Discussion on Future Prospects: In addition, we offer an insightful discussion on future prospects based on the papers discussed in this work. This discussion covers aspects related to datasets, effective methods for adapting these models for 3D tasks, and other emerging trends in the field.

By offering a comprehensive synthesis of FMs, taxonomy, datasets, and methodologies, this survey serves as a valuable guide for researchers, practitioners, and enthusiasts alike, with the aim of advancing the field of 3D world understanding.

# 2 BACKGROUND

This section serves as an introductory overview, designed to provide readers with essential background and familiarise beginners with key concepts necessary to understand the context of this survey. First, we explain what 3D point clouds are and how they're represented (Sec. 2.1). Following this, we will briefly introduce the available 3D point cloud datasets, including object- level, scene- level, and their extended versions (Sec. 2.2). Next, we will explain the pretraining technique, emphasising its significance in enhancing model performance (Sec. 2.3). Then, we introduce large uni- modal multi- modal models (Sec. 2.5), focussing on their applications, impact, and relevance to 3D point cloud learning. Finally, we discuss the downstream adoption technique, which plays a critical role in adapting pre- trained models to new tasks (Sec. 2.6). We refer the readers to Fig. 2 for an illustration of the interconnections of the topics covered in this section.

![](https://cdn-mineru.openxlab.org.cn/extract/3430f59e-2fc1-4af3-894b-c60360873657/7e2c818f4de3d43ebc71574953dd1fea8442e12f0455f31843603fc79963b0d2.jpg)  
Fig. 2. A general pipeline of using FMs for 3D vision tasks.

# 2.1 Point Clouds

3D data can take on diverse forms, including multi- view images, meshes, volumetric displays, voxel grids, depth maps, and notably point clouds. Point clouds emerge as particularly prevalent representations for 3D data. A point cloud, denoted as  $\mathbf{P}$ , constitutes a compilation of 3D vectors (points), expressed as  $\mathbf{P} = \{p_{1},p_{2},p_{3},\ldots ,p_{N}\}$ , where each vector is defined as a point  $p_{i} = \{\mathbf{C}_{i},\mathbf{F}_{i}\}$ . In this context,  $\mathbf{C}_{i}\in \mathbb{R}^{1\times 3}$  signifies the 3D coordinates  $(x_{i},y_{i},z_{i})$  of the point, and  $\mathbf{F}_{i}$  encompasses the attributes of the characteristic of the point such as RGB values, intensity, normal vectors, etc. These attributes are discretionary and subject to variation depending on factors such as the 3D sensor used, the data collection process, and specific application requirements. This data is often captured using 3D scanners like LiDAR or RGB- D sensors, potentially incorporating colour information from RGB cameras. Notably, distinct from images represented as matrices, point clouds are unordered sets. Therefore, processing such data necessitates a permutation- invariant approach to ensure consistent output, regardless of the ordering of the same point cloud. The adaptability and versatility of point clouds make them indispensable for capturing and processing 3D spatial information across diverse applications.

# 2.2 3D Datasets

Here, we summarise commonly used datasets for point cloud understanding tasks such as classification, detection, and segmentation, along with recently developed multimodality 3D datasets that address the data requirements of large models. We categorise these datasets into three groups: object- level, scene- level, and their extensions for 3D language understanding tasks.

# 2.2.1 Object Level Datasets

2.2.1 Object Level DatasetsBuilding a large- scale, realistic 3D database is both costly and challenging. An approach is to leverage controllable data synthesis. For instance, ShapeNet [51] offers 51,300 CAD models across 55 categories, while ModelNet40 [52] includes 12,311 models spanning 40 categories. Additionally, datasets such as 3D- FUTURE [53] and ABO [54] enhance this collection with CAD models that feature intricate geometry and detailed textures. However, the gap between synthetic and real- world objects continues to drive the demand for extensive, real- world 3D object datasets. To this end, DTU [55] and BlendedMVS [56] provide photorealistic data intended for multi- view stereo benchmarks. ScanObjectNN [57], derived from scanned indoor environments, includes approximately 15,000 coloured point cloud objects across 15 categories. GSO [58] offers 1,030 scanned objects with fine- grained geometry and texture for 17 common household items, while AKB- 48 [59], focused on robotics, provides 2,037 articulated models across 48 categories. CO3D [60] presents 40,000 object- centric video clips with annotated point clouds generated by COLMAP. Finally, the OmniObject3D [61] dataset comprises 6,000 3D objects, including meshes, textures, and multi- view images across 190 everyday categories.

# 2.2.2 Scene Level Datasets

Scene- level datasets are essential for a wide range of applications in 3D domain. Several datasets have emerged for both indoor and outdoor environments. For example, Matterport3D [62] consists of low- resolution reconstructions derived from panoramic RGB- D images and includes semantic annotations. RealEstate10k [63] features camera poses corresponding to 10 million frames extracted from approximately 80,000 video clips gathered from around 10,000 YouTube videos. ARKitScenes [64] enhances ground- truth geometry by providing box annotations for 17 object classes. ScanNet [8] offers 3D reconstructions and annotations at scale, comprising 1,503 RGB- D sequences from 707 unique scenes recorded using an iPad mounted with a structure sensor. Building on the ScanNet dataset, ScanNet200 [65] focusses on the recognition of 200 annotated classes, while ScanNet++ [9] expands this with 460 scenes, 280,000 captured DSLR images, and over 3.7 million iPhone RGB- D frames.

In outdoor domain, SemanticKITTI [66] provides dense point- wise annotations for the complete 360- degree field of view captured by automotive LiDAR. Paris- CARLA3D [67] supports semantic, instance, and scene completion tasks in dense point clouds for outdoor mapping. nuScenes [68] features a comprehensive suite of autonomous vehicle sensors, including 6 cameras, 5 radars, and 1 LiDAR, all with a 360- degree field of view. It comprises 1,000 scenes, each 20 seconds long, fully annotated with 3D bounding boxes for 23 classes and 8 attributes.

# 2.2.3 Modified 3D Datasets

The datasets highlighted above do not incorporate a understanding of textual information. To effectively employ large language models for 3D understanding, it is necessary to develop 3D- language or multimodal datasets. These datasets are built upon earlier datasets such as ScanNet [69], Matterport3D [62], ShapeNet [51], and nuScenes [70]. In the following, we highlight datasets that have been developed on top of these foundations.

Cap3D [33] utilises 660,000 objects from Objaverse [71] to consolidate multi- view 2D image captions of 3D objects. Text2Shape [72] leverages human- annotated data from ShapeNet, creating templates for generative text- to- 3D tasks. SceneVerse [73] compiles 68,000 annotated scenes with 2.5 million vision- language pairs generated through 3D scene graphs and LLMs to support object and scene captioning. nu- Caption [74] annotates 420,000 LiDAR scans from nuScenes using GPT- 4 [75] to provide scene descriptions and object relationships. Building on nu- Caption, nu- Grounding [74] facilitates visual grounding tasks with 280,000 question- answer pairs. ScanRefer [76] introduces natural language grounding for 3D scenes, featuring over 51,000 annotated referring expressions in 800 ScanNet scenes. ReferIt3D [77] refines this task by focussing on disambiguation in scenes with multiple object instances. Lastly, Multi3DRefer [78] extends ScanRefer by providing referring descriptions for zero- target, single- target, and multiple- target, which accommodate more complex queries.

For a more exhaustive review of such extended datasets, please refer to [79], which summarises an extensive range of datasets in the 3D vision- language domain.

# 2.3 Pre-Training

2.3 Pre- TrainingThe concept of pre- training [80] has been widely used since the early days of computer vision, particularly with the advent of large datasets like ImageNet [81]. When substantial data availability and computational resources are accessible, pre- training involves training a large model on an extensive dataset using supervised, unsupervised, self- supervised, or weakly supervised methods, as evidenced by significant work in the field [82, 17, 41, 20]. The main idea is that the model learns diverse features, enabling it to create representations that are effective for a wide range of tasks. Pre- training acts as a universal feature extractor, providing representations characterised by adaptability and versatility. These serve as a valuable starting point for various tasks, particularly those suffering from low data availability.

# 2.4 Large Uni-Modal Models

Recent research indicates that scaling pre- trained models by increasing both model size and dataset size enhances capacity and generalisation to downstream tasks. While scaling typically retains similar architectures and pre- training tasks, larger models exhibit distinct behaviours. For instance, the 330M parameter BERT [15] and the 1.5B parameter GPT- 2 [83] demonstrate different capabilities, with larger models exhibiting surprising abilities [84]. GPT- 3 [16], for example, excels at few- shot tasks via in- context learning, a milestone beyond the capabilities of GPT- 2. These models employ the same pre- training strategies outlined in Sec. 2.3, but scale up in model size, dataset size, and training strategies.

This section focusses on large models typically pretrained on single- modality datasets. The transformer architecture has revolutionised pre- training in NLP, enabling efficient handling of sequential data, including text, audio,

video, and images, outperforming earlier LSTM- based models. In CV, models such as Vision Transformer (ViT) [85], MAE [86], and DINOv1/v2 are trained on extensive datasets such as ImageNet21K [87] and JFT [88]. In NLP, large models trained solely on language datasets include BERT [15], GPT- 1/2/3 [16], and LLaMA [30], which are applied to various downstream tasks in both CV and NLP. This paper discusses only the models used by the methods it covers; see [89] for a comprehensive and updated list.

# 2.5 Large Multi-Modal Models

For a long time, machine learning models operated within multi- modal tasks. To fully harness their potential, models must integrate multiple modalities and generalise beyond predefined objectives. Recently, many multi- modal systems have been developed to achieve this. CLIP [17] was the first model capable of generalising to image classification tasks with zero- /few- shot learning. The first large- scale models in this domain were BLIP [90] and Flamingo [91], which further scaled the model size and enabled open- ended responses, marking a significant milestone in the multi- modal domain similar to GPT- 3 [16] in NLP. Visual instruction tuning quickly became a prominent training paradigm in the multi- modal domain, alongside the use of parameter- efficient fine- tuning (PEFT) [92] techniques to adapt these pre- trained (or FMs) for downstream tasks. Another notable direction of research is SAM [20], pre- trained on segmentation tasks, which demonstrated impressive performance due to its class- agnostic nature and has been applied across various domains, from medical imaging [93] to 3D point clouds [36].

# 2.6 Downstream Task Adaptation

Downstream task adaptation is achieved through transfer learning [14] via two main approaches: full fine- tuning, which updates all model parameters, and feature extraction, which adds a task- specific head. Both require significant computational resources and large datasets. With the rise of large models containing billions of parameters, fully fine- tuning has become impractical. To address this, PEFT techniques have been developed, introducing only a few new parameters to effectively adapt the models. For example, prompt- tuning [94] learns a small set of vectors as soft prompts before input text, while low- rank adaptation (LoRA) [95] reduces the number of new weights by learning low- rank matrices. Combined with quantisation methods such as QLoRA [96], these techniques further decrease memory requirements compared to standard half- precision weights. These strategies enable efficient adaptation of large pre- trained models to a wide range of downstream tasks.

# 3 BUILDING 3D FOUNDATIONAL MODELS

Following the definition by [12], this section provides an overview of initial efforts toward building 3DFMs. These approaches employ diverse pre- training strategies and modalities such as RGB images, text, audio, and point clouds to construct robust representations for 3D data. Previous work on 3DFMs includes ConvNet- based methods such as PointContrast [134], OcCo [135, 136] and DepthContrast [137], alongside transformer- based approaches such as Point- BERT [138], Point- MAE [139], Voxel- MAE [140], and SimIP [141]. However, these methods fall outside of the scope of this survey as they do not utilise 2DFMs for building 3DFMs and will not be discussed further.

Given the rapid development of methods aimed at building 3DFMs, it can be challenging to grasp the full scope at once. To facilitate comprehension, we categorise these methods into three groups based on their use of 2DFMs. The first category includes methods that build on top of 2DFMs as a base model. The second group consists of dual encoders, where one encoder is pre- trained 2DFMs and the other encodes 3D data. Lastly, we group methods that build on triplet alignment to learn point cloud representations through alignment between text, image, and point cloud encoders. Fig. 3 provides a high- level architecture overview and Tab. 1 summarises these methods. Each of these categories is further detailed in the following subsections.

# $\S 3.1$  Direct Adaptation

![](https://cdn-mineru.openxlab.org.cn/extract/3430f59e-2fc1-4af3-894b-c60360873657/077d5377cb5a3d21a0af667fd9701af7f6cbfd3f892e76dce651441a06f12ee2.jpg)  
Fig. 3. Methods of building 3D models for representation learning by leveraging 2DFMs, categorised by how FMs are used.

# 3.1 Direct Adaptation

Both 2D images and 3D point clouds serve as intuitive visual representations of the physical worlds. Despite significant differences in their underlying representations, they convey similar fundamental visual concepts, and human vision can seamlessly comprehend both forms. Building on this premise, this section explores methods that leverage 2DFMs—such as ConvNets [82], ViTs [85] pre- trained on ImageNet- 1K/21K [87, 85], and CLIP [17]—to represent point clouds with minimal adjustments, using the same 2DFMs as a base model and without using an entirely new point backbone.

TABLE 1 Summary of methods that aim to build a 3D foundational (or representation) model by utilising 2D foundational models.  

<table><tr><td rowspan="2">Types</td><td rowspan="2">Method</td><td rowspan="2">Venue</td><td rowspan="2">2D Backbone</td><td rowspan="2">3D Backbone</td><td rowspan="2">3D Pre-train Dataset</td><td colspan="2">Zero-shot Classification Acc</td><td>Part Segmations</td></tr><tr><td>ModelNet40</td><td>ScanObjectNN</td><td>ShapeNetPart (mIOU)</td></tr><tr><td rowspan="5">Direct Adaptation</td><td>Image2Point [97]</td><td>ECCV&#x27;22</td><td>ResNet [82], VIT [85]</td><td>-</td><td>ImageNet1K [87]</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Pix4Point [98]</td><td>3DV&#x27;24</td><td>VIT [85]</td><td>-</td><td>ImageNet1K [87]</td><td>-</td><td>-</td><td>86.8</td></tr><tr><td>PointCLIP [25]</td><td>CVPR&#x27;22</td><td>CLIP [17]</td><td>-</td><td>CLIP-dataset [17]</td><td>20.18</td><td>14.12</td><td>31</td></tr><tr><td>PointCLIPV2 [18]</td><td>ICCV&#x27;23</td><td>VIT [85]</td><td>-</td><td>CLIP-dataset [17]</td><td>29.71</td><td>18.18</td><td>49.5</td></tr><tr><td>PCEExpert [99]</td><td>Arxiv&#x27;23</td><td>CLIP [17], Control-Net [102], StableD-ResNet [103]</td><td>-</td><td>ShapeNet [100]</td><td>-</td><td>-</td><td>-</td></tr><tr><td rowspan="6">Dual Encoders</td><td>PPKT [105]</td><td>Arxiv&#x27;21</td><td>ResNet [82]</td><td>SparseUNet [106]</td><td>ScanNet [69]</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CrossPoint [107]</td><td>CVPR&#x27;22</td><td>ResNet [82]</td><td>PointNet [4], DGCNN [108]</td><td>ShapeNet [100]</td><td>-</td><td>-</td><td>85.5</td></tr><tr><td>CLIP2Point [109]</td><td>ICCV&#x27;23</td><td>CLIP [17]</td><td>Transformer [110]</td><td>ShapeNet [100]</td><td>33</td><td>23.32</td><td>-</td></tr><tr><td>PointCLIPKD [111]</td><td>Arxiv&#x27;22</td><td>CLIP [17], ClipCap [112]</td><td>Transformer [110]</td><td>ScanNetv2 [69]</td><td>-</td><td>-</td><td>86.1</td></tr><tr><td>I2P-MAE [113]</td><td>CVPR&#x27;23</td><td>CLIP [17]</td><td>MAE (Transformer) [81]</td><td>ShapeNet [100]</td><td>-</td><td>-</td><td>86.76</td></tr><tr><td>Bridge3D [114]</td><td>NeurIPS&#x27;23</td><td>DINOV2 [115], Tag2Text [116]</td><td>MAE (Transformer) [81]</td><td>ScanNet [69], SUN RGB-D [117]</td><td>-</td><td>-</td><td>-</td></tr><tr><td rowspan="10">Triplet Alignment</td><td>ULIP [23]</td><td>CVPR&#x27;23</td><td>CLIP [17], BLP2</td><td>PointNet [4]</td><td>Triplet Collection form ShapeNet [100]</td><td>60.4</td><td>48.5</td><td>-</td></tr><tr><td>ULIP2 [24]</td><td>Arxiv&#x27;23</td><td>[90]</td><td>PointBERT [118], Point-Net [119]</td><td>Triplet Collection form ShapeNet [100], Objectverse [71]</td><td>69.7</td><td>-</td><td>-</td></tr><tr><td>CG3D [120]</td><td>CVPR&#x27;23</td><td>CLIP [17]</td><td>PointNet [119]</td><td>ShapeNet [100]</td><td>50.6</td><td>25.6</td><td>-</td></tr><tr><td>CLIP2Scene [123]</td><td>CVPR&#x27;23</td><td>CLIP [17]</td><td>PointMLP [122]</td><td>NuScenes [70]</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Text4Point [125]</td><td>PR&#x27;23</td><td>CLIP [17]</td><td>SPVConv [124], Sparse-Net [106]</td><td>ScanNet [69]</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CLIP2 [126]</td><td>CVPR&#x27;23</td><td>CLIP [17]</td><td>SparsetNet [106]</td><td>Triplet Collection [126]</td><td>37.8</td><td>39.1</td><td>-</td></tr><tr><td>OpenShape [128]</td><td>Arxiv&#x27;23</td><td>CLIP [17], BLP [90]</td><td>PointNet [118], Point-Net [119]</td><td>Combination of ShapeNet [100], 3D-Future [53], ABO [54], Objectverse [71]</td><td>85.3</td><td>47.2</td><td>-</td></tr><tr><td>Multi-CLIP [129]</td><td>Arxiv&#x27;23</td><td>CLIP [17], EVA [131], DINO [132]</td><td>PointNet++ [127]</td><td>ScanRefer [76], ScanNet [69]</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Uni3D [130]</td><td>Arxiv&#x27;23</td><td>CLIP [17]</td><td>ViT [85]</td><td>Combination of ShapeNet [100], 3D-Future [53], ABO [54], Objectverse [71]</td><td>87.3</td><td>63.9</td><td>78.2</td></tr><tr><td>JM3D [133]</td><td>Arxiv&#x27;23</td><td>CLIP [17]</td><td>PointNet [4], PointERT [118], PointMLP [122]</td><td>ShapeNet [100]</td><td>55.5</td><td>47.5</td><td>83.6</td></tr></table>

Image2Point [97] demonstrates how commonly used image- pre- trained models, such as 2D ConvNet and ViT, can be effectively adapted for various 3D tasks. Specifically, pre- trained 2D ConvNet and ViT can be transformed into projection- based, voxel- based, and transformer- based point- cloud models by either copying or inflating weights. The approach focusses primarily on 3D ConvNet, which utilises voxel input by inflating 2D kernels into 3D ones derived from 2DFMs. To these transformed models, linear input and output layers are added. During fine- tuning on target point cloud datasets, only the input/output layers and batch normalisation layers are adjusted while the pretrained model weights remain unchanged. It uses ResNet50 [82] trained on ImageNet- 1K [87] as the base model; this approach outperforms previous techniques of that time.

Similarly Pix4Point [98] learns 3D representations on top of 2DFMs by introducing a point tokenizer at the input and a task- specific head or decoder at the output. Instead of inflating weights as in Image2Point, this method focusses on transformer architecture, which can process various input types through tokenization. The 3D model is initialised from a standard pre- trained transformer and incorporates a relative progressive tokenizer that gradually converts point clouds into tokens using graph convolutions. The model employs a ViT [85] trained in a self- supervised manner as the base model to learn the point cloud representation.

In contrast to previous approaches that rely solely on point clouds as input, PCExpert [99] leverages images to guide point cloud learning. Inspired by the mixture- ofmodality- expert [142], which encodes different modalities using modality- specific experts, PCExpert employs a pretrained ViT to encode both point clouds and images. This is achieved by separating the feed forward network (FFN) and layer normalisation within each transformer [110] layer for each modality. Additionally, a projection head is added to the output for point cloud processing. The model incorporates a shared multi- head self- attention mechanism to ensure knowledge transfer between the image and point cloud modalities, while the separate FFNs capture the unique features of each modality. During pre- training, the ViT parameters are frozen, and only the parameters related to point clouds and the projection head are updated.

PointCLIP [25] leverages the CLIP [17] framework for point cloud understanding. It bridges the modality gap by projecting point clouds onto predefined image planes to generate multi- view depth maps, allowing for feature extraction with CLIP's pre- trained visual encoder, and the text encoder is used for zero- shot classification without dedicated 3D training; however, the method lags behind state- of- the- art methods. Additionally, a learnable interview adapter is introduced to consolidate features from multiple views, and fine- tuned while keeping the CLIP encoders frozen, leading to competitive performance with state- of- the- art methods. PointCLIPV2 [18] improves upon PointCLIP by integrating the strengths of CLIP and GPT3 [16] for unified 3D open- world understanding. Enhances zero- and few- shot tasks, including 3D classification, part segmentation, and object detection, without requiring 3D training data.

DiffCLIP [101] adopts a novel approach for learning point cloud representations using the CLIP [85] model. Since CLIP was originally trained on image- text pairs, there is a significant domain gap between point clouds and images. To address this, DiffCLIP leverages a diffusion model integrated with ControlNet [102]. Given a point cloud as input, on the image side of CLIP, it generates multi- view depth

maps. These depth maps are then transformed through a style transfer process, guided by stable diffusion [103] and ControlNet, into photo- realistic 2D RGB images, which are fed into the frozen image encoder of CLIP. On the text side, for zero- shot tasks, hand- crafted prompts are applied to both the stable diffusion module and the frozen text encoder. For few- shot tasks, a style- prompt generation module is introduced, generating prompts alongside class labels that are fed into the frozen text encoder of CLIP. Both the frozen image and text encoders are used to produce feature representations of images and text, which are subsequently processed through a multi- modal fusion block. This block computes cosine similarity between the modalities and includes an additional ConvNet block to fuse logits from each view for the final output.

# 3.2 Dual Encoders

This section highlights methods that utilise 2DFMs for 3D understanding by adding a separate point cloud backbone in parallel to the 2D backbone. These methods are typically trained using contrastive learning, knowledge distillation from the 2D model, and later employ the point cloud backbone for feature extraction in downstream tasks.

The pixel- to- point knowledge transfer (PPKT) [105] method leverages 2D information by mapping pixel- level and point- level features into a shared embedding space through a differentiable back- projection function. However, the output of the 2D backbone lacks pixel- level resolution. To address this, an upsampling layer, a modified version of the projection layer used in contrastive learning frameworks [143], is introduced. This upsampling layer restores the spatial resolution of the image feature map to its original size. Simultaneously, the point cloud is processed by a 3D backbone to extract 3D features. During pre- training, the 2D model is frozen, while only the 3D backbone is trained using point- pixel noise contrastive estimation (NCE) [144] loss.

Similar to PPKT, CrossPoint [107] adopts a dual- branch architecture- one branch processes point clouds, while the other processes images. The point cloud branch establishes intra- modal correspondence by ensuring robustness to point cloud augmentations, meaning it can handle variations in the data effectively. Meanwhile, the image branch creates cross- modal correspondence by applying a contrastive loss between the rendered 2D image features and the point cloud prototype features. CrossPoint jointly trains the model by combining the learning objectives of both branches. For downstream tasks, only the point cloud backbone is used, and the image branch is discarded.

CLIP2Point [109] focusses on aligning depth maps rendered from 3D point clouds with CLIP visual features. This is done using a self- supervised pre- training scheme that incorporates both intra- modality and cross- modality contrastive learning to align depth features with CLIP's visual features. During the process, a pair of rendered depth maps is constructed by randomly selecting camera views for each 3D point cloud and modifying their view distances. The method applies a combination of NT- Xent [145] and InfoNCE loss [146] to pairs of depth features from the depth encoder and between the depth and image features. The image encoder is kept frozen during training to ensure that depth features align with the CLIP visual encoder. With the aligned depth encoder, the point cloud features are then compared with the CLIP text features for downstream tasks 2.

Yao et al. [111] proposed PointCLIPKD, a point cloud learning method using CLIP. Like CLIP2Point, it has both point cloud and image backbones. However, unlike CLIP2Point, it adds a ClipCap [112] module on the image side and directly takes point clouds instead of depth maps on the point cloud side. The point cloud backbone receives two inputs: point cloud tokens and concept query sets. These concept tokens are extracted layer by layer using cross- attention between the image and point cloud branches. Knowledge is transferred by aligning the concept tokens with the image embedding through a distillation loss [147].

Bridge3D [114] bridges 2D and 3D vision by pre- training a 3D model using features, semantic masks, and captions extracted from multiple 2D FMs in a self- supervised manner. Semantic masks from the 2D models guide the masking and reconstruction process for MAE [86], focussing attention on important foreground objects. At the scene level, an image captioning FMs, such as Tag2Text [116], bridges the gap between point clouds and text for scene- level knowledge distillation. Additionally, object- level knowledge distillation leverages precise object masks and semantic text from 2D FMs.

Zhang et al. [113] proposed I2P- MAE, a method for transferring knowledge from images to points for self- supervised pre- training of point clouds in MAE [86] style. This method projects point clouds into multi- view depth maps to obtain 2D semantics of 3D shapes. Instead of randomly masking point clouds for the MAE input, a saliency map from 2DFMs guides the point cloud masking, focussing on key visible structures to comprehend the global 3D shape. During training, 2D semantics are reconstructed from visible point tokens, ensuring that both 3D spatial patterns and high- level semantic information from 2D models are captured, enhancing 3D representation.

# 3.3 Triplet Alignment

This section reviews methods that focus on aligning point clouds, images, and text together. Most of these approaches are built on top of CLIP, incorporating a new encoder for point clouds and aligning it with the frozen encoders of CLIP. Subsequently, the point cloud encoder is utilised to extract point cloud features for downstream tasks.

ULIP [23] seeks to unify representations by aligning all three modalities- image, text, and point cloud. The inputs to ULIP consist of objects represented as triplets of images, text, and point clouds. The image and text features are extracted from the pre- aligned frozen vision and language encoders of CLIP [17], while the 3D features are extracted from a randomly initialised point cloud encoder. Contrastive losses [143] are applied to align the 3D feature of an object with its image and text features during pre- training. The point cloud encoder is then used to extract point

cloud features for downstream tasks. ULIP shows strong performance as an end- to- end method for point cloud pretraining. However, the textual descriptions in ULIP are relatively shallow, lacking the fine- grained information and details necessary for a comprehensive understanding. To address this limitation, ULIP- 2 [24] introduces the use of large vision- language models (LVLMs), such as BLIP- 2 [90], which improve scalability and reduce data requirements.

CLIP Goes 3D (CG3D) [120] was proposed around the same time as ULIP and shares similar goals and training strategies. However, CG3D observes a distribution shift in 2D rendered images within the CLIP 3D setting. To address this, CG3D introduces a learnable prompt and incorporates additional training parameters in the image input space to align CLIP with the 3D pre- training dataset. The effectiveness of the pre- trained 3D encoder is demonstrated in tasks such as open scene understanding and retrieval.

Similarly, CLIP2Scene [123] uses CLIP's image and text encoders to train a 3D encoder in cross- modal learning tasks. It regularises a 3D network by incorporating CLIP's semantic and visual information. CLIP2Scene introduces semantic consistency regularisation (SCR) and spatial- temporal consistency regularisation (STCR) modules. For SCR, CLIP's text semantics are used to select positive and negative point samples for conflict- free contrastive learning. For STCR, CLIP's image pixel features impose a soft consistency constraint on temporally coherent point features. The trained 3D encoder is applied to indoor and outdoor datasets for downstream semantic segmentation tasks.

TextPoint [140] focuses on integrating the language modality into 3D vision models, bridging the gap between point clouds and text by leveraging 2D images to learn point cloud representations. The model establishes correspondences between images and point clouds. Using CLIP's well- aligned image and text features, point cloud features are implicitly aligned with text embeddings. To enable the 3D model to leverage text information from CLIP's pretrained text encoder, the embedding spaces of the CLIP 2D encoder and the 3D encoder are aligned through a pixel- voxel contrastive loss [143]. If needed, the 2D and 3D decoder features are further aligned for downstream tasks like semantic segmentation, instance segmentation, and object detection.

Drawing inspiration from CLIP's applications in open- world understanding, CLIP2 [126] pre- trains a 3D encoder to acquire a general representation of 3D data. Instead of projecting 3D point clouds into images, CLIP2 directly aligns the 3D space with the raw text to learn a 3D representation in an open- world setting. This model collects one million triplets of point clouds, images, and text by employing 2DFMs. The feature space alignment of the three modalities is optimised jointly, encompassing the semantic- label text- 3D correlation and the instance- level image- 3D correlation. The model demonstrates promising results for zero- shot recognition on indoor and outdoor datasets, especially for long- tail categories.

OpenShape [128], similar to ULIP [23], focusses on improving the understanding of open- world 3D shapes. To achieve this, OpenShape scales up the 3D dataset by integrating four public 3D shape datasets, covering a wide range of categories. In addition, it improves the quality of the text through filtering, captioning, and image retrieval strategies to automatically refine and improve text descriptions. The 3D backbone is scaled to suit large 3D datasets, and data resampling is handled using hard negative mining, improving the model's discrimination ability. OpenShape is tested on a zero- shot classification task, demonstrating its efficacy in open- world recognition.

Although most methods discussed so far have focused on pre- training models for tasks such as classification, detection, and segmentation, Multi- CLIP [129] facilitates learning language- grounded and transferable 3D scene representations. Multi- CLIP aligns 3D scene features with 2D multi- view images and text embeddings using a contrastive objective in the CLIP space. The model is evaluated in challenging downstream tasks, such as 3D visual question answering (3D- VQA) and 3D situated question answering (3D- SQA).

Uni3D [130] addresses the limitations of existing 3D pretraining methods, offering a unified and scalable framework for large- scale 3D representation learning. By leveraging a 2D ViT as the 3D encoder and initialising it with the best 2D priors, Uni3D undergoes end- to- end pre- training to align 3D point cloud features with image- text aligned features. With a billion parameters, a million 3D shapes, and ten million images paired with 70 million texts, Uni3D explores scalability in 3D representation learning. It utilises abundant 2D pre- trained models like Eva [131] and DINO [132], and image- text aligned models such as CLIP [17] and scaling law [149], demonstrating continuous performance improvements as the model scales.

Applying 2D alignment strategies to 3D data presents challenges such as information degradation, insufficient synergy between 3D, image, and text features, and underutilisation of fine- grained details. JM3D [133] addresses these issues, information degradation by integrating point cloud, text, and image information using a structured multimodal organiser which generates a continuous sequence of multi- view rendered images and establishes a hierarchical text tree. The inadequate synergy is addressed by seamlessly integrating textual and visual modalities, which leads to a unified representation through joint multi- modal alignment. This approach is extended in JM3D- LLM [133], embedding 3D representations into LLMs for tasks such as image- 3D retrieval and zero- shot 3D classification, providing more granular information.

# 4 ADAPTING 2DFMS FOR 3D

For the categories in this area, most of the methods overlap with those described in Section 3. To minimise the repetition, we focus on the methods that use 2DFM for a specific task even with pre- training, i.e. even if it is pre- trained, it can only be used for that specific task.

# 4.1 2DFMs for 3D Classification

This section reviews methods for point cloud classification that leverages 2DFMs.

Similarly, CALIP [151] adapts CLIP models without any learnable parameters or training. It uses a parameter- free

TABLE 2 Summary of methods for point cloud classification on ModelNet40 and ScanObjectNN in different shots.  

<table><tr><td rowspan="2">Method</td><td colspan="5">ModelNet40</td><td colspan="5">ScanObjectNN</td></tr><tr><td>0-shot</td><td>4-shot</td><td>8-shot</td><td>16-shot</td><td>All</td><td>0-shot</td><td>4-shot</td><td>8-shot</td><td>16-shot</td><td>All</td></tr><tr><td>PointCLIP [25]</td><td>20.18</td><td>77.07</td><td>81.35</td><td>87.2</td><td>-</td><td>15.38</td><td>46.14</td><td>50</td><td>55.5</td><td>-</td></tr><tr><td>P2P [150]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>89.3</td></tr><tr><td>CALIP [151]</td><td>21.47</td><td>-</td><td>-</td><td>-</td><td>-</td><td>16.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MvNet [152]</td><td>-</td><td>84.88</td><td>86.1</td><td>91.16</td><td>-</td><td>-</td><td>59.47</td><td>68.77</td><td>75.36</td><td>-</td></tr><tr><td>InvJoint [153]</td><td>-</td><td>78.95</td><td>83.61</td><td>88.97</td><td>-</td><td>-</td><td>48.1</td><td>53.36</td><td>57.02</td><td>-</td></tr></table>

cross- modal attention mechanism to align spatial visual and textual features and introduces non- parametric and bidirectional modifications, similar to PointCLIP. In the non- parametric approach, linear layers are omitted, relying solely on well- aligned CLIP features to compute attention maps. In the bidirectional approach, both visual and textual features are updated simultaneously, with visual features guided by category semantics from text and textual features made visual- aware and image- conditional.

P2P [150] offers a different approach, introducing a prompting technique inspired by VPT [154] to transfer 2D pre- trained knowledge into the 3D domain. It transforms point clouds into colourful images via geometry- preserved projection and geometry- aware colouring. These images are processed by a frozen pre- trained image model to extract features, which are then used by downstream task- specific heads. This method promotes bidirectional knowledge flow between points and pixels, retaining geometric information through projection and infusing colour information back into the colourless point clouds via the interaction between the geometry- aware colouring module and the pre- trained image model.

In contrast to the text- prompting methods, MvNet [152] uses multi- view projected features to prompt a pre- trained vision model. The point cloud is encoded into multiple views, and a multi- view prompt vision fusion module interchanges and merges information across views through an attention mechanism. This produces prompts that leverage prior knowledge from a 2DFM for 3D few- shot learning.

Adapting 2D pre- trained models for point cloud recognition through multi- view projection is effective, but they can introduce challenges such as incomplete geometric information and rendering artifacts. To address this, InvJoint [153] proposes leveraging both 2D and 3D domains simultaneously. By analysing the confusion matrices of 2D and 3D models, the authors demonstrate that each model is confused by different classes, limiting the effectiveness of late fusion. Their method selects joint hard samples using a Gaussian mixture model (GMM) [155], followed by a joint learning module that captures collaborative representations across domains through an invariant feature selector.

Table 4.1 shows the overview of these methods and their performance on ModelNet40 and ScanObjectNN in different shots.

# 4.2 2DFMs for 3D Segmentation

In this section, we explore 2DFMs such as CLIP and SAM for point cloud segmentation. As most FMs- focused works revolve around open- vocabulary or open- world 3D segmentation, we summarise methods specifically targeting these areas. Additionally, we briefly discuss works that utilise 2DFMs for part segmentation.

# 4.2.1 3D Part Segmentation

The introduction of VL models such as CLIP and segmentation models like SAM has unleashed significant advancements in 2D CV tasks. Leveraging this progress, recent work has extended the capabilities of these 2DFMs to tackle 3D part segmentation challenges. PointCLIP V2 [18] stands as an early example, achieving zero- shot part point cloud segmentation solely through 2D vision and text backbones. PartSLIP [26] further builds on this foundation by utilising CLIP, an enhanced version of CLIP to localise 2D objects based on free- form text and employ a 3D voting and grouping module, effectively transitioning multi- view 2D bounding boxes into 3D semantic and instance segmentation. ZenoPS [157] takes a similar approach, utilising SAM and GLIP alongside multi- view perspectives to achieve zero- shot 3D part segmentation. It expands 3D groups from local to global levels, introduces a merging algorithm for part- level groups, integrates a two- dimensional checking mechanism to identify the best matching 3D part for each 2D box, and incorporates a class non- highest vote penalty function to refine the vote matrix. In the case of PartDistill [27], a departure from direct use of the 2D backbone is observed. Instead, it adopts a knowledge distillation mechanism. This involves a teacher network employing a 2DFMs for 2D predictions and a student network learning from these predictions while extracting geometric features from multiple 3D shapes to facilitate 3D part segmentation.

In addition to 3D part segmentation, researchers have harnessed the capabilities of robust 2DFMs, such as SAM, to segment more intricate scenes. One line of approaches focuses on accurately segmenting 2D frames using various scene deconstruction techniques. For instance, SAM3D [22] utilises multi- view images as input, employing SAM to generate 2D masks which are then mapped to 3D. This method iteratively merges adjacent point clouds with the bidirectional merging approach until obtaining 3D masks for the entire scene. Similarly, SAM- Graph [158] over- segments input mesh/point cloud into superpoints, constructing a graph structure based on adjacency. SAM annotates nodes and edges, while node features aggregate multi- view SAM backbone features and edge weights rely on intersection ratios between superpoint masks. A graph neural network refines the graph, executing a graph cut for instance segmentation. SA13D [159] is another variant that utilises a sim

TABLE 3 3D Zero-Shot part segmentation results on the PartNetE [156] dataset. Object category mloU  $(\%)$  results are shown, with 'Overall' denoting results across all 45 categories  

<table><tr><td>Methods</td><td>Venue</td><td>Overall (45)</td><td>Chair</td><td>Clock</td><td>Dishwasher</td><td>Door</td><td>Knife</td><td>Rfererator</td><td>Table</td><td>Box</td><td>Bucket</td><td>Lighter</td><td>Oven</td><td>Per</td><td>Safe</td><td>Stapler</td><td>Suitcase</td><td>Toaster</td></tr><tr><td>PointClip V2 [18]</td><td>ICCV&#x27;23</td><td>16.1</td><td>30.8</td><td>0.9</td><td>6.9</td><td>20.7</td><td>26.2</td><td>9.3</td><td>6.1</td><td>32.5</td><td>3.5</td><td>13.0</td><td>7.8</td><td>16.9</td><td>3.6</td><td>20.0</td><td>5.6</td><td>0.3</td></tr><tr><td>PartSlip [26]</td><td>CVPR&#x27;23</td><td>134.4</td><td>77.1</td><td>17.1</td><td>30.5</td><td>35.7</td><td>31.7</td><td>35.7</td><td>46.0</td><td>60.5</td><td>22.1</td><td>34.3</td><td>34.1</td><td>5.6</td><td>14.8</td><td>26.4</td><td>50.8</td><td>10.7</td></tr><tr><td>ZeroPS [157]</td><td>arXiv&#x27;23</td><td>139.3</td><td>73.1</td><td>29.7</td><td>47.2</td><td>27.5</td><td>49.6</td><td>47.7</td><td>41.6</td><td>53.9</td><td>74.8</td><td>47.2</td><td>27.2</td><td>18.5</td><td>20.0</td><td>38.7</td><td>62.9</td><td>17.8</td></tr><tr><td>PartDistill [27]</td><td>CVPR&#x27;24</td><td>139.9</td><td>74.1</td><td>23.6</td><td>18.6</td><td>41.1</td><td>59.2</td><td>25.2</td><td>50.2</td><td>69.7</td><td>16.8</td><td>37.3</td><td>34.2</td><td>15.7</td><td>18.2</td><td>65.1</td><td>43.2</td><td>11.4</td></tr></table>

TABLE4 Performance of 3D instance segmentation on ScanNet [8], ScanNet++ [9] datasets in mAP and mAP25,and mAP50 scores.  

<table><tr><td rowspan="2">Method</td><td rowspan="2">Venue</td><td colspan="3">ScanNet</td><td colspan="3">ScanNet++</td></tr><tr><td>mAP</td><td>mAP50</td><td>mAP25</td><td>mAP</td><td>AP50</td><td>AP25</td></tr><tr><td>SAM3D [22]</td><td>ICCV&#x27;23</td><td>13.7</td><td>29.7</td><td>54.5</td><td>8.3</td><td>17.5</td><td>33.7</td></tr><tr><td>SAM-graph [158]</td><td>arXiv&#x27;23</td><td>13.1</td><td>32.3</td><td>59.1</td><td>12.9</td><td>25.3</td><td>49.6</td></tr><tr><td>SAI3D [159]</td><td>CVPR&#x27;24</td><td>15.8</td><td>43.5</td><td>62.3</td><td>17.1</td><td>31.1</td><td>45.6</td></tr><tr><td>SAMPro3D [160]</td><td>arXiv&#x27;23</td><td>12.2</td><td>45.6</td><td>65.7</td><td>18.9</td><td>33.7</td><td>51.6</td></tr><tr><td>PointSeg [161]</td><td>arXiv&#x27;24</td><td>33.8</td><td>59.7</td><td>78.1</td><td>30.2</td><td>45.7</td><td>64.2</td></tr></table>

ilar technique for instance segmentation without the graph neural network. Another line of approaches is learning high- quality 3D points to prompt SAM using 3D projections, such as SAMPro3D [160]. Given a 3D point cloud and multiple posed RGB frames, SAM is applied on RGB frames to segment 3D scenes without requiring training data. Furthermore, PointSeg [161] adopts a two- branch prompts learning structure, featuring bidirectional matching- based prompts generation, iterative prompt- refinement, and affinity- aware merging, enhancing FMs' ability to improve 3D segmentation quality.

# 4.2.2 Open-Vocabulary Segmentation

TABLE5 Performance summary on ScanNet200 [65].Three splits: Head, common, tail; \*The metrics of mloU (mAcc).  

<table><tr><td>Method</td><td>Venue</td><td>mAP</td><td>mAP50</td><td>mAP25</td><td>Head</td><td>Common</td><td>Tail</td></tr><tr><td colspan="8">Feature Distillation Methods</td></tr><tr><td>OpenScene [162]</td><td>CVPR&#x27;23</td><td>11.7</td><td>15.2</td><td>17.8</td><td>13.4</td><td>11.6</td><td>9.9</td></tr><tr><td>CLIP-FO3D [163]</td><td>CVPR&#x27;23</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan="8">3D Captioning Methods</td></tr><tr><td>PLA [164]</td><td>CVPR&#x27;23</td><td>118 (3.1)*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RegionPLC [165]</td><td>CVPR&#x27;24</td><td>9.6 (17.8)*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan="8">3D projection Methods</td></tr><tr><td>SemAbs [166]</td><td>CoRL&#x27;22</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>OpenMask3D [167]</td><td>NIPS&#x27;23</td><td>15.4</td><td>19.9</td><td>23.1</td><td>17.1</td><td>14.1</td><td>14.9</td></tr><tr><td>OpenIns3D [168]</td><td>ECCV&#x27;24</td><td>88</td><td>10.3</td><td>14.4</td><td>16</td><td>6.5</td><td>4.2</td></tr><tr><td>OVIR-3D [169]</td><td>CoRL&#x27;23</td><td>13</td><td>24.9</td><td>32.3</td><td>14.4</td><td>12.7</td><td>11.7</td></tr><tr><td>Open3DIS [170]</td><td>CVPR&#x27;24</td><td>18.6</td><td>23.1</td><td>27.3</td><td>24.7</td><td>16.9</td><td>13.3</td></tr><tr><td>Segment3D [171]</td><td>ECCV&#x27;24</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Open-YOLO3D [172]</td><td>Arxiv&#x27;24</td><td>24.7</td><td>31.7</td><td>36.2</td><td>27.8</td><td>24.3</td><td>21.6</td></tr></table>

Open- vocabulary (OV) representation methods can naturally address the settings of open- set and open- world. This task extends to open- set classification, introducing an additional generalisation requirement to novel vocabulary, visual properties, and domains. The inherent challenge lies in the limited diversity and scale of 3D- text pairs datasets compared to their 2D counterparts. Training on such 3D data may not adequately prepare AI systems for the complexities of the open 3D world. These methods can be divided into three groups: Feature distillation, Captioning based, and 2D projection methods.

# Feature Distillation

![](https://cdn-mineru.openxlab.org.cn/extract/3430f59e-2fc1-4af3-894b-c60360873657/d7d1bce8de49f5825eef77ca7de942a9e5a4fefc2771cb57569fae128964117c.jpg)  
Fig. 4. High level overview of open vocabulary methods for point clouds using 2DFMs.

Feature Distillation bridges the gap between 2D and 3D representations by aligning language- associated features with 3D representations. For example, OpenScene [162] uses 2D pixel labels for 3D comprehension, avoiding the need for explicit 3D annotations. It computes dense features for 3D points, embedding them with text and image pixels in the CLIP feature space. By aligning 3D points with pixels from posed images, it trains a 3D network to embed points using CLIP pixel features. This allows open vocabulary queries on 3D points, facilitated by multi- view fusion of pixel features. A sparse 3D ConvNet is trained to extract features from point cloud geometry, reducing differences to aggregated pixel features. The final step combines 2D and 3D features into a hybrid strategy, enabling robust 3D scene

understanding queries, capturing concepts like affordances, materials, and functions in tasks such as segmentation, detection, and exploration.

Similarly, CLIP- FO3D [163] eliminates the need for point cloud label, image pixel supervision, or pseudo- caption. Instead, it directly transfers CLIP's knowledge to 3D models without annotations. CLIP- FO3D addresses the challenge of applying CLIP to 3D vision by extracting pixel- level features, overcoming the limitations of global features and 3D scene complexity. Building on MaskCLIP, it crops images at multiple scales to preserve object semantics and extracts pixel- level features from each RGB view. Using the 3D multi- view projection scheme, these features are mapped onto the point cloud, serving as training targets aligned with the CLIP feature space. Through feature distillation, the 3D model minimises the gap between learnt and target features, enabling CLIP- FO3D to extract open- world 3D scene representations. In particular, it excels in annotation- free OV 3D semantic segmentation, using text embeddings of class names as classification weights.

3D Captioning methods focus on building 3D- text pairs and training a 3D instance proposal network alongside a contrastive OV model. This approach aligns the predicted proposals with their corresponding text captions. PLA [164] employs an image- captioning model to generate captions aligned with 3D data, enhancing representation through text embedding while training a 3D network to learn language- aware embeddings from these pseudo- captions. The challenge lies in the complex object compositions in 3D scene- level data, making it difficult to link objects to their corresponding words in the captions. Unlike object- centric images, multi- view images of a 3D scene are related by geometry, enabling the creation of hierarchical point- caption pairs at scene, view, and entity levels. These pairs provide coarse- to- fine supervision, facilitating the learning of visual- semantic representations for OV 3D scene understanding tasks like semantic and instance segmentation. The PLA point- language association paradigm is versatile for such tasks without requiring a task- specific design.

In RegionPLC [165], it generates 3D language pairs at the region level by combining various region- level captions from 2DFMs, such as image captioning, dense captioning, and detection models. These captions are mapped to 3D regions to create paired data. The framework includes a region- aware point- discriminative contrastive learning mechanism that prevents confusion between unrelated points in close proximity, enhancing the discriminatory power of point- wise embeddings. It also normalises the learning process across regions of varying sizes, making feature learning more robust. The final model integrates with language models for open- ended 3D reasoning, achieving grounding capabilities without the need for task- specific training data.

3D Projection methods obtain the 2D features and/or mask using methods such as CLIP [83] and SAM [21], and are back- projected into 3D. An example of this, SemAbs [166], addresses visual- semantic reasoning in open- world 3D scene understanding tasks using 2D VLMs. Its hypothesis posits that, while open- world visual- semantic reasoning necessitates exposure to internet- scale datasets, 3D spatial and geometric reasoning is tractable even with a limited synthetic dataset and could generalise more effectively if learnt in a semantic- agnostic manner. Instead of learning the complete concept, the model focusses on abstract concepts. To achieve this level of abstraction, it leverages relevancy maps extracted from CLIP, using representations that remain agnostic to semantic labels. An evaluation was conducted on the completion of the semantic scene of the OV and the location of visually obscured objects. Despite training only the 3D network on a restricted synthetic dataset, the model demonstrates the ability to generalise to any novel semantic labels, vocabulary, visual properties, and domains to which the 2DFMs can effectively generalise.

OpenMask3D [167] build on top of Mask3D, distinguishes itself from existing 3D OV methods by employing an instance- based feature computation approach instead of a point- based one. Given an RGB- D sequence and the corresponding 3D geometry, OpenMask3D predicts 3D object instance masks and computes a mask- feature representation. Its two- stage pipeline includes a class- agnostic mask proposal head and a mask- feature aggregation module. The aggregation module identifies top- k frames with high instance visibility, extracts CLIP features from the best images using a multi- scale, crop- based approach. By aggregating these features across views, they obtain a feature representation for each 3D instance mask. This allows for the retrieval of object instance masks based on similarity to any given query, facilitating open- vocabulary 3D instance segmentation. It shows that the zero- shot nature of feature computation enhances the preservation of information about novel and long- tail objects compared to trained counterparts.

OpenMask3D [168] is a 3D open- vocabulary framework designed for versatile deployment without relying on well- aligned 2D images. It involves three key steps: Mask, Snap, and Lookup. The mask proposal module generates class- agnostic 3D mask proposals, evaluates them with the mask scoring module, and filters invalid masks. Synthetic scene- level images are then generated using calibrated camera poses, minimising rendering needs. These images are processed by 2D OV models, creating a class lookup table to store detected categories. The Mask2Pixel maps project 3D proposals onto 2D images, allowing category assignment during Lookup. The results of multiple views are combined for the initial classification, with refinements to remove unassigned masks.

OVIR- 3D [169] introduces an OV instance retrieval method that ranks 3D instance segments from a point cloud reconstructed using RGB- D video and a language query. By leveraging multi- view fusion and smoothing techniques, it surpasses the limitations of 2D segmentation for more precise 3D object identification. Instead of relying on pixel- level data or additional training, it integrates instance- level information directly into the 3D scene. The method generates 2D object region proposals via a 2D OV detector, followed by data association and filtering to enhance masks and reduce noise. Experiments on video datasets show its efficiency, with a 2D- to- 3D fusion module that achieved around 30 fps and near- instant text query inference.

Methods like OpenMask3D and OVIR- 3D use pre- trained 2D OV models to generate 2D instance masks, which are projected onto 3D point clouds but often suffer from misalignments. Open3DIS [170] overcomes this by

aggregating 2D masks across multiple frames and using 3D- aware feature extraction for precise alignment with text queries. A class- agnostic 3D instance segmenter generates initial 3D proposals, refined by combining 2D masks with superpoints. The refined proposals are integrated with the initial ones, and the Pointwise Feature Extraction module aligns point cloud CLIP features with text embeddings to produce accurate instance masks.

Segment3D [171] introduces a two- stage training framework for class- agnostic 3D segmentation, eliminating the need for manually labelled 3D data. Unlike earlier methods such as SAM3D [22], which rely on complex and errorprone merging processes, Segment3D offers a simplified and direct 3D segmentation pipeline. In the first stage, the model is pre- trained on partial RGB- D point clouds using 2D segmentation masks automatically generated by SAM. These masks, projected from 2D RGB- D images into the 3D domain, enable the model to learn 3D structures by leveraging large- scale RGB- D datasets without manual annotations. In the second stage, the model is fine- tuned on full 3D scenes to bridge the domain gap between partial and complete 3D point clouds. High- confidence mask predictions from the pre- trained model serve as training signals, improving segmentation accuracy on complete 3D reconstructions and enhancing real- world applicability.

Open- YOLO3D [172] presents a more efficient alternative to traditional methods that depend on computationally expensive foundation models such as SAM and CLIP by integrating joint 2D- 3D reasoning using 2D bounding- box predictions. An open- vocabulary 2D object detector generates bounding boxes with class labels for each RGB frame associated with the 3D scene, which eliminates the need for heavy 3D proposal generation and provides faster results than CLIP- based methods. A 3D instance segmentation network then generates class- agnostic instance masks for the point clouds. The predicted bounding boxes are used to create a low granularity label map for each RGB frame, assigning class labels to the bounding box areas. Finally, the 3D point cloud is projected onto these LG label maps using intrinsic and extrinsic parameters, enabling efficient class assignment to 3D instances without the complexity of lifting 2D features into 3D.

# 4.3 2DFMs for 3D Object Detection

This section summarises methods using 2DFMs for 3D object detection, a less explored area compared to semantic segmentation, which provides both detection and segmentation. The emergence of segmentation FMs like SAM, a 2DFM capable of flexible image segmentation, has driven greater progress in 3D segmentation than in object detection.

Inspired by SAM's application in 3D tasks, SAM3D [173] adapts this model for 3D object detection in outdoor scenarios. The approach first projects LiDAR points into colourised bird's eye view (BEV) images using a predefined colour palette, then post- processes these BEV images to optimise them for SAM's requirements. After segmenting these images with SAM, it refines the resulting noisy masks and ultimately predicts 3D bounding boxes with the support of the original LiDAR points.

VFMM3D [174] combines two 2DFMs- SAM for segmentation and DAM for depth estimation- to address chal lenges in monocular 3D object detection. It generates highquality pseudo- LiDAR data, enriching monocular images with semantic and depth information without requiring dataset- specific fine- tuning. A sparsification technique reduces the noise in the pseudo- LiDAR data, improving computational efficiency. Compatible with various LiDAR- based 3D detectors, VFMM3D enhances 3D spatial information by integrating depth and semantic cues.

Traditional approaches to OV 3D detection face substantial challenges due to the scarcity of comprehensive 3D- text datasets, limiting the integration of OV capabilities typically derived from vision- text pairs. To overcome these constraints, FM- OV3D [175] introduces a cross- modal knowledge integration framework, using FM such as SAM, CLIP, and GPT- 3 [16]. This methodology focusses on improving object localisation and recognition in 3D models. For object localisation, FM- OV3D utilises SAM's segmentation capabilities to produce 2D bounding boxes, effectively strengthening 3D object positioning. In terms of recognition, it combines 3D detector point cloud features with textual features generated by GPT- 3 and visual features derived from stable diffusion, unified within CLIP's shared feature space. This alignment of features across modalities enables OV detection by bridging these diverse information domains. Furthermore, the adaptable structure of FM- OV3D supports the application to any OV training set by dynamically generating language and visual prompts for specified classes, expanding its versatility across various tasks.

# 5 ADAPTING 2DFMS +LLMs FOR 3D TASKS

Recent advancements in LLMs have sparked interest in using these pre- trained models for 3D tasks. This approach helps overcome the scarcity of 3D datasets and annotations by leveraging dense descriptions and incorporating language as an additional modality to enhance 3D tasks. We now review both object- level and scene- level approaches, with the latter further categorised by architecture and the integration of 2DFMs and LLMs. In this section, we refer to large vision- language models (LVLMs) for 3D tasks as 3DLVLMs.

# 5.1 Object Level 3DLVLMs

This section will highlight the methods based on LLMs for object- level 3D tasks such as 3D object caption generation, retrieval, question answering, etc.

We start by summarising a simplex method Cap3D [33], a method designed to generate descriptive captions for point clouds by leveraging the BLIP2 [90], CLIP [83], and GPT- 4 [75] models. Cap3D employs a three- stage data processing pipeline to produce high- quality annotations: first, 3D assets are rendered into 2D images; these images are then passed through the BLIP2 model to generate initial captions. Next, the CLIP model filters these captions to ensure relevance and quality by measuring the alignment between the captions and the images. Finally, GPT- 4 consolidates captions across multiple views of the same object, creating cohesive and contextually rich descriptions. This integrated approach harnesses the power of pre- trained models on large- scale text- image and text data to produce accurate and informative captions for 3D assets.

X- InstructBLIP [176] integrates multiple modalities- such as images, text, audio, and video- into LLMs, enabling both single- modal reasoning tasks and cross- modal reasoning across three or more modalities. To achieve this, the paper explores two projection methods for frozen LLMs: Q- Formers [90] and linear projection [42]. In response to the scarcity of instruction- tuning datasets for modalities beyond images, the authors developed a three- stage query- based data augmentation technique that leverages open- source LLMs to extract instruction- tuning data from existing captioning datasets. This approach allows for efficient dataset generation across diverse modalities. Extensive evaluations on 13 benchmarks spanning four modalities reveal that Q- Formers demonstrate strong performance in single- modal tasks and exhibit the versatility to switch flexibly between joint and discriminative reasoning in multi- modal contexts.

GPT4Point [177] is a unified framework designed for point- language understanding and generation. It leverages a BERT- based Point- QFormer [15, 90] to align point- cloud features with textual descriptions, enabling deep integration of 3D geometry in text- based reasoning and generation tasks. This aligned feature representation is then fed into the model for text inference tasks and into a diffusion model for controlled 3D object generation for enhanced geometric and colour fidelity. To address the scarcity of annotated 3D point- language data, the work introduces Pyramid-  $\mathrm{XL},$  a data annotation engine built upon the Objaverse- XL [178] dataset. Pyramid- XL uses VLMs to generate multiview text annotations and organises these annotations into three hierarchical levels of detail, improving accuracy and completeness in 3D object descriptions.

ShapeLLM [34] is a two- stage method aimed at supporting 3D tasks that require interaction with objects, such as object grasping. This method emphasizes multi- level shape understanding, addressing limitations in previous models that often rely on single- view 2D features, resulting in limited shape comprehension. Existing methods, such as ReCon [179], are hindered by scarce pre- training data. To overcome this, the authors introduce  $\mathrm{ReCon + + }$  a 3D backbone that uses multi- view image tokens to capture semantic information across RGB and depth maps. They also propose a crossmodal alignment based on bipartite matching for implicit pose estimation and expand the pre- training dataset and parameters to improve 3D representations. A LLaMa- style finetuning [30] is then applied to align 3D representations with language models.

MiniGPT- 3D [180] addresses the challenges in resourceintensive VI alignment methods. Unlike previous approaches that require significant computational resources, MiniGPT- 3D leverages 2D visual priors to efficiently bridge the gap between 2D and 3D modalities. It involves a four- stage training strategy that cascades through modality alignment steps, progressively refining the connection between point cloud data and language understanding. This method utilizes a mixture- of- expert model that adaptively aggregates features, enhancing efficiency without sacrificing accuracy. To further optimise performance, MiniGPT- 3D employs LoRA [95] and Norm tuning. These techniques minimise the number of trainable parameters to just 47.8 million, a reduction of up to 260 times compared to other methods.

Different to MiniGPT- 3D, GreenPLM [181], a model aimed at achieving 3D- language understanding with minimal 3D data by leveraging extensive text- based training to align with LLMs. Targeting the newly defined 3D data efficient point cloud understanding task, GreenPLM shifts the alignment focus from limited 3D data to abundant text data using the curated T3D dataset, which includes 6 million 3D object descriptions. Its methodology consists of a three- stage training process where the first two stages use only text data to build foundational language skills, while the third stage incorporates minimal 3D data to enhance point- LLMs alignment. Architecturally, GreenPLM employs 0M- Pooling, a cross- attention module that optimises token pooling, allowing the model to align point clouds with LLMs efficiently, even on limited hardware.

LLaNA [182] adopts a different approach by directly embedding neural radiation fields (NeRFs) in LLM for tasks such as NeRF [183] captioning, question- answering, and zero- shot classification. NeRFs, typically MLPs, capture both geometry and photorealistic details of objects, offering a richer alternative to traditional 2D images or point clouds. LLaNA utilises a meta- network encoder to process NeRF weights and map them into the embedding space of a pretrained language model, specifically LLaMA 2 [30], allowing NeRF- language tasks to be performed without rendering NeRFs into 2D images. To train and evaluate LLaNA, the authors introduce a new NeRF- language dataset, generated with automated annotations based on LVLMs in NeRFs from ShapeNet [51], supplemented by a split with manually curated descriptions, establishing a benchmark for NeRF- driven language tasks.

# 5.2 Scene Level 3DLVM

These methods focus on understanding complex real- world scenes using LVLMs. We categorise them into three groups based on how point cloud data is processed (Fig. 5): methods that directly handle point cloud scenes, those that leverage 2D priors like masks or features, and those that decompose point clouds into smaller assets or multi- view images.

# 5.2.1 Direct Scene Encoding

Large Language 3D Assistant (LL3DA [36]), showcases robust instruction- following capabilities in understanding, reasoning, and planning within complex 3D environments. By incorporating both textual instructions and potential visual interactions, LL3DA effectively reduces ambiguities in addressing various tasks across diverse 3D scenes. The model aggregates information from textual instructions, visual prompts, and 3D scenes into a fixed length of learnable querying tokens through an attention mechanism. These querying tokens act as a prefix for the textual instructions, enabling efficient interaction- aware 3D scene embeddings for instruction following.

SIG3D [184] aims to improve the understanding and reasoning of embodied agents in complex 3D environments through effective situational modelling. It begins with estimating the agent's ego- location and orientation using largescale pre- trained language and visual encoders to enhance

![](https://cdn-mineru.openxlab.org.cn/extract/3430f59e-2fc1-4af3-894b-c60360873657/799e185e712fb9b2797d1073700e724ccdc94ecc8f65093f33f9f907c83ba382.jpg)  
Fig. 5. High level overview of scene level methods for 3D understanding, categorised by how point cloud data is handled.

situational awareness from textual descriptions. Then tokenize the textual and 3D scene data, utilising a multimodal transformer with self- attention and cross- attention mechanisms to fuse information from both modalities. To address the expansive search space in 3D environments, they reformulate the situation prediction task as an anchor- based classification problem, where visual tokens serve as anchor points, allowing for robust situational context estimation through regressed position and rotation parameters. Finally, situational alignment is applied to adjust and re- encode the visual tokens with situationally- aware embeddings, enhancing their relevance for 3D tasks.

# 5.2.2 With 2D Priors

A RegionBLIP [35], consisting of three core modules: modal feature extraction, alignment, and LLM comprehension. The extraction module retrieves features from modalities like images and point clouds, focussing on overall rather than fine- grained features, allowing shared encoders for both I- text and P- text data. For images, the frozen CLIP model is utilised, while the Point- BERT [118] model is employed for point clouds. The alignment module is crucial, as LLMs are primarily trained on language data, making the comprehension of image or point cloud inputs challenging. By aligning features with textual descriptions through learnable queries, comprehension is enhanced before inputting them into the LLM, enabling effective processing of aligned features.

3D- LLMs [185] emphasises the challenges of training from scratch due to limited datasets compared to the extensive billion- scale image- language datasets used for 2D FMs. Lacking pre- trained 3D encoders, they propose extracting 3D features from 2D multi- view images. This allows the use of pre- trained image encoders to map features to 3D data, integrating them into VLM backbones. The training process involves building aligned 3D features using three reconstruction methods: direct reconstruction, feature fusion, and neural field [183]. 3D- LLMs leverage frozen 2D VLMs, such as CLIP, for feature extraction. A 3D localisation mechanism is introduced to enhance spatial information absorption by augmenting 3D features with position embeddings and incorporating location tokens into the LLM vocabularies, effectively aligning 3D locations with the language model.

LEO [38] is designed around two principles: it integrates multi- modal inputs- egocentric 2D images, global 3D data, and textual instructions- producing both textual responses and embodied action commands, and it leverages pre- trained LLMs for enhanced performance on downstream tasks. Data is converted into a token sequence for GPT- style autoregressive modeling [16]. Tokenization uses SentencePiece for text, 2D image tokens for egocentric images, and object- centric 3D tokens from Mask3D proposals [10]. Continuous embodied actions are discretised for a unified action space. Various token embeddings are processed before being fed into the Vicuna- 7B LLM [31], which generates responses. Text and 2D tokens are embedded using lookup tables, while 3D tokens are refined with a Spatial Transformer for better 3D relationship capture. LoRA [95] is implemented to manage the alignment of multi- modal tokens while preserving the pre- trained model's knowledge.

SpatialRGPT [186] discusses an approach to constructing a 3D scene graph from a single 3D image, focussing on open- vocabulary detection and segmentation, metric depth estimation, and camera calibration to generate accurate spatial representations. The model employs a visual encoder that integrates monocular depth information into an existing 2D VLM. The construction pipeline includes filtering unsuitable images, identifying objects, and creating a 3D scene graph composed of nodes and edges that represent object instances and their spatial relationships. To train the model, it generates spatially aware question- answer pairs using both template- based and LLM- based methods, resulting in a rich Open Spatial Dataset with millions of images and annotations. The SpatialRGPT architecture combines a visual encoder, a region- feature extractor, and a language model, incorporating depth information to enhance geometric reasoning while maintaining flexibility in handling RGB images and depth data.

# 5.2.3 Point Cloud Decomposition

The Uni3D- LLM [187] approach aligns point clouds with their corresponding images using modality- specific projectors. It employs a two- step process for point cloud alignment: first, objects are extracted with a detection method and then encoded using a pre- trained Point- BERT [15] model, leveraging LLaMA2's [30] cognitive capabilities. For object- level tasks, point cloud data is mapped to textual space, while scene- level tasks utilize position encoding to preserve spatial integrity. Image alignment involves extracting features with multiple pre- trained encoders and addressing occlusion through a top- view representation. The LLM- to- Generation mapping block connects language model outputs to generation models via learnable generative tokens, guiding the diffusion process. The editing process allows modifications to the 3D model using rendered images, ensuring consistency through Gaussian splatting

and instruct- pix2pix techniques. Training consists of two stages: first, focus on the text- to- generation mapping block and then on the perception module using PEFT [92] with the Sphinx [188] model integrated with LLaMA2.

Previous 2D LMMs utilise visual encoders to extract 2D features from images, aligning them with LLMs via projection layers for joint reasoning. LLaVA- 3D [189] enhances this by integrating 2D features into a 3D spatial context, creating 3D patches and employing 3D- aware pooling strategies to compress these patches, leading to a 3D- aware encoding and decoding process. Building on LLaVA [42], which uses a CLIP encoder for 2D patch extraction, we introduce a 3D patch representation that embeds 3D spatial information into 2D features through a two- layer MLP. To manage computational overhead, we implement pooling mechanisms like voxelization pooling, which averages patches in occupied voxels, and farthest point sampling for representative subset selection. The final architecture enables the LLM to process 3D coordinates, generate outputs such as language responses and 3D bounding boxes, using a 3D Coordinate Token for coordinate context and a location token to guide accurate 3D box predictions, facilitating precise object localisation.

# 6 OUTLOOK INTO FUTURE DIRECTIONS

Robust 3DFMs While there has been a significant surge in research aimed at developing 3DFMs by leveraging 2D FMs (see Sec. 3), it is evident that there is significant room for improvement. These models demonstrate proficiency in understanding object- level point clouds; however, they often fall short when tasked with comprehending larger 3D point scenes. For example, models such as PointCLIP [25] and PointMAE- I2P [113] leverage 2DFMs to learn from point clouds. Yet, their downstream applications are primarily limited to small scenes from datasets such as PartNet, posing challenges when attempting to extend their applicability to larger scenes. Similarly, models such as ULIP/- V2 [23] utilise 2D FMs to extract features from pre- trained 3D models, showcasing some advantages in tackling more complex 3D tasks. Despite these advancements, there remains a need to address the limitations associated with the scalability and generalisation of these models to larger and more diverse 3D environments.

Large 3D datasets As we look toward the future of 3DFMs, it becomes increasingly apparent that addressing the scarcity of 3D data compared to the abundance of 2D image datasets is a pressing challenge. The question of how to bridge this gap is a pressing concern in the field. While the Internet is teeming with 2D images, the collection of 3D data remains a daunting task due to the high cost and time- consuming nature of equipment and processes involved in analysing and creating 3D datasets. One potential solution lies in leveraging existing 2D datasets to generate 3D counterparts. While methods like ERFs [183] have been explored for this purpose, they often require significant computational resources and time to render a single scene. However, recent advancements in techniques such as Gaussian splating [190] offer a more computationally efficient approach. By harnessing these methods, researchers and practitioners can expedite the process of collecting large- scale 3D datasets, thus narrowing the gap between the availability of 2D and 3D data. For instance, datasets such as Objaverse [191] have demonstrated the potential of such approaches by providing a vast collection of 3D objects derived from various sources. These efforts exemplify how innovative methodologies can contribute to the rapid expansion of 3D data repositories, ultimately enhancing the development and applicability of future 3D foundational models.

Efficient data and resources When FMs are applied to downstream tasks, they adapt efficiently with fewer parameters and data. However, they face challenges when dealing with complex datasets, such as 3D scenes in real- world environments [79]. To address these issues, we need more advanced methods that emulate human learning, such as adapting to minimal examples and learning from abstract concepts. Although these models show promising adaptability, deploying them on edge devices remains a significant hurdle [43]. The few- shot learning approach aligns with this goal, but is currently limited to object- level scenarios and needs further development to handle more complex scene- level tasks [192, 193].

Continual adaptation FMs have been applied to various learning paradigms, including open- set and open- vocabulary tasks. However, their adaptability is often limited to single downstream datasets, lacking the flexibility to generalise across evolving data or new object classes in dynamic environments. In real- world 3D settings, where new data or object classes continuously emerge, adapting FMs to these changes remains a significant challenge [194]. Although OV methods are useful, continual learning becomes essential in cases where OV is not feasible. For instance, where AI systems need to figure out class labels simply by observing objects or scenes, without relying on predefined descriptions [195].

# 7 CONCLUSION

This paper presents a thorough examination of 2D image and large language pre- trained models, commonly referred to as Vision- Language Models (VLMs), Language Models (LLMs), and Foundation Models (FMs)- with a particular emphasis on their applications in 3D representation and understanding, especially regarding point cloud data. We highlight the transformative potential of FMs for enhancing 3D understanding. Key contributions include the categorisation of these methods based on various tasks and architectures. By establishing a well- structured taxonomy and providing concise introductions to various methodologies, we facilitate meaningful comparisons among them. Our novel grouping strategy and step- by- step explanations aim to cater to researchers at all levels, improving accessibility in this rapidly evolving field. This comprehensive overview not only outlines the current state of 3D representation techniques but also addresses the challenges faced in this domain and offers insights into future research directions, making it a valuable resource for guiding ongoing advancements in 3D understanding.

# REFERENCES

[1] W. D. Heaven. AI begins to understand the 3- D world. MIT Technology Review, December 9 2016. 1

[2] J. Lahoud et al. 3D vision with transformers: A survey. arXiv preprint arXiv:2208.04309, 2022. 1, 2[3] W. contributors. Point cloud. https://en.wikipedia.org/wiki/Point_cloud, 2025. Accessed December 6, 2024. 1[4] C. R. Qi et al. Pointnet: Deep learning on point sets for 3D classification and segmentation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 652- 660, 2017. 1, 6[5] D. Lu et al. Transformers in 3D point clouds: A survey. arXiv preprint arXiv:2205.07417, 2022. 1, 2[6] J. Mao et al. 3D object detection for autonomous driving: A comprehensive survey. Int. J. Comput. Vis. (IJCV), 131(8):1909- 1963, 2023. 1, 2[7] B. Fei et al. Self- supervised learning for pre- training 3D point clouds: A survey. arXiv preprint arXiv:2305.04691, 2023. 1, 2[8] A. Dai et al. ScanNet: Richly- annotated 3D Reconstructions of Indoor Scenes. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. 1, 4, 10[9] C. Yeshwanth et al. Scannet++: A high- fidelity dataset of 3D indoor scenes. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 12- 22, 2023. 1, 4, 10[10] J. Schult et al. Mask3d: Mask transformer for 3D semantic instance segmentation. In Proc. IEEE/RSJ Int. Conf. Robot. Autom. (ICRA), pp. 8216- 8223. IEEE, 2023. 1, 14[11] S. Wang et al. UniBEV: Multi- modal 3D Object Detection with Uniform BEV Encoders for Robustness against Missing Sensor Modalities. In IEEE Intelligent Vehicles Symposium, 2024. 1[12] R. Bommasani et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 1, 5[13] G. E. Hinton et al. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527- 1554, 2006. 2[14] D. N. Perkins and G. Salomon. Transfer of Learning. 1992. 2, 5[15] J. Devlin et al. BERT: Pre- training of Deep Bidirectional Transformers for Language Understanding. In Proc. North Amer. Chapter Assoc. Comput. Linguistics (NAACL), pp. 4171- 4186, 2019. 2, 3, 5, 13, 14[16] T. Brown et al. Language models are few- shot learners. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 33:1877- 1901, 2020. 2, 4, 5, 6, 12, 14[17] A. Radford et al. Learning transferable visual models from natural language supervision. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 8748- 8763, 2021. 2, 4, 5, 6, 7, 8[18] X. Zhu et al. PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open- world Learning. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 2639- 2650, 2023. 2, 6, 9, 10[19] V. Thengane et al. Clip model is an efficient continual learner. arXiv preprint arXiv:2210.03114, 2022. 2[20] A. Kirillov et al. Segment anything. Proc. IEEE Int. Conf. Comput. Vis. (ICCV), 2023. 2, 4, 5[21] Y. Zhang and R. Jiao. How segment anything model (SAM) boost medical image segmentation? arXiv preprint arXiv:2305.03678, 2023. 2, 11[22] Y. Yang et al. Sam3d: Segment anything in 3D scenes. Proc. IEEE Int. Conf. Comput. Vis. (ICCV), 2023. 2, 9, 10, 12[23] L. Xue et al. ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 1179- 1189, 2023. 2, 6, 7, 8, 15[24] L. Xue et al. ULIP- 2: Towards Scalable Multimodal Pretraining For 3D Understanding. Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 2, 6, 8[25] R. Zhang et al. PointCLIP: Point Cloud Understanding by CLIP. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 8552- 8562, 2022. 2, 6, 9, 15[26] M. Liu et al. Partslip: Low- shot part segmentation for 3D

point clouds via pretrained image- language models. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 21736- 21746, 2023. 2, 9, 10[27] A. Umam et al. PartDistill: 3D Shape Part Segmentation by Vision- Language Model Distillation. Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 2, 9, 10[28] X. Bai et al. Transfusion: Robust LiDAR- camera fusion for 3D object detection with transformers. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 1090- 1099, 2022. 2[29] Z. Liu et al. Bevfusion: Multi- task multi- sensor fusion with unified bird's- eye view representation. In Proc. IEEE/RSJ Int. Conf. Robot. Autom. (ICRA), pp. 2774- 2781. IEEE, 2023. 2[30] H. Touvron et al. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 5, 13, 14[31] W.- L. Chiang et al. Vicuna: An Open- Source Chatbot Impressing GPT- 4 with 90% ChatGPT Quality, 2023. 2, 14[32] S. Gunasekar et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. 2[33] T. Luo et al. Scalable 3D captioning with pretrained models. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 36, 2024. 2, 4, 12[34] Z. Qi et al. Shapellm: Universal 3D object understanding for embodied interaction. In Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 214- 238. Springer, 2025. 2, 13[35] Q. Zhou et al. Regionblip: A unified multi- modal pretraining framework for holistic and regional comprehension. arXiv preprint arXiv:2308.02799, 2023. 2, 14[36] S. Chen et al. LL3DA: Visual Interactive Instruction Tuning for Omni- 3D Understanding Reasoning and Planning. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 26428- 26438, 2024. 2, 5, 13[37] J. Roh et al. Language- geferer: Spatial- language model for 3D visual grounding. In Conference on Robot Learning, pp. 1046- 1056, 2022. 2[38] J. Huang et al. An embodied generalist agent in 3D world. Proc. Int. Conf. Mach. Learn. (ICML), 2024. 2, 14[39] S. Ye et al. 3D Question Answering. IEEE Trans. Vis. Comput. Graph. (TVCG), pp. 1- 16, 2022. 2[40] J. Han et al. Imagebind- LLM: Multi- modality instruction tuning. arXiv preprint arXiv:2309.03905, 2023. 2[41] C. Jia et al. Scaling up visual and vision- language representation learning with noisy text supervision. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 4904- 4916, 2021. 2, 4[42] H. Liu et al. Visual instruction tuning. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 36, 2024. 2, 13, 15[43] D. Zhu et al. Minigpt- 4: Enhancing vision- language understanding with advanced large language models. Proc. Int. Conf. Learn. Represent. (ICLR), 2024. 2, 15[44] Y. Guo et al. Deep learning for 3D point clouds: A survey. IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 43(12):4338- 4364, 2020. 2[45] J. Zeng et al. A survey on transformers for point cloud processing: An updated overview. IEEE Access, 10:86510- 86527, 2022. 2[46] Y. Wang et al. Multi- modal 3D object detection in autonomous driving: A survey. Int. J. Comput. Vis. (IJCV), 131(8):2122- 2152, 2023. 2[47] A. Prakash et al. Multi- modal fusion transformer for end- to- end autonomous driving. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 7077- 7087, 2021. 2[48] Y. Tang et al. Multi- modality 3D object detection in autonomous driving: A review. Neurocomputing, pp. 126587, 2023. 2[49] M. Awais et al. Foundational models defining a new era in vision: A survey and outlook. IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 2024. 2

[50] A. Xiao et al. A survey of label- efficient deep learning for 3D point clouds. IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 2024. 2[51] A. X. Chang et al. ShapeNet: An Information- Rich 3D Model Repository. 4, 13[52] Z. Wu et al. 3D ShapeNets: A Deep Representation for Volumetric Shapes. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2015. 4[53] H. Fu et al. 3D- future: 3D furniture shape with texture. Int. J. Comput. Vis. (IJCV), 129:3313- 3337, 2021. 4, 6[54] J. Collins et al. Abo: Dataset and benchmarks for real- world 3D object understanding. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 21126- 21136, 2022. 4, 6[55] R. Jensen et al. Large Scale Multi- view Stereopsis Evaluation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2014. 4[56] Y. Yao et al. Blendedmvs: A large- scale dataset for generalized multi- view stereo networks. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 1790- 1799, 2020. 4[57] M. A. Uy et al. Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real- World Data. In International Conference on Computer Vision (ICCV), 2019. 4[58] L. Downs et al. Google scanned objects: A high- quality dataset of 3D scanned household items. In Proc. IEEE/RSJ Int. Conf. Robot. Autom. (ICRA), pp. 2553- 2560. IEEE, 2022. 4[59] L. Liu et al. Akb- 48: A real- world articulated object knowledge base. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 14809- 14818, 2022. 4[60] J. Reizenstein et al. Common objects in 3D: Large- scale learning and evaluation of real- life 3D category reconstruction. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 10901- 10911, 2021. 4[61] T. Wu et al. Omniobject3d: Large- vocabulary 3D object dataset for realistic perception, reconstruction and generation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 803- 814, 2023. 4[62] A. Chang et al. Matterport3D: Learning from RGB- D Data in Indoor Environments. Proc. IEEE Int. Conf. 3D Vision (3DV), 2017. 4[63] T. Zhou et al. Stereo magnification: Learning view synthesis using multiple images. Proc. ACM Conf. Comput. Graph. (SIGGRAPH), 2018. 4[64] G. Baruch et al. ArkItScenes: A diverse real- world dataset for 3D indoor scene understanding using mobile rgb- d data. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2021. 4[65] D. Rozenberszki et al. Language- Grounded Indoor 3D Semantic Segmentation in the Wild. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. 4, 10[66] J. Behley et al. SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), 2019. 4[67] J.- E. Deschaud et al. Paris- CARLA- 3D: A real and synthetic outdoor point cloud dataset for challenging tasks in 3D mapping. Remote Sensing, 13(22):4713, 2021. 4[68] H. Caesar et al. muscenes: A multimodal dataset for autonomous driving. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 11621- 11631, 2020. 4[69] A. Dai et al. ScanNet: Richly- Annotated 3D Reconstructions of Indoor Scenes. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 5828- 5839, 2017. 4, 6[70] H. Caesar et al. ruScenes: A Multimodal Dataset for Autonomous Driving. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 11621- 11631, 2020. 4, 6[71] M. Deitke et al. Objaverse: A universe of annotated 3D objects. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 13142- 13153, 2023. 4, 6

[72] K. Chen et al. Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings. Proc. Asian Conf. Comput. Vis. (ACCV), 2019. 4[73] B. Jia et al. SceneVerse: Scaling 3D Vision- Language Learning for Grounded Scene Understanding. arXiv preprint arXiv:2401.09340, 2024. 4[74] S. Yang et al. LiDAR- LLM: Exploring the potential of large language models for 3D LiDAR understanding. arXiv preprint arXiv:2312.14074, 2023. 4[75] J. Achiam et al. Gpt- 4 technical report. arXiv preprint arXiv:2303.08774, 2023. 4, 12[76] D. Z. Chen et al. ScanRefer: 3D Object Localization in RGB- D Scans using Natural Language. Proc. Eur. Conf. Comput. Vis. (ECCV), 2020. 4, 6[77] P. Achlioptas et al. ReferIt3D: Neural Listeners for Fine- Grained 3D Object Identification in Real- World Scenes. In Proc. Eur. Conf. Comput. Vis. (ECCV), 2020. 4[78] Y. Zhang et al. Multi3drefer: Grounding text description to multiple 3D objects. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 15225- 15236, 2023. 4[79] X. Ma et al. When LLMs step into the 3D World: A survey and Meta- Analysis of 3D Tasks via Multi- modal Large Language Models. arXiv preprint arXiv:2405.10255, 2024. 4, 15[80] J. Donahue et al. Decaf: A deep convolutional activation feature for generic visual recognition. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 647- 655, 2014. 4[81] O. Russakovsky et al. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. (IJCV), 115:211- 252, 2015. 4[82] K. He et al. Deep residual learning for image recognition. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 770- 778, 2015. 4, 5[83] A. Radford et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 4, 11, 12[84] J. Wei et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. 4[85] A. Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. Proc. Int. Conf. Learn. Represent. (ICLR), 2021. 5, 6[86] K. He et al. Masked autoencoders are scalable vision learners. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 16000- 16009, 2022. 5, 6, 7[87] J. Deng et al. Imagenet: A large- scale hierarchical image database. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 248- 255. Ieee, 2009. 5, 6[88] C. Sun et al. Revisiting unreasonable effectiveness of data in deep learning era. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), 2017. 5[89] W. X. Zhao et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 5[90] J. Li et al. Blip- 2: Bootstrapping language- image pretraining with frozen image encoders and large language models. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 19730- 19742, 2023. 5, 6, 8, 12, 13[91] J.- B. Alayrac et al. Flamingo: a visual language model for few- shot learning. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:23716- 23736, 2022. 5[92] L. Xu et al. Parameter- efficient fine- tuning methods for pretrained language models: A critical review and assessment. arXiv preprint arXiv:2312.12148, 2023. 5, 15[93] J. Wu et al. Medical sam adapter: Adapting segment anything model for medical image segmentation. arXiv preprint arXiv:2304.12620, 2023. 5[94] B. Lester et al. The power of scale for parameter- efficient prompt tuning. Conf. Empir. Methods Nat. Lang. Process. (EMNLP), 2021. 5[95] E. J. Hu et al. Lora: Low- rank adaptation of large

2022. 5, 13, 14[96] T. Dettmers et al. Qhora: Efficient finetuning of quantized llms. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 36, 2024. 5[97] C. Xu et al. Image2point: 3D point- cloud understanding with 2d image pretrained models. Proc. Eur. Conf. Comput. Vis. (ECCV), 2022. 6[98] G. Qian et al. Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud Understanding. Proc. IEEE Int. Conf. 3D Vision (3DV), 2024. 6[99] J. Kang et al. Point Clouds Are Specialized Images: A Knowledge Transfer Approach for 3D Understanding. IEEE Trans. Multimedia (TMM), 2024. 6[100] Z. Wu et al. 3D ShapeNets: A Deep Representation for Volumetric Shapes. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 1912- 1920, 2015. 6[101] S. Shen et al. DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification. Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), 2024. 6[102] L. Zhang et al. Adding conditional control to text- to- image diffusion models. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 3836- 3847, 2023. 6[103] R. Rombach et al. High- resolution image synthesis with latent diffusion models. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 10684- 10695, 2022. 6, 7[104] C. Schuhmann et al. Laion- 5b: An open large- scale dataset for training next generation image- text models. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:25278- 25294, 2022. 6[105] Y.- C. Liu et al. Learning from 2d: Contrastive pixel- to- point knowledge transfer for 3D pretraining. arXiv preprint arXiv:2104.04687, 2021. 6, 7[106] C. Choy et al. 4d spatio- temporal convnets: Minkowski convolutional neural networks. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 3075- 3084, 2019. 6[107] M. Afham et al. Crosspoint: Self- supervised cross- modal contrastive learning for 3D point cloud understanding. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 9902- 9912, 2022. 6, 7[108] Y. Wang et al. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):1- 12, 2019. 6[109] T. Huang et al. Clip2point: Transfer clip to point cloud classification with image- depth pre- training. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 22157- 22167, 2023. 6, 7[110] A. Vaswani et al. Attention is all you need. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 30, 2017. 6[111] Y. Yao et al. 3D Point Cloud Pre- training with Knowledge Distillation from 2D Images. arXiv preprint arXiv:2212.08974, 2022. 6, 7[112] R. Mokady et al. Cispcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. 6, 7[113] R. Zhang et al. Learning 3D representations from 2d pre- trained models via image- to- point masked autoencoders. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 21769- 21780, 2023. 6, 7, 15[114] Z. Chen and B. Li. Bridging the Domain Gap: Self- Supervised 3D Scene Understanding with Foundation Models. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 6, 7[115] M. Oquab et al. Dimov2: Learning robust visual features without supervision. Trans. Mach. Learn. Res (TMLR), 2024. 6[116] X. Huang et al. Tag2text: Guiding vision- language model via image tagging. Proc. Int. Conf. Learn. Represent. (ICLR), 2024. 6, 7[117] S. Song et al. SUN RGB- D: A RGB- D scene understanding benchmark suite. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 567- 576, 2015. 6

[118] X. Yu et al. Point- bert: Pre- training 3D point cloud transformers with masked point modeling. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 19313- 19322, 2022. 6, 14[119] G. Qian et al. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:23192- 23204, 2022. 6[120] D. Hegde et al. Clip goes 3D: Leveraging prompt tuning for language grounded 3D recognition. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 2025- 2038, 2023. 6, 8[121] H. Zhao et al. Point transformer. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 16259- 16268, 2021. 6[122] X. Ma et al. Rethinking network design and local geometry in point cloud: A simple residual MLP framework. Proc. Int. Conf. Learn. Represent. (ICLR), 2022. 6[123] R. Chen et al. CLIP2Scene: Towards Label- efficient 3D Scene Understanding by CLIP. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 7020- 7030, 2023. 6, 8[124] H. Tang et al. Searching efficient 3D architectures with sparse point- voxel convolution. In Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 685- 702. Springer, 2020. 6[125] R. Huang et al. Joint representation learning for text and 3D point cloud. Pattern Recognition, pp. 110086, 2023. 6[126] Y. Zeng et al. CLIP2: Contrastive Language- Image- Point Pretraining from Real- World Point Cloud Data. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 15244- 15253, 2023. 6, 8[127] C. R. Qi et al. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 30, 2017. 6[128] M. Liu et al. OpenShape: Scaling Up 3D Shape Representation Towards Open- World Understanding. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2024. 6, 8[129] A. Delitzkas et al. Multi- CLIP: Contrastive Vision- Language Pre- training for Question Answering tasks in 3D Scenes. Proc. Brit. Mach. Vis. Conf. (BMVC), 2023. 6, 8[130] J. Zhou et al. Uni3d: Exploring unified 3D representation at scale. Proc. Int. Conf. Learn. Represent. (ICLR), 2024. 6, 8[131] Y. Fang et al. Eva: Exploring the limits of masked visual representation learning at scale. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 19358- 19369, 2023. 6, 8[132] M. Caron et al. Emerging properties in self- supervised vision transformers. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 9650- 9660, 2021. 6, 8[133] J. Ji et al. JM3D & JM3D- LLM: Elevating 3D Representation with Joint Multi- modal Cues. arXiv preprint arXiv:2310.09503, 2023. 6, 8[134] S. Xie et al. Pointcontrast: Unsupervised pre- training for 3D point cloud understanding. In Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 574- 591. Springer, 2020. 5[135] H. Wang et al. Unsupervised point cloud pre- training via occlusion completion. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 9782- 9792, 2021. 5[136] J. Hou et al. Exploring data- efficient 3D scene understanding with contrastive scene contexts. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 15587- 15597, 2021. 5[137] Z. Zhang et al. Self- Supervised Pretraining of 3D Features on Any Point- Cloud. In Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 10252- 10263, 2021. 5[138] X. Yu et al. Point- BERT: Pre- Training 3D Point Cloud Transformers With Masked Point Modeling. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 19313- 19322, 2022. 5[139] R. Zhang et al. Point- m2ae: multi- scale masked autoencoders for hierarchical point cloud pre- training. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:27061- 27074,

[140] G. Hess et al. Masked autoencoders for self- supervised learning on automotive point clouds. Proc. Eur. Conf. Comput. Vis. (ECCV), 2022. 5[141] Z. Li et al. Simipu: Simple 2d image and 3D point cloud unsupervised pre- training for spatial- aware visual representations. In Proc. AAAI Conf. Artif. Intell. (AAAI), volume 36, pp. 1500- 1508, 2022. 5[142] H. Bao et al. Vlmo: Unified vision- language pretraining with mixture- of- modality- experts. arXiv preprint arXiv:2111.02358, 2021. 5[143] R. Hadsell et al. Dimensionality Reduction by Learning an Invariant Mapping. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), volume 2, pp. 1735- 1742, 2006. 7, 8[144] M. Guttmann and A. Hyvarinen. Noise- contrastive estimation: A new estimation principle for unnormalized statistical models. In Y. W. Teh and M. Titterington, editors, Proc. Int. Conf. Artif. Intell. Statist. (AISTAT), volume 9 of Proceedings of Machine Learning Research, pp. 297- 304, Chia Laguna Resort, Sardinia, Italy, 13- 15 May 2010. PMLR. 7[145] T. Chen et al. A simple framework for contrastive learning of visual representations. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 1597- 1607, 2020. 7[146] A. v. d. Oord et al. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 7[147] G. Hinton. Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531, 2015. 7[148] R. Huang et al. Joint representation learning for text and 3D point cloud. Pattern Recognition, 147:110086, 2024. 8[149] M. Cherti et al. Reproducible scaling laws for contrastive language- image learning. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 2818- 2829, 2023. 8[150] Z. Wang et al. P2p: Tuning pre- trained image models for point cloud analysis with point- to- pixel prompting. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:14388- 14402, 2022. 9[151] Z. Guo et al. Calip: Zero- shot enhancement of clip with parameter- free attention. In Proc. AAAI Conf. Artif. Intell. (AAAI), volume 37, pp. 746- 754, 2023. 8, 9[152] H. Peng et al. Multi- view Vision- Prompt Fusion Network: Can 2D Pre- trained Model Boost 3D Point Cloud Data- scarce Learning? arXiv preprint arXiv:2304.10224, 2023. 9[153] X. Yi et al. Invariant training 2d- 3D joint hard samples for few- shot point cloud recognition. In Proc. IEEE Int. Conf. Comput. Vis. (ECCV), pp. 14463- 14474, 2023. 9[154] M. Jia et al. Visual prompt tuning. In Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 709- 727. Springer, 2022. 9[155] D. A. Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659- 663), 2009. 9[156] K. Mo et al. PartNet: A Large- Scale Benchmark for Fine- Grained and Hierarchical Part- Level 3D Object Understanding. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2019. 10[157] Y. Xue et al. ZeroPS: High- quality Cross- modal Knowledge Transfer for Zero- Shot 3D Part Segmentation. arXiv preprint arXiv:2311.14262, 2023. 9, 10[158] H. Guo et al. SAM- guided Graph Cut for 3D Instance Segmentation. arXiv preprint arXiv:2312.08372, 2023. 9, 10[159] Y. Yin et al. SA13D: Segment Any Instance in 3D Scenes. Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 9, 10[160] M. Xu et al. SAMPro3D: Locating SAM Prompts in 3D for Zero- Shot Scene Segmentation. arXiv preprint arXiv:2311.17707, 2023. 10[161] Q. He et al. PointSeg: A Training- Free Paradigm for 3D Scene Segmentation via Foundation Models. arXiv preprint arXiv:2403.06403, 2024. 10

[162] S. Peng et al. Openscene: 3D scene understanding with open vocabularies. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 815- 824, 2023. 10[163] J. Zhang et al. Clip- fo3d: Learning free open- world 3D scene representations from 2d dense clip. Proc. IEEE Int. Conf. Comput. Vis. (ICCV), 2023. 10, 11[164] R. Ding et al. PLA: Language- Driven Open- Vocabulary 3D Scene Understanding. Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 10, 11[165] J. Yang et al. Regionplc: Regional point- language contrastive learning for open- world 3D scene understanding. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 19823- 19832, 2024. 10, 11[166] H. Ha and S. Song. Semantic Abstraction: Open- World 3D Scene Understanding from 2D Vision- Language Models. In 6th Annual Conference on Robot Learning, 2022. 10, 11[167] A. Takmaz et al. Openmask3d: Open- vocabulary 3D instance segmentation. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 10, 11[168] Z. Huang et al. Openins3d: Snap and lookup for 3D open- vocabulary instance segmentation. Proc. Eur. Conf. Comput. Vis. (ECCV), 2024. 10, 11[169] S. Lu et al. Ovir- 3D: Open- vocabulary 3D instance retrieval without training on 3D data. In Conference on Robot Learning, pp. 1610- 1620, 2023. 10, 11[170] P. Nguyen et al. Open3dis: Open- vocabulary 3D instance segmentation with 2d mask guidance. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 4018- 4028, 2024. 10, 11[171] R. Huang et al. Segment3d: Learning fine- grained class- agnostic 3D segmentation without manual labels. In Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 278- 295. Springer, 2025. 10, 12[172] M. E. A. Boudjoghra et al. Open- YOLO 3D: Towards Fast and Accurate Open- Vocabulary 3D Instance Segmentation. arXiv preprint arXiv:2406.02548, 2024. 10, 12[173] D. Zhang et al. Sam3d: Zero- shot 3D object detection via segment anything model. arXiv preprint arXiv:2306.02245, 2023. 12[174] B. Ding et al. VFMM3D: Releasing the Potential of Image by Vision Foundation Model for Monocular 3D Object Detection. arXiv preprint arXiv:2404.09431, 2024. 12[175] D. Zhang et al. FM- OV3D: Foundation Model- Based Cross- Modal Knowledge Blending for Open- Vocabulary 3D Detection. In Proc. AAAI Conf. Artif. Intell. (AAAI), volume 38, pp. 16723- 16731, 2024. 12[176] A. Panagopoulou et al. X- instructblip: A framework for aligning x- modal instruction- aware representations to llms and emergent cross- modal reasoning. arXiv preprint arXiv:2311.18799, 2023. 13[177] Z. Qi et al. Gpt4point: A unified framework for point- language understanding and generation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 26417- 26427, 2024. 13[178] M. Deitke et al. Objaverse- x1: A universe of 10m+ 3D objects. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 36, 2024. 13[179] Z. Qi et al. Contrast with reconstruct: Contrastive 3D representation learning guided by generative pretraining. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 28223- 28243, 2023. 13[180] Y. Tang et al. MiniGPT- 3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors. Proc. ACM Int. Conf. Multimedia (ACMMM), 2024. 13[181] Y. Tang et al. More Text, Less Point: Towards 3D Data- Efficient Point- Language Understanding. arXiv preprint arXiv:2408.15966, 2024. 13[182] A. Amaduzzi et al. LLaNA: Large Language and NeRF Assistant. arXiv preprint arXiv:2406.11840, 2024. 13

[183] B. Mildenhall et al. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99–106, 2021. 13, 14, 15[184] Y. Man et al. Situational Awareness Matters in 3D Vision Language Reasoning. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 13678–13688, 2024. 13[185] Y. Hong et al. 3D- LLM: Injecting the 3D world into large language models. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 14[186] A.- C. Cheng et al. SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model. Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2024. 14[187] D. Liu et al. Uni3d- LLM: Unifying point cloud perception, generation and editing with large language models. arXiv preprint arXiv:2402.03327, 2024. 14[188] Z. Lin et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi- modal large language models. arXiv preprint arXiv:2311.07575, 2023. 15[189] C. Zhu et al. LLaVA- 3D: A Simple yet Effective Pathway to Empowering LMMs with 3D- awareness. arXiv preprint arXiv:2409.18125, 2024. 15[190] B. Kerbl et al. 3D Gaussian Splatting for Real- Time Radiance Field Rendering. ACM Transactions on Graphics, 42(4), 2023. 15[191] M. Deitke et al. Objaverse: A Universe of Annotated 3D Objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13142–13153, 2023. 15[192] C. Ye et al. A closer look at few- shot 3d point cloud classification. International Journal of Computer Vision, 131(3):772–795, 2023. 15[193] S. Ahmadi et al. Foundation Model- Powered 3D Few- Shot Class Incremental Learning via Training- free Adaptor. In Proceedings of the Asian Conference on Computer Vision, pp. 2282–2299, 2024. 15[194] Y. Yang et al. Geometry and Uncertainty- Aware 3D Point Cloud Class- Incremental Semantic Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 21759–21768, June 2023. 15[195] V. Lomonaco et al. Continual reinforcement learning in 3d non- stationary environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 248–249, 2020. 15