# HUMAN-CENTERED EXPLAINABILITY IN INTERACTIVE INFORMATION SYSTEMS: A SURVEY

Yuhao Zhang Peking University China The University of Oklahoma USA zyh_ruc@163. com

Jiaxin An The University of Texas at Austin USA jiaxin.an@utexas.edu

Ben Wang The University of Oklahoma USA benw@ou.edu

Yan Zhang The University of Texas at Austin USA yanz@utexas.edu

Jiqun Liu * The University of Oklahoma USA jiqunliu@ou.edu

# ABSTRACT

Human- centered explainability has become a critical foundation for the responsible development of interactive information systems, where users must be able to understand, interpret, and scrutinize AI- driven outputs to make informed decisions. This systematic survey of literature aims to characterize recent progress in user studies on explainability in interactive information systems by reviewing how explainability has been conceptualized, designed, and evaluated in practice. Following PRISMA guidelines, eight academic databases were searched, and 100 relevant articles were identified. A structural encoding approach was then utilized to extract and synthesize insights from these articles. The main contributions include 1) five dimensions that researchers have used to conceptualize explainability; 2) a classification scheme of explanation designs; 3) a categorization of explainability measurements into six user- centered dimensions. The review concludes by reflecting on ongoing challenges and providing recommendations for future exploration of related issues. The findings shed light on the theoretical foundations of human- centered explainability, informing the design of interactive information systems that better align with diverse user needs and promoting the development of systems that are transparent, trustworthy, and accountable.

Keywords Explainability  $\cdot$  Human- centered XAI  $\cdot$  Systematic Review  $\cdot$  User study

# 1 Introduction

In recent years, Artificial Intelligence (AI) has been integrated into and evaluated within a wide range of interactive information systems (e.g., search and recommendation [1, 2, 3, 4, 5, 6, 7]) across various domains, such as healthcare (e.g., [8, 9]), finance (e.g., [10]), training and entertainment (e.g., [11, 12, 13]), to name a few. These systems increasingly deploy complex, high- dimensional AI algorithms, which notoriously operate as "black- boxes." This opacity poses significant challenges, including those related to trust, accountability, and fairness. Consequently, there is a growing demand for humans to pursue more controllable processes, and efforts to encourage algorithms to explain their outputs are being increasingly prioritized in practice. Building on these advancements, explainable AI (XAI) arises, with explainability emerging as a vital property of AI systems that aim at making algorithmic reasoning processes more transparent and trustworthy for human users [14, 15].

Although existing research in XAI has primarily focused on developing algorithmic techniques to enhance technical explainability, there has been a growing emphasis in recent years on evaluating the effectiveness of these explanations. In the early stages of such research, humans are less involved in the evaluation process and receive less attention, which results in users being unable to understand and utilize the explanations sufficiently [16, 17]. The effectiveness of explanations lies in the perception and reception of the person at the receiving end [18]. Therefore, embracing more human- centered approaches and getting humans involved in the explainable AI development and research are gaining momentum [19]. Many studies, particularly those from the Human- Computer Interaction (HCI) community, have explored human- centered approaches to further enhance AI explainability [20, 21]. Such studies have targeted various stakeholders, from laypeople to experts [2, 20, 22]. For example, [20] conducted a mixed- methods study to examine how users' AI backgrounds shape their perceptions of AI explanations, revealing disparities that inform the design of more inclusive and trustworthy explainable AI systems.

Building on these advancements, scholars seek to gain overviews of the ever- growing field to build up knowledge of Human- Centered Explainable AI. For instance, [23] has examined explainable interfaces in 33 publications and mapped out the current state of user interface and user experience design aspects of XAI. The work by [24] included 58 articles and identified the dimensions of end users' explanation needs with an aim to distill the effects of explanations on end users' perceptions.

Despite these promising developments, three critical issues persist. First, while existing studies mainly focus on AI, they overlook the practice of explainability in traditional information systems, such as the explanations provided by search engines using classic models and phishing reminders. These practices can still be valuable for today's AI- assisted systems. Second, although humancentered approaches have gained considerable traction, a comprehensive synthesis of empirical user studies that directly examine users' perceptions, behaviors, and experiences with explainable systems remains lacking. Third, while recent reviews have made valuable contributions, there have not been systematic efforts to offer an integrated perspective that connects definitions, design practices, and evaluation approaches within interactive information systems. To address these gaps, this review systematically examines the current state of user studies on explainability in interactive information systems, aiming to clarify how explainability has been conceptualized, designed, and evaluated in practice. Accordingly, we seek to answer the following research questions (RQs):

RQ1: How is explainability defined in research on interactive information systems?

RQ2: How are explainability features integrated into the interface design and user interactions of interactive information systems?

RQ3: How is the explainability of interactive information systems evaluated?

This review employs the Preferred Reporting Items for Systematic Reviews and Meta- Analyses (PRISMA) guideline [25], a standard framework for conducting transparent and rigorous literature reviews. Its application enables us to systematically identify and analyze 100 publications while maintaining methodological clarity and consistency. The remainder of the paper is organized as

follows. Section 2 introduces related work in the field and situates our study within the existing literature. Section 3 describes the methods used for identifying, screening, and analyzing the literature. Section 4 presents the findings in response to the three research questions above. Finally, Section 5 discusses the implications of our findings and outlines directions for future research.

# 2 Related work

## 2.1 The definition of explainability

Explainability is a complex and multifaceted concept characterized by varying definitions and terminologies across different research domains. Terms such as interpretability, understandability, and transparency are frequently employed interchangeably or synonymously with explainability, despite notable overlaps and subtle distinctions among them [26, 27]. Different disciplines prioritize distinct aspects and dimensions of explainability, reflecting the diverse objectives and methodological concerns inherent to each domain.

Despite diverse definitions, previous research has attempted systematically categorize terminologies of explainability based on different dimensions. [28] categorized and summarized the concepts of XAI through aspects including scope, approach, goal, and presentation format. Scope distinguishes between global and local explanations, depending on whether the explanation focuses on the entire model or specific results [29]. The approach focuses on the model itself, contrasting inherently interpretable models with ad- hoc explainers designed for interpreting black- box models [30]. The goal refers to the objectives or types of user questions addressed by explanations, such as "How,"[31] "Why,"[32] "Why- not,"[33] "What- if,"[34] "How- to,"[35] and "What- else"[36] explanations. Lastly, Presentation format is how explanations are conveyed, including visual forms (e.g., saliency maps [37]), verbal descriptions (e.g., natural language explanations [38]), explainable interfaces with multiple modalities [39], and analytic presentations (e.g., numerical metrics or structured visualizations [40]). This dimension also includes considerations of content specifics and language.

To enrich the conceptual clarity of XAI terminology, [41] proposed a taxonomy of key terms from the perspective of Explainable Information Systems (XIS), including general, explanation- specific, and system- specific terminologies. General terminologies include concepts fundamental to the construction and evaluation of XIS, such as user, explanation, explainability, interpretability, and trust. Within this general perspective, the user with different background can influence the effectiveness of explanations [32]. Explanation is defined as the output of an XIS, interpretability and explainability represent the capability of an XIS to produce understandable and meaningful outputs, and trust ensures that explanations are effective, credible, and acceptable from the user's perspective [42].

Explanation- specific terminology comprises four primary elements: focus, means, modality, and reasoning. Focus aligns with the concept of scope in [33], distinguishing between global and local explanations. Means differentiates direct explanations from post hoc explanations, which is relevant to approach. Modality addresses whether explanations are static or interactive, corresponding to presentation format [43]. Reasoning encompasses approaches like abductive and counterfactual reasoning, linking to the goal and content dimensions of explanations [26].

Beyond these aspects, [41] also identified system- specific terminologies, classifying them into interpretable and trustworthy systems. Interpretable systems include characteristics such as black- box- ness, complexity [44], simulatability, and decomposability. Trustworthy systems emphasize properties such as reliability, fairness [45, 46, 47], and transparency [48, 49]. These characteristics represent the effectiveness of XIS from multiple aspects.

## 2.2 The design of explainability

The design of explainability has received growing interest in XAI research, as it fundamentally shapes how to design XAI so that it can better serve the needs of real users[23]. An immediate goal

of such work is to comprehend  $HOW$  to provide explanations effectively for particular user groups, which have been explored in various facets, including the format, structure, and interactivity of the explanations.

Regarding the format of explanations, [28] highlighted that explanations can be designed in diverse formats tailored to different user groups, and proposed a classification of formats: visual, verbal, and analytic explanations. Visual explanations employ visual elements to illustrate the model's reasoning processes, such as visual saliency maps. Verbal explanations deliver information through words, phrases, or natural language, implemented in recommendation systems and robotics. Analytic explanations, on the other hand, present information via numerical metrics and data visualizations to convey model performance or reasoning in a more technical manner. Similarly, when examining the scientific work proposing human- centered approaches to evaluate methods for explainability, [27] emphasized the output format of the explanation, including textual, visual, numerical, and mixed. Furthermore, studies such as [50] introduced the voice output, exploring how the different modalities of a virtual agent influence the perceived trustworthiness of an AI system. This line of research highlights the growing interest in multi- modal explanation designs, where combining different formats, such as text, visuals, and voice, can enhance user experience and support diverse user needs across application contexts.

The structure of the information from the explainable interface and the interaction design have emerged as crucial aspects of explainability research, as they directly affect how users interact with AI systems. Previous studies have explored how explanatory information can be organized both theoretically and practically. For instance, [23] analyzed the design requirements and design outcomes applied by XAI researchers in explainable interfaces through a systematic literature review. They specified design requirements through the concept of visual hierarchy, referring to "the organization of the design elements on the page so that the eye is guided to consume each design element in the order of intended importance," including global overview, hierarchy overview, multiple instances, focal points, and grouping overview. Regarding how the information structure from the system interface is implemented in the final design, they identified the following information architectures: sequential, matrix, structure, hierarchical, and found that the sequential information architecture was used the most commonly in final designs.

Interaction design represents another crucial aspect of designing for explainability, including whether the explanation user interfaces are interactive and what specific forms that interactive techniques take. Static explanations remain predominant [23, 51]. However, recent studies found that effective and engaging design is a critical component of any digital product [52, 17] and that there is a growing effort to explore interactive explanation designs [53, 54, 51]. For example, [23] adopted the framework in [52] to summarize four types of interaction: instructing, conversing, manipulating, and exploring. [51] identified a series of interactive functions that support users to select, mutate, and dialogue with XAI, including clarify, arrange, filter/focus, reconfigure, simulate, compare, progress, answer, and ask. They found that clarify and simulate are the most used interaction techniques, combined with compare, filter/focus and arrange.

Some studies have also investigated the interplay between explanation formats and interaction designs, identifying patterns in how particular formats tend to be paired with specific interaction types. One of the examples is [55]'s work, which characterized five concepts of interaction in XAI, including interaction as information transmission, as dialogue, as control, as experience, as optimal behavior, as tool use, and as embodied action. The authors further presented how various modalities (e.g., visual, verbal, chat- based) are embedded in the above interactions. In parallel, [51] observed that charts and textual explanations are the most commonly used elements in XAI design, with tables proving particularly effective for supporting Filter/Focus and Compare interactions, and textual explanations frequently accompanied by Clarify interactions that allow users to control the information displayed on demand.

## 2.3 Measurement of explainability

### 2.3.1 Measurement dimension

Measurement dimensions of explainability can be grouped into two categories: explanation- grounded dimensions, which assess the properties and quality of the explanations themselves, and effects- grounded dimensions, which evaluate the impacts of explainability on users and system- related outcomes. For instance, [24] adopted the information quality dimensions to conceptualize the explanation quality dimensions, including format, completeness, accuracy, and currency. They also categorized explainable AI effects into trust, transparency, usability, understandability, and fairness. [56] highlighted four major classes of measures corresponding to the XAI explaining process: the goodness of explanations, explanation satisfaction, mental models, and performance. The first two are related to the explanation itself, while the latter two are assessed for outcomes. Similarly, [57] identified that evaluating the use of explanation methods comprises three aspects: model, explanation, and user. The user aspect includes four criteria: trust, appropriate trust, bias detection, and explanation satisfaction. The explanation aspect encompasses five criteria: fidelity, identity, separability, novelty, and representativeness. Regarding the model aspect, there are three evaluation criteria: performance, fairness, and reliability.

Some other studies take a broad perspective by compiling evaluation measures derived from prior work. For instance, [28] identified broad categories of evaluation measures for XAI systems and algorithms, including user mental model, explanation usefulness and satisfaction, user trust and reliance, human- AI task performance, and computational measures. [23] conducted a focused review of how explainability interfaces are evaluated and found that prior studies have used multiple metrics, including understandability, task performance (e.g., task accomplishment), transparency, effectiveness, efficiency, user trust, and satisfaction. Additionally, [58] aimed to report on the evaluation of explanations in past user studies, similar to the present review. They presented the question topics in subjective perception questionnaires using a tag cloud, in which transparency, usefulness, usability, satisfaction, and trust are the main aspects evaluated in user studies.

### 2.3.2 Measurement approach

Since the approaches are developed to address different evaluation goals of explainability, various methods have been proposed to assess both the quality of explanations and their effects based on these goals. Recent studies have widely recognized a dual categorization for evaluating methods of explainability [27, 59, 60, 61, 62]. This categorization features a different naming scheme that distinguishes between objective evaluations and human- centered evaluations. For instance, [27] proposed that there are two main ways to evaluate explainability methods in XAI research: (1) objective evaluations, i.e., objective metrics and automated approaches to evaluate methods for explainability. A thorough review of metrics that can be computed without human subjects is available in [63]; (2) human- centered evaluations, employing a human- in- the- loop approach that relies on human participants to collect feedback and evaluations. [60] reviewed the current metrics of XAI systems in the healthcare domain, suggesting two classes of metrics: metrics without a human- in- the- loop and metrics with a human- in- the- loop. Similarly, reviews by [59] divided the evaluation measures into two groups: human- grounded measures and functionally- grounded metrics.

Regarding human- centered evaluations, which focus on user studies concerning XAI, [27] identified two categories based on the nature of the questions posed to participants. Qualitative studies typically collect detailed, in- depth feedback from end- users, providing valuable insights into how participants engage with reasoning processes and their subjective experiences when interacting with explanations. Common methods in this category include think- aloud protocols, semi- structured interviews, free- text analyses, etc [51, 56]. In contrast, quantitative studies typically employ methods such as experiments and surveys to generate data that can be quantitatively measured and statistically analyzed. Commonly used measurement tools include various scales (e.g., System Causability Scale, NASA- TLX, self- developed Likert scales), subjective ratings, and objective measures [27, 60].

The indicators measured often included human performance and interaction patterns, such as task completion time, accuracy, and interaction time [51, 59, 64]. Additionally, a considerable number of studies utilize mixed- methods designs, combining qualitative and quantitative data to provide a holistic view of XAI contexts.

## 2.4 Summary

In summary, the existing research largely addresses the question of explainability in three key areas. First, the concept of explainability varies across the research community, with related terms such as interpretability, understandability, and transparency frequently used interchangeably. These conceptualizations reflect distinct disciplinary priorities and methodological perspectives, leaving a gap for a unified, operational definition. Second, concerning the design of explainability, prior studies have explored diverse explanation formats, interaction mechanisms, and presentation strategies. Earlier research has examined how visual, verbal, and analytic explanations are implemented, as well as how interaction techniques shape users' understanding and trust in AI systems. These designs are frequently customized for various user types, needs, and environments. However, no existing review has systematically mapped how these design elements correspond to specific interface types and how they are adapted to accommodate the expectations of different audiences within interactive information systems. Third, we reviewed the related work on dimensions and approaches for measuring explainability. Prior work has proposed a dual categorization that distinguishes between objective evaluations based on computational metrics and human- centered evaluations involving user studies to assess explanations and their effects on users. No review to date has systematically considered how to measure explainability itself in user studies of interactive information systems. Moreover, beyond listing measures, there remains a lack of higher- level categorization that organizes these diverse metrics into meaningful groups, limiting our understanding of identifying consistent evaluation practices across studies.

Against this background, this survey paper addresses these gaps by offering a comprehensive synthesis of human- centered explainability research in interactive information systems. Specifically, it examines how explainability has been conceptualized, how explanation features have been designed and integrated into system interfaces and user interactions, and how explainability has been measured and evaluated in empirical user studies. This review seeks to provide a clearer and more structured understanding of human- centered explainability, while also guiding future design, implementation, and evaluation practices in this rapidly evolving research area.

# 3 Methods

We chose a systematic review method to gain a thorough understanding of human- centered explainability. We adhered to the guidelines set forth by the Preferred Reporting Items for Systematic Review and Meta- Analysis (PRISMA) [25]. To outline a robust PRISMA process, in this section, we will present the literature review search (Section 3.1), review inclusion and exclusion (Section 3.2), study identification (Section 3.3), and the qualitative coding methodology (Section 3.4). Figure 1 demonstrates the whole literature searching and screening process.

## 3.1 Literature Search

We conducted our search across eight scholarly databases: the ACM Full- Text Collection, Web of Science, PsycInfo, Wiley Online Library, ScienceDirect, Google Scholar, IEEE Xplore, and MIS Quarterly (via Google Scholar). These databases were chosen as they encompass scholarly publications from academic disciplines such as human- computer interaction and information science, which are most relevant to research on Human- Centered Explainability. The search queries- AI OR search OR information retrieval OR recommendation- combined with the following keywords and their variations: user, explainability, and interactive information system. Specific queries were tailored to each database and are detailed in Table 7 of the Appendix.

![](https://cdn-mineru.openxlab.org.cn/extract/71a4cad9-3056-4e59-85ac-a005c743a336/4c30aa1228fc133ca0b805bec7e4fe52a1bd6fa1808cd7a92382378e0b99099b.jpg)  
Figure 1: Number of included articles

Searches targeted titles, keywords, and abstracts in all databases except Google Scholar. For Google Scholar, the first and second authors utilized the "with all of the words" field to apply the search queries. We initially screened the top 20 results. If relevant articles not covered in other databases were identified, the screening was expanded to the top 50 and subsequently the top 100 results. No starting year was set for the search, ensuring a comprehensive coverage of the literature.

One author conducted the searches, removing duplicates by examining titles, abstracts, and DOI numbers. The initial batch search took place in September 2023. To collect missing literature with different keywords or related to the newly emerged AI topic, we conducted the second search on ACM and Google Scholar in January 2024. This search included keywords such as "interpretability," "scrutability," (to replace "explainability" in the original query) and "Generative AI (GenAI)" (to replace "interactive information system"). We collected 1391 papers in the first search and 100 papers in the second search.

## 3.2 Inclusion and Exclusion Criteria

As shown in Figure 2, we screened the literature based on the following inclusion and exclusion criteria: 1) Empirical studies that involve real users in system design and evaluation. Papers that only include data- driven simulations without human evaluation were excluded. For example, [65] proposed a method to integrate user knowledge into explanation to make it understandable. However, it mainly utilized existing dataset without further user studies or engagement, so it was excluded. [66] shared perspectives in explainable human- robot interactions without empirical studies, so it was excluded. 2) The findings can contribute to the understanding of user needs, perception, and evaluation of explainability. For example, [67] investigated explainability in UAV algorithms. Although it involves human user evaluations, it is not related to user needs or perceptions, so it was excluded. 3) The findings shed light on the way that explainability is embodied in system design. For example, [68] mainly focused on the self- explainable interactive method, which is not related to the information systems, so it was excluded. 4) Full papers written in English that were published in peer- reviewed journals or conference proceedings, as a majority of recent works were published in English venues. Non- full or non- peer- reviewed papers, such as workshop papers and extended abstracts, were excluded.

![](https://cdn-mineru.openxlab.org.cn/extract/71a4cad9-3056-4e59-85ac-a005c743a336/2c4940a46ab1b937b3b05253f3f92dfdff1a9da63f1c88075a6d6ada69ad0126.jpg)  
Figure 2: Literature search: Inclusion and exclusion criteria.

## 3.3 Study identification

Using the inclusion and exclusion criteria, we filtered and analyzed papers. Three researchers separately coded one- third of articles and cross- checked with each other. Disagreements were addressed through discussions. In total, we included 100 articles. Among them, 17 included more than one user study, resulting in a total of 121 reported user studies.

## 3.4 Data extraction

The data extraction process was conducted using Google Sheets. Key study characteristics were manually recorded, including publication venues, authors' geographical locations based on their affiliations, and participant demographics, including the number of participants, age, gender, education, race, income, and occupation. The first three authors each performed the initial data extraction on one third of the papers and then cross- checked the coding results with each other. Discrepancies were addressed through group discussion. We also referred to previous studies to develop the codebook for research method [69], participants' level of knowledge on AI [51] or specific domain [51], and the domain to which explainable interface is applied [70, 71]. Table 1 presents the definition of each code.

We applied different analysis approaches to answer each research question. For the definition of explainability (RQ1), we extracted the original definition of explainability- related concepts from the included papers into Google Sheet, where we identified three groups of definitions regarding explanation, explainability, and XAI. We then conducted textual analysis using Python to shed light on the similarity and differences among the three concepts (see Section 4.2).

We further categorized the keywords based on different facets of the definition, focusing on aspects including where (the context in which an explanation is needed), why (the goal that the explanation aims to achieve), whom (the target audience that the explanation is intended for), what (the content included in the explanation), and how (the actions or effects that the explanation has on the user). These dimensions help illuminate various perspectives, such as different roles and functions within the concept of explainability, providing a comprehensive framework for analysis.

Table 1: The codebook for study characteristics  

<table><tr><td colspan="2">Research method</td></tr><tr><td>Qualitative</td><td>The research method that gather and analyze non-numerical data, e.g., interview and focus groups.</td></tr><tr><td>Quantitative</td><td>The research method that gather and analyze numerical data, e.g., exper-iment and survey.</td></tr><tr><td>Mixed method</td><td>The research design that combines quantitative and qualitative research methods to draw on the strengths of each.</td></tr><tr><td colspan="2">Participants’ level of knowledge on AI/domain</td></tr><tr><td>Expert</td><td>Participants are qualified as expert if they have sufficient knowledge on AI or related domains (e.g., healthcare professionals and lawyers).</td></tr><tr><td>Novice</td><td>Participants have limited knowledge of AI or related domains.</td></tr><tr><td>Uninitiated</td><td>Participants have no knowledge of AI or related domains.</td></tr><tr><td>Unreported</td><td>The paper didn’t reported participants’ level of knowledge on AI or related domains.</td></tr><tr><td colspan="2">Application domain and examples</td></tr><tr><td>Academia</td><td>The explainability design targeted academia scholars, e.g., scholar rec-ommendation explanation [72].</td></tr><tr><td>Architecture</td><td>The explanation that helps user understand specific architecture design recommendation [73].</td></tr><tr><td>Education</td><td>The explanation that help students or teachers adopt systems, e.g., intel-ligent tutoring system [74], to assist their learning.</td></tr><tr><td>Engineering</td><td>The explainability design concerned with the design, building, and use of engines, machines, and structures, such as energy efficiency recom-mendation [75] and air-handling unit faults diagnosis [76].</td></tr><tr><td>Food safety</td><td>The explainability design supporting the identification of safe food, e.g., edible mushroom identification [77].</td></tr><tr><td>Finance &amp;amp; Business</td><td>The explanation that helps user to make financial and business decisions, such as hotel price prediction [78] and income prediction [79].</td></tr><tr><td>Generic</td><td>The explainability design that can be applied to different domains, such as text classification [80], pfishing attack [81], programming [82], robotics [83], facial image recognition [84], and unmanned aerial vehicle [85].</td></tr><tr><td>Law &amp;amp; Civic</td><td>The explanation aims to assist legal and civic activities, such as criminal investigation [86] and legal case reasoning [87].</td></tr><tr><td>Leisure</td><td>The explanation that helps with users’ entertainment, such as the expla-nations for book recommendation [88] and movie recommendation [89].</td></tr><tr><td>Medical &amp;amp; Health</td><td>The explanation that help with specific tasks of patients, healthcare providers, and other stakeholders in medical or general health contexts, such as the explanation of COVID information [90], COVID diagnosis [91], and personal lifestyle recommendation [92].</td></tr><tr><td>Transportation</td><td>The explainability design that help users to understand and use systems for transportations, such as intelligent vehicles [93].</td></tr></table>

For the design elements of explainability (RQ2), an codebook (see Table 2) was developed referring to existing design catalogs [94, 95]; we took screenshots of all interfaces all these articles, one researcher coded the screenshots following the codebook and another researcher validated the coding results, discrepancies were addressed through group discussion. We then conducted descriptive analysis of the coding results (see Section 4.3).

For the measure of explainability (RQ3), we first extracted the constructs measured and corresponding items into Google Sheet, we then categorized these constructs and items referring to a past data quality framework [107] and XAI evaluation measures [28, 27]. The previous framework and measures suggest multidimensional, user- centered approaches for explainability studies. Following these approaches, we extracted categories for explainability measures, which include intrinsic, format and presentation, usability, experiential, ethics, and interaction with the explanation. The first five are subjective measurements, while the last focuses on objective metrics. The codebook is shown in Table 3 (see Section 4.4 for corresponding results)

Table 2: Categories of explainability design  

<table><tr><td>Category</td><td>Sub-category</td><td>Definitions</td></tr><tr><td>Interactivity</td><td>Non-interactive</td><td>A pre-generated prototype or explanations without interactive func-tions. e.g., a screenshot of an explanation interface [78].</td></tr><tr><td rowspan="5">Modality</td><td>Interactive</td><td>A prototype with interactive functions [96, 93].</td></tr><tr><td>Text-based</td><td>The explanation is conveyed through text, e.g., a textual explanation [79].</td></tr><tr><td>Graphical-based</td><td>The explanation is conveyed through visual representations, e.g., the data visualization such as bar chart, word cloud, and radar graph [72].</td></tr><tr><td>Video-based</td><td>The explanation is presented through video media, such as a video loop [97].</td></tr><tr><td>Voice-based</td><td>The explanation is conveyed through the voice, e.g., natural language in voice from wearable glasses [98].</td></tr><tr><td rowspan="9">Interface type</td><td>Chatbot</td><td>A software program designed to simulate conversation in natural language with human users [92, 99].</td></tr><tr><td>Dashboard</td><td>A type of graphical user interface (GUI) which often provides at-a-glance views of data relevant to a particular objective or process [96, 100].</td></tr><tr><td>Desktop app</td><td>A software program that runs on a desktop computer or a laptop [101, 102].</td></tr><tr><td>Game</td><td>A game played using a computer, typically a video game [103].</td></tr><tr><td>Wearable glass</td><td>Eyeglasses that have built-in technology that adds digital information to the wearer&#x27;s view [98].</td></tr><tr><td>Mobile app</td><td>A software program that runs on a mobile device, like a smartphone or tablet [104, 105].</td></tr><tr><td>Pop-up message</td><td>A brief message that appears on a website or mobile app to inform or prompt users to take an action [81, 93].</td></tr><tr><td>Virtual reality</td><td>A computer-generated environment that simulates reality [106].</td></tr><tr><td>Web-based tool</td><td>A software application that runs in a web browser and can be accessed over the internet [88, 87].</td></tr></table>

Table 3: Categories of measured constructs  

<table><tr><td>Category</td><td>Definitions</td></tr><tr><td>Intrinsic</td><td>The inherent attributes of explanations in its own right.</td></tr><tr><td>Format and presentation</td><td>The way that how information is structured, styled, and delivered to users.</td></tr><tr><td>Usability</td><td>Refers to the ease with which users can understand, interact with, and derive value from the explanation provided.</td></tr><tr><td>Experiential</td><td>Focuses on the subjective user experience and how users perceive and interact with the explanations.</td></tr><tr><td>Ethics</td><td>Whether the ethical considerations were given attention.</td></tr><tr><td>Interaction with the Explanation</td><td>Objective metrics to measure if and how the users engaged with the explanations.</td></tr></table>

# 4 Findings

To report the findings of our systematic review, we begin by providing an overview of the descriptive statistics for the 121 studies included in this review (Section 4.1). Following this, we present a synthesis of our coding themes to address each research question (Section 4.2 - 4.4).

## 4.1 Study characteristics

### 4.1.1 Participant characteristics

We provided an overview of participant characteristics in 121 studies reported in the 100 included articles. The sample size of participants ranged from 3 to 680 ( $M = 95.37$ ,  $SD = 126.10$ ). Most of

these studies  $(N = 81)$  involved fewer than 100 participants. We described participant characteristics as follows:

Age. Across the 121 studies from 100 included articles, there were significant variations in the age ranges of the participants. Specifically, 42 studies reported the age range of participants, the minimum age was between 16 and 25  $(M = 19.98, SD = 2.40)$  and the maximum age was between 23 and 84  $(M = 50.43, SD = 14.81)$ . 28 studies reported the mean age of participants, which is between 20.7 and 61.0  $(M = 34.71, SD = 9.05)$ ; 61 studies didn't report participants' age.

Gender. Of the 121 studies, 56 reported participants' gender. The average percentage of female participants is  $47.56\%$ $(SD = 17.37\%)$ .  $46.43\%$  studies  $(N = 26)$  had a sample with more than  $50\%$  being women participants.

Level of knowledge on AI. Among the 121 studies, 34 reported participants' knowledge levels of AI. Of these, 22 studies specified participants as having no AI knowledge  $(N = 4)$ , basic knowledge as novices  $(N = 12)$ , or sufficient knowledge as experts  $(N = 27)$ ; the rest of them  $(N = 12)$  recruited participants with various levels of knowledge on AI.

Level of domain knowledge. 57 studies reported participants' knowledge on the domain that the system targeted to, including domain experts  $(N = 35)$ , with various level of domain knowledge  $(N = 18)$ , and with no domain knowledge  $(N = 2)$ .

### 4.1.2 Research method

Among these 121 included studies, more than half of them  $(N = 65)$  adopted the quantitative approach; 20 of them used the qualitative approach; and 56 of them applied mixed- method approaches.

Quantitative approach. Experiment  $(N = 46)$  and survey  $(N = 19)$  were two major design in these approach. For experiment studies, 22 of them were conducted on the cloud- sourcing platform (e.g., Amazon Mechanical Turk [82] and Profic [80]), six of them conducted the experiments through online surveys (e.g., [108] distributed survey through their direct contacts), one conducted field experiment ([92] invited users to use their systems for four days in order to obtain their evaluation in real life). For survey studies, one of them combined survey with eye- tracking to enrich the data collection [88].

Qualitative approach. This group of studies mainly applied interview  $(N = 19)$  to collect data. The rest design included workshop [109] and open- ended question survey [73].

Mixed method approach. We identified the following combinations of methods in this group of studies: (1) Experiment combined with methods to collect qualitative feedback  $(N = 17)$ , including open- ended questions in surveys  $(N = 11$ , e.g., [80, 110], asking general comments  $(N = 5$ , e.g., [111]) or textual feedback  $(N = 1$ , [112]), written reflection  $(N = 1$ , [113]), think- aloud  $(N = 1$ , [114]), and interviews  $(N = 1$ , [106]). (2) Survey combined with open- ended question  $(N = 11$ , e.g., [96, 115]), interviews  $(N = 3$ , e.g., [116]), and general comments  $(N = 3$ , e.g., [117]). Additionally, one study applied a combination of card- sorting and interviews to understand participants' ranking of different visualizations and corresponding qualitative feedback [72].

## 4.2 Explainability Definition

As the papers used different terms in their definition, we examined how variations in keywords for explanation, XAI, and explainability contribute to differences among the definitions of these three definition terms. Table 4 lists the keywords from definitions of the three terms. In addition, we separated the keywords into five categories: where, why, whom, what, and how.

The results show that certain high- frequency keywords overlap across all three definition terms, including "system", "decision", "model", "understand", and "user". These shared keywords point to a fundamental focus on clarifying how explanations/explainability relates to systems, users, and decision- making, which is the main focus of the definition of explainability in the three terms.

Table 4: Definitions Keywords in three definition terms. (The full list of word frequency is in Appendix Table 8)  

<table><tr><td colspan="2">XAI</td><td colspan="2">Explanation</td><td colspan="2">Explainability</td></tr><tr><td colspan="2">Where</td><td colspan="2">Where</td><td colspan="2">Where</td></tr><tr><td>AI</td><td>45</td><td>decision</td><td>10</td><td>model</td><td>9</td></tr><tr><td>system</td><td>21</td><td>model</td><td>7</td><td>system</td><td>9</td></tr><tr><td>decision</td><td>13</td><td>system</td><td>4</td><td>decision</td><td>6</td></tr><tr><td>model</td><td>13</td><td>Why</td><td>AI</td><td>5</td><td></td></tr><tr><td>ML</td><td>4</td><td>understand</td><td>7</td><td>ML</td><td>3</td></tr><tr><td>blackbox</td><td>4</td><td>help</td><td>5</td><td>Why</td><td></td></tr><tr><td>Why</td><td>4</td><td>Whom</td><td>7</td><td>understand</td><td>9</td></tr><tr><td>understand</td><td>15</td><td>user</td><td>7</td><td>ability</td><td>7</td></tr><tr><td>transparency</td><td>8</td><td>human</td><td>3</td><td>interpretable</td><td>3</td></tr><tr><td>interpretable</td><td>5</td><td>What</td><td></td><td>clearer</td><td>2</td></tr><tr><td>trust</td><td>4</td><td>recommendation</td><td>10</td><td>comprehensible</td><td>2</td></tr><tr><td>help</td><td>3</td><td>reason</td><td>6</td><td>Whom</td><td></td></tr><tr><td>accuracy</td><td>2</td><td>feature</td><td>5</td><td>human</td><td>9</td></tr><tr><td>improve</td><td>2</td><td>item</td><td>4</td><td>user</td><td>8</td></tr><tr><td>Whom</td><td></td><td>information</td><td>3</td><td>What</td><td></td></tr><tr><td>user</td><td>13</td><td>insight</td><td>3</td><td>information</td><td>7</td></tr><tr><td>human</td><td>8</td><td>justification</td><td>3</td><td>item</td><td>4</td></tr><tr><td>What</td><td></td><td>alternative</td><td>2</td><td>recommendation</td><td>4</td></tr><tr><td>recommendation</td><td>6</td><td>behavior</td><td>2</td><td>xxx-based</td><td>3</td></tr><tr><td>specific</td><td>4</td><td>causality</td><td>2</td><td>output</td><td>3</td></tr><tr><td>prediction</td><td>3</td><td>classification</td><td>2</td><td>prediction</td><td>3</td></tr><tr><td>result</td><td>3</td><td>quality</td><td>2</td><td>specific</td><td>3</td></tr><tr><td>technology</td><td>3</td><td>specific</td><td>2</td><td>everything</td><td>2</td></tr><tr><td>inner workings</td><td>3</td><td>How</td><td></td><td>feature</td><td>2</td></tr><tr><td>output</td><td>2</td><td>describe</td><td>3</td><td>internal</td><td>2</td></tr><tr><td>reason</td><td>2</td><td>present</td><td>3</td><td>reason</td><td>2</td></tr><tr><td>How</td><td></td><td></td><td></td><td>How</td><td></td></tr><tr><td>Reveal</td><td>2</td><td></td><td></td><td>present</td><td>2</td></tr></table>

The main keywords shared by three terms are in bold face.

These words indicate an emphasis on the context in the system and its decisions and models where explanations are needed, the underlying goal of fostering understanding, and the user who receives the explanations.

Despite these commonalities, each definition term exhibits distinctive emphases. Figure 3 presents the Venn diagram of the definition keywords. The Explanation term tends to focus on rich, content- driven definitions (what), highlighting elements such as "justification", "alternatives", "behaviors", "causality", and "classification". By contrast, the XAI term widens the scope to include comprehensive aspects (where, why, and what) related to "transparency", and "trust", as well as explicitly referencing "black- box" models and the "inner workings". Except for the major commonalities, the overlap between only explanation and XAI is limited with the keyword "help", suggesting divergent priorities. Explanation remains content- centric, whereas XAI incorporates a broader set of objectives related to the goal of the explanability.

The Explainability term appears to bridge these two terms above. It overlaps with Explanation in terms of content- related keywords (what), such as "features", "items", and "information", indicating a shared focus on the content- oriented definition of providing meaningful, detailed explanations. It also overlaps with XAI in terms of context- oriented and goal- related keywords (where and why) like "AI", "ML", and "interpretable", reflecting an integrated perspective that addresses not only what information is provided but also how and why it should be made understandable. This bridging role suggests that Explainability merges the content- rich foundation of Explanation with the broader aims of XAI.

Interestingly, the highest frequency keyword "AI" exists in the XAI and Explainability definitions but is absent from Explanation. This pattern suggests that the traditional notion of Explanation

![](https://cdn-mineru.openxlab.org.cn/extract/71a4cad9-3056-4e59-85ac-a005c743a336/195da14259de77b9d7e8aae9670857fde46981eb4ac7789e7065b6981c328031.jpg)  
Figure 3: Definition keyword distribution in three definition terms. The colored texts indicate different types of the keyword. The font size of the keyword is adjusted according to its frequency.

may have a long- standing conceptual heritage in various fields but may not need to be restricted to AI contexts, while The concepts of XAI and Explainability are more distinctly grounded in the challenges posed by complex AI and ML systems, which is closely tied to the need for explanation. In addition, in terms of how explainability is conveyed, we identified keywords such as "describe", "reveal", and "present" in the definitions. These actions do not refer to the specific form, like visual or textual explanations, but reflect the intended actions that explanations perform on behalf of the system to achieve the goal to the user.

## 4.3 Explainability interface

Out of the 100 articles included in our analysis, 85 detailed their explanation design. The interface design was analyzed along three dimensions: interactivity, modality, and interface type.

Interactivity refers to whether the explanation interfaces evaluated in user studies allowed users to interact with the explanation content actively. Among the 85 articles, 44 utilized interactive interfaces, which are prototypes enabling participants to adjust, query, or manipulate explanations through interactions with them, such as chatbots and mobile applications. Another 42 studies investigated non- interactive interfaces, which provide static, pre- generated explanations without user control, such as prepared text or prototype screenshots. Notably, one article employed both types of interfaces in multiple user studies [99], resulting in a total of 86 explanation setups analyzed across 85 papers.

Modality refers to the medium through which the explanation is delivered to the user. We identified four primary modalities in the reviewed studies: text- based, graphical- based, video- based, and voice- based explanations. Additionally, some studies employed multi- modal designs, combining two of these modalities to enhance explanatory effectiveness. Examples of various types of

![](https://cdn-mineru.openxlab.org.cn/extract/71a4cad9-3056-4e59-85ac-a005c743a336/aa0248baee8fe58211c9d5da8f9a1385f2cbd120caa2a907fb1c4ac45c06e850.jpg)  
Figure 4: Examples of explainability modalities. Top Row (Left to Right): [90, 118, 119]. Second Row [98, 105, 97]

modalities are presented in Figure 4. A considerable number of studies adopted multi- modal designs, most often combining graphics and text to improve explanatory clarity and engagement. The remaining instances utilized a single modality, accompanied by formatting techniques. For example, text- based explanations often emphasize key information and improve readability through visual techniques such as bold, italic, or underlining. Similarly, graphical explanations varied in their visual strategies, including the use of color coding and highlighting to increase explainability. Video- based explanations, although relatively uncommon, appeared in a small subset of studies  $(N = 3)$  to present animated or dynamic content. These explanations typically illustrated sequential system behaviors and the transformation of decision outcomes (e.g., from negative to positive [97]) in a clear and intuitive manner. Only one paper delivered explanations through spoken natural language [98]. Despite their potential advantages, particularly in mobile, assistive, or hands- free contexts, auditory modalities remain significantly underexplored within the current explainability research.

Interface type refers to the platform or application environment in which explanations are delivered. Nine categories of interface types were identified. Among the 44 studies employed interactive interfaces, the vast majority were graphical user interfaces (GUI)  $(N = 51)$ , with dashboards being the most common  $(N = 21)$ , followed by desktop applications  $(N = 5)$ , chatbots  $(N = 4)$ , web- based tools  $(N = 4)$ , mobile apps  $(N = 3)$ , pop- up messages  $(N = 2)$ , computer programs  $(N = 1)$ , VR applications  $(N = 1)$ , and games  $(N = 1)$ . Only one study employed a voice user interface (VUI), using a wearable smart glasses system to provide explanations via voice- based natural language [98]. Two studies reported their interactive functions without specifying the exact interface type. The identified interface types highlight the diverse and evolving contexts in which explainability is embedded, spanning from conventional dashboards to immersive VR experiences and emerging wearable technologies.

Interaction of interface type and modality To further understand the pairing between interface types and explanation modalities, we examined how different interactive interface types incorporate various modalities of explanation delivery. Studies using, for example, dashboards, desktop applications, and mobile applications predominantly adopted visual modalities as the basis for delivering explanations, including functional visualizations such as saliency maps, charts, and decision paths (Figure 5a). In addition to these functional visual elements, many interfaces also incorporate

![](https://cdn-mineru.openxlab.org.cn/extract/71a4cad9-3056-4e59-85ac-a005c743a336/855e515ed956d464902d1cdfc8f77886c770efa2f40ee16ddb6bcc03ccd8ab87.jpg)  
Figure 5: Examples of explainability modalities. Top Row (Left to Right): [120, 77]. Second Row: [121, 106]

images, icons, and illustrative graphics to improve visual appeal and assist users in comprehending complex content (Figure 5b). In some cases, these graphical elements were supplemented with brief text- based explanations for clarity (Figure 4e). Conversely, chatbots and pop- up messages exclusively relied on text- based explanations (Figure 5c). This aligns with the conversational or time- sensitive nature of these interfaces, where explanations are presented in short, sequential, and easily understandable textual formats. These forms of interactions prioritize conciseness and directness, allowing little room for graphical or multi- modal presentations. The single case of Virtual Reality applied graphical- based explanations, embedding explanatory cues directly into the immersive environment (Figure 5d). Such visual explanations leveraged the spatial and dynamic affordances of VR, where visual metaphors are more effective than text or audio.

## 4.4 Measures of Explainability

### 4.4.1 Frequency of appeared dimensions

As outlined in the data analysis section, we mapped the evaluating constructs for explainability into the higher- level categories. Table 5 depicts the frequency distribution of the collapsed dimensions within the five main categories. In general, explainability was quantitatively measured 126 times. The most measured category is experiential, accounting for around  $23.0\%$  of the total  $(N = 29)$ . Although usability is the second most measured category  $(N = 26)$ , it encompasses the most dimensions, reflecting its broad and multifaceted nature. The intrinsic categories and the format and presentation are measured 23 and 21 times, respectively. The remaining two categories, ethics and interaction with the explanation, represent a smaller share when measuring explainability.

For Intrinsic, the most frequently measured dimension is accuracy and fidelity, accounting for  $60.9\%$  of all the dimensions that appeared, followed by quality  $(34.8\%)$  and reliability  $(4.3\%)$ . Format and presentation categories have four dimensions, among which the presence of information is the most frequently measured  $(42.9\%)$ , followed by the visual aesthetic, modality, and organization of the information. The constructs that examined usability in the included studies have seven unique dimensions. The most frequently measured dimension is understandable, taking up around  $38.5\%$ . Readability follows, making up  $15.4\%$  of the total. The three dimensions—usability, ease of use, and

Table 5: Distribution of collapsed dimensions within categories  

<table><tr><td>Experiential</td><td>N=29</td><td>Usability</td><td>N=26</td><td>Intrinsic</td><td>N=23</td><td>Format and Presentation</td><td>N=21</td><td>Ethics</td><td>N=14</td><td>Interaction with the Explanation</td><td>N=13</td></tr><tr><td>Usefulness</td><td>14(48.3%)</td><td>Understandable</td><td>10(38.5%)</td><td>Accuracy and Fidelity</td><td>14(60.9%)</td><td>Presence of information</td><td>9(42.9%)</td><td>Transparency/7(50.0%)</td><td>Explanation</td><td>5(38.5%)</td><td></td></tr><tr><td>Satisfaction</td><td>10(34.5%)</td><td>Readable</td><td>4(15.4%)</td><td>Quality</td><td>8(34.8%)</td><td rowspan="2">Visual 
Aesthetics 
Modality</td><td>6(28.6%)</td><td>Trust</td><td>4(28.6%)</td><td>clicks 
Time spent</td><td>4(30.8%)</td></tr><tr><td>Behavioral intention</td><td>3(10.3%)</td><td>Ease of use</td><td>3(11.5%)</td><td>Reliability</td><td>1(4.3%)</td><td>4(19.0%)</td><td>Scrutability</td><td>3(23.1%)</td><td>Explanation 
Access 
Explanation initiation</td><td>3(23.1%)</td></tr><tr><td>Importance</td><td>1(3.4%)</td><td>Usability</td><td>3(11.5%)</td><td></td><td></td><td>Organization of the information information</td><td>2(9.5%)</td><td></td><td></td><td></td><td>1(7.7%)</td></tr><tr><td>Interest</td><td>1(3.4%)</td><td>Awareness of the explanation 
Intrusiveness 
Simulatability</td><td>3(11.5%)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>

awareness of the explanations—are evenly distributed, each holding  $11.5\%$  of the counts for usability measurements. Finally, both intrusiveness and simulatability were measured twice. Five dimensions belong to the experiential category, among which usefulness and satisfaction appeared the most frequently, occurring in more than  $82.8\%$  of the total. The remaining three dimensions—behavioral intention, importance, and interest—appeared in the remaining  $17.2\%$  articles. The ethics category included three key aspects: transparency, trust, and scrutability, which were measured seven, four, and three times, respectively. Four types of user actions are logged and sorted in descending order by measurement proportion: explanation clicks  $(38.5\%)$ , time spent  $(30.8\%)$ , access to explanations  $(23.1\%)$ , and explanation initiation  $(7.7\%)$ .

### 4.4.2 Categories of explainability dimensions

Table 6 presents the dimensions of explainability and the categories of dimensions used by the included articles for measuring explainability. It further outlines the synonyms for each dimension as used in the included articles, along with examples of measurement tools and studies. It is worth noting that three studies used an umbrella word (e.g., explainability) to cover their evaluation rubrics without specifying which dimension they measured [122, 123, 124]. We did not include the constructs in such studies in our table.

Table 6: Categories of explainability dimensions  

<table><tr><td>Categories</td><td>Dimensions</td><td>Measurement tools examples</td><td>Example studies</td></tr><tr><td>Intrinsic Category</td><td></td><td></td><td></td></tr><tr><td>Accuracy and Fidelity</td><td>reasonableness / acceptability / the fidelity of the counterfactual states / perceived component accuracy / suitable</td><td>Whether the explanations represented the actual fault. Judging whether a recommended tag is suitable.</td><td>[76, 82, 125, 101, 113, 126]</td></tr><tr><td>Quality</td><td>perceived explanation quality/ the quality of explanations</td><td>System Causability Scale</td><td>[120, 127, 112, 128]</td></tr><tr><td>Reliability</td><td>reliability</td><td>Did you find the explanations to be unpredictable across different queries?</td><td>[88]</td></tr><tr><td>Format and Presentation Category</td><td></td><td></td><td></td></tr><tr><td>Visual Aesthetics</td><td>readability of the letters / use of color / use of images</td><td>Rating from 0 to 100 on the readability of the letters / use of color / use of images</td><td>[129]</td></tr><tr><td>Modality</td><td>enjoyable / preference of different explanation forms</td><td>Grade each visualization on a scale from 0 (worst) to 10 (best)</td><td>[130, 117, 131]</td></tr></table>

(Continued on next page)

<table><tr><td>Categories</td><td>Dimensions</td><td>Measurement tools examples</td><td>Example studies</td></tr><tr><td>Organization of information</td><td>organization of information</td><td>Rating from 0 to 100 on the organization of information</td><td>[129]</td></tr><tr><td>Presence of information</td><td>completeness / presence of irrelevant, misleading, contradictory information / the information preference in an accompanying explanation</td><td>This information is complete. I want to be able to view only the most important variables that influence the likelihood of the fault. This explanation has misleading, contradictory, irrelevant information.</td><td>[90, 80, 76, 132]</td></tr><tr><td>Usability Category</td><td></td><td></td><td></td></tr><tr><td>Understandable</td><td>understandability / understanding / understandable</td><td>This explanation-component is un-derstandable. I understood the warning dialog.</td><td>[133, 134, 115, 135]</td></tr><tr><td>Readability</td><td>readable / use of language / familiarity</td><td rowspan="2">The explanation is legible, easy to read. It was clear to me how to access the explanations. The explanation navigation was clear to me.</td><td>[133, 129, 81]</td></tr><tr><td>Usability</td><td>usability</td><td>[136, 74, 137]</td></tr><tr><td>Ease of use</td><td>perceived ease of use / easiness of interpretation of visual output</td><td>I would need an expert opinion to understand the explanations provided by SmellChecker. The color bar / scatter plot / bar plot with the score is easy to interpret.</td><td>[110, 138, 87]</td></tr><tr><td>Simulatability</td><td>simulatability / user mental model</td><td>I believe that I could provide an explanation similar to the agent&#x27;s explanation.</td><td>[136, 88]</td></tr><tr><td>Awareness</td><td>awareness of the explanation</td><td>Whether they observed the explanation. Whether participants were aware of the provided explanation</td><td>[104, 78]</td></tr><tr><td>Intrusiveness</td><td>intrusiveness / alignment</td><td>The explanations distracted me from my learning task. This attention map aligns with my attention when reading the natural language prompt.</td><td>[74, 124]</td></tr><tr><td>Experiential Category</td><td></td><td></td><td></td></tr><tr><td>Satisfaction</td><td>satisfaction</td><td>I was satisfied with the explanations.</td><td>[139, 140, 141]</td></tr><tr><td>Usefulness</td><td>usefulness / goodness / helpfulness / sufficiency / value of explanation</td><td>The explanations provided of how the AI-system works have sufficient detail. The explanations were helpful for me.</td><td>[74, 101, 142, 83]</td></tr><tr><td>Intention</td><td>willingness to pay for the explanation / intention to use the explanation</td><td>I want to see this attention when working with code generation models in real life.</td><td>[124, 143]</td></tr></table>

(Continued on next page)

<table><tr><td>Categories</td><td>Dimensions</td><td>Measurement tools examples</td><td>Example studies</td></tr><tr><td>Importance</td><td>importance</td><td>This explanation-component is important.</td><td>[134]</td></tr><tr><td>Interest</td><td>interest</td><td>I am not interested in this warning dialog.</td><td>[81]</td></tr><tr><td>Ethics Category</td><td></td><td></td><td></td></tr><tr><td>Transparency</td><td>transparency / explanation(s) that best helped them understand a model prediction</td><td>The visualization presents the relationship between the recommended person and me. I understand why the cells were marked suspicious through the explanations.</td><td>[136, 72, 110]</td></tr><tr><td>Scrutability</td><td>scrutability</td><td>The visualization allows me to compare and decide whether the system is correct or wrong.</td><td>[144, 72]</td></tr><tr><td>Trust</td><td>trust</td><td>Did you find the explanations trustworthy for search results?</td><td>[88, 72, 81]</td></tr><tr><td colspan="4">Interaction with the Explanation</td></tr><tr><td>Explanation clicks</td><td>Explanation clicks</td><td>tapping rate of the button number of clicks on the explanation icon</td><td>[130, 118, 99, 72]</td></tr><tr><td>Explanation access</td><td>Explanation type accessed / interaction with Explanations</td><td>Whether participants accessed the explanation or skipped it.</td><td>[145, 74]</td></tr><tr><td>Time spent</td><td>Time spent on explanation</td><td>Attention to explanations per hints received</td><td>[145, 128, 74, 88]</td></tr><tr><td rowspan="3">Explanation initiation</td><td rowspan="3">Explanation initiation</td><td>Time Spent on Tooltips</td><td rowspan="3">[74]</td></tr><tr><td>Hints before first explanation initiation</td></tr><tr><td>Explanation initiations per hints received</td></tr></table>

The intrinsic category reflects the inherent attributes of explanations in their own right. The most commonly measured intrinsic feature is accuracy and fidelity, which reflects how accurately the explanation mirrors the underlying model. It aims to ensure that the explanation faithfully represents the AI model's internal logic, for example, whether the explanations represent the actual air- handling unit faults [76]. Quality is a broad term that overlaps with other dimensions to some extent in actual usage. For example, the evaluation of the quality of explanations here is closely tied to usability because a major scale used for measuring it is the system causability scale [120, 127, 146], which was developed based on a previous usability scale and is applied to measure how much the explanation supports users' causal understanding and how usable the explanation interface is [147]. Reliability refers to the consistency and stability of the explanations provided by a model or system across similar inputs. Only one study examined the reliability of explanations, and it was conducted in the context of searching for fiction books [88].

The category of format and presentation evaluates the explainable interface to ensure that the explanations are well- organized and represented. Choosing the appropriate modality is crucial to maximize the expressive effectiveness of an explanation. An explanation can be delivered to users in different forms: text- based, visual- based, statistical- based, audio- based, or a hybrid of the above. It can be quantified by users' preference for different explanation presentations [130]. Aesthetics concerns whether an interface is visually appealing, e.g., what color it applies to, the appropriate usage of fonts and spacing, or images. Only one study considered visual aesthetics[129]. The organization of information and presence of information emphasize how the explanation conveys structured, relevant, necessary information to users. They aim to balance the amount of information

according to users' expectations or needs. For example, [76] evaluated the importance of different types of information that fault diagnosis explanations might contain, such as the most important variables, probability, etc.

The third category evaluates usability requirements of the explanations to ensure that none of these factors hinder users from utilizing explanations. Understandable refers to how easily the target audience can comprehend the explanations. In some cases, readability is the foundation of understandability, focusing on the ease with which explanations can be read. Usability refers to how easy it is for users to access, navigate, and interact with the explainability content. Though the evaluation of ease- of- use overlaps with the above three - understandability, readability, and usability - it still deserves to be listed uniquely. Ease of use is sometimes measured by whether users need extra support, i.e., an expert opinion, to understand the explanations. Simulatability, which takes a user- centered perspective, measures how well the user is able to recreate or repeat the computational process based on provided explanations of a system [148]. Balanced against the above usability requirements is the need to avoid explanations that attract too much attention from users. On the one hand, it can be checked by manipulating different explanation designs, such as whether users observed the explanation [104, 78]. On the other hand, it can be measured as the distraction caused by explanations. For example, Conati et al. included intrusiveness in the explanation questionnaire to gauge the potential intrusiveness of the explanations in terms of distraction, confusion, and overwhelming [74].

The fourth category of explainability dimensions focuses on the experiential aspect of the user- explanation interaction. It examines users' experiences, behavioral intentions, and perceptions while engaging with explainability. Experiential measurement is often contextualized by a specific scenario or task. Satisfaction could be measured in different ways. One way was to examine end users' overall satisfaction with the explanations [85, 140] by using the item, "the explanation of how the simulation tool works is satisfying" [85]. In other work, it can be measured in terms of satisfaction with types of visualization techniques [76], satisfaction with explanation styles [141], and satisfaction with information provided [72], etc. Usefulness, interest, and perceived importance of explanations are also crucial factors when evaluating explanations in intelligent systems. The former is assessed by the end user on how informative, useful, or helpful the explanation is, while the other two relate to whether users are interested in the explanation and whether they perceive the explanation component as important. Behavioral intention is often seen as a prerequisite for actual behavior. Unlike other experiential dimensions that measure contextual feelings during the interactive process, behavioral intentions focus on evaluating the likelihood of engaging with the explanations in real- world scenarios. For instance, researchers measured whether expert users want to see attention- based explanations when working with code generation models in real life [124].

The fifth category, ethics, concerns the alignment of explanations with ethical principles. AI ethics is often broken down into principles in previous studies, including transparency, fairness, responsibility, trust, privacy, etc [149, 150]. In this review, we identify three of them. These three measure ethical considerations of the explainability approach. Transparency, considered one of the most prevalent aspects, aims to measure how well an XAI system can explain how the intelligent system works. An explainable approach that achieves transparency should provide explanations covering data, models, and predictions [151]. Scrutability enables users to correct the system's reasoning or modify preferences in the user model [144, 152]. For example, in a scrutable recommender system, the user can leverage explanations, e.g., explore and determine the recommendation quality or compare and decide whether the system is correct or wrong [72]. The final metric, explanation trustworthiness, evaluates whether users find the explanation itself trustworthy. This is distinct from a common measure in other studies, where trust refers to whether the explanation helps the user build a stronger sense of trust in the overall system [99].

Unlike the previous five categories, which mainly collect data through self- reported methods such as questionnaires or Likert- scale surveys, the last category focuses on objective metrics related to the participants' interaction with and various actions related to explanations. These actions include explanation clicks, explanation access, time spent, and explanation initiation. Four studies logged

the explanation clicks to measure the number of mouse clicks on the explanation icons or to view the explanation interface [130, 118, 72]. In some tasks, the explanation functionality is triggered by a clickable button, allowing participants to choose whether to enable it. Therefore, access to explanations while performing the tasks is logged. [145] used a binary variable to indicate whether participants accessed the explanation or skipped it. When multiple types of explanations were available to users, the metric was also calculated as a discrete variable to measure explanation types accessed [74]. The next aspect is the time spent on explanations. While other studies track task completion times, the logs for time spent on the explanations can provide insight into user engagement and cognitive effort in the explanation conditions. Two studies utilized an eye tracker to capture how much time the participants spent looking at the explanation [74, 88]. Explanation initiation, measured only in one article, indicates how participants initiated explanations in response to hints [74].

# 5 Discussion

## 5.1 Answers to the three research questions

### 5.1.1 Explainability definition

This literature review identified three key constructs that have recurred in prior explainability research: explainability, explanation, and explainable AI (XAI). Through a textual analysis of these definitions, we identified five dimensions that researchers have used to conceptualize explainability: (1) where explainability is needed, (2) why it is provided, (3) to whom it is directed, (4) what it includes, and (5) how it is conveyed. These dimensions reveal that explainability acts as a conceptual bridge, connecting the content- rich foundation of explanation with the broader objectives and normative expectations associated with XAI. However, our review also uncovered conceptual gaps across these dimensions, which hinder the development of a comprehensive definition of explainability for interactive information systems.

In terms of where explainability is needed, [153] proposed a framework identifying the contexts in which explainability can be applied, including data preparation, model development, and explainability visualization. However, our analysis reveals that prior research has predominantly conceptualized explainability from the perspective of model and system development, with limited attention to explainability in data preparation, such as transparency regarding the data used for training algorithms.

Regarding why explainability is provided, our analysis found that existing research commonly defines explainability through concepts such as transparency, trust, and comprehensibility. These terms embody underlying assumptions about the intended nature of the human- system relationship. For example, when explainability is framed as a way to foster trust, it implies that offering explanations enhances user trust and reliance on the system. Alternatively, when explainability is associated with comprehensibility, the emphasis shifts to ensuring that users gain a thorough understanding of system behaviors and reasoning processes, thereby allowing them to maintain complete control. Future definitions should begin by clarifying the human- system relationship that explainability aims to establish, as the aim shapes both design priorities and evaluation criteria.

As for whom explainability is directed, previous studies suggest that different user groups, such as those with varying levels of AI expertise [28], distinct social roles (e.g., data scientists vs. managers) [26], or domain- specific needs [24], require different types and levels of explainability [154]. This highlights the growing recognition of user diversity, and that explainability is inherently user- oriented and user- sensitive. Yet, existing definitions still employ an overly generic term, such as "user" or "human," overlooking context- specific differences in user needs, perspectives, and capabilities.

Regarding the content included in the explanation, our analysis suggests that previous definitions conceptualize explanation content from several perspectives. The first perspective concerns the

granularity of information in explainability. Some scholars adopt a narrow, feature- level approach, defining explainability in terms of the specific features or variables that influence the outputs of AI systems [79]. In contrast, other scholars propose a much broader view, suggesting that explainability encompasses everything that makes AI systems more understandable to human beings [155]. These two views reflect fundamentally different assumptions about what types of information are valuable or necessary in human- AI interactions. The second perspective addresses the focus of the explanation, as existing definitions differ in the aspects of the system they aim to make understandable. Some conceptualizations emphasize explanations of the inner workings of models, including the underlying algorithms, decision rules, and reasoning processes [74, 156]; whereas other definitions focus on explaining the outputs or predictions of AI systems [79], providing users with reasons for current results without revealing the system's internal mechanisms. The other perspective involves the types of content that serve different functions. For example, causality focuses on the factual relationships between variables [93], while insights are more subjective and task- specific, offering guidance for decision- making [121]. This variation suggests that different studies conceptualize explainability with distinct goals in mind, reflecting diverse perspectives on its intended function.

Finally, in terms of how explainability is conveyed, our analysis identified keywords such as "reveal" and "present" in the literature. While prior studies have extensively discussed the modalities of explanation delivery [41, 43], such as whether the explanation is static or interactive, or whether it is textual or visual, these works primarily answer the operational question of "how do we explain?" in terms of the medium or format. In contrast, our analysis of definitions does not refer to the specific form of explanation per se, but rather to describe the intended actions that explanations perform on behalf of the system, whether to "open up" the hidden processes, clarify internal reasoning, or depict outputs for the user's understanding. This distinction highlights that definitions of explainability implicitly prescribe the role expected of explanations in shaping human- AI interactions, which is often overlooked in discussions centered on modality.

### 5.1.2 Explainability design

The design of explanation interfaces has gained increasing attention in XAI research, as it fundamentally shapes how real users perceive, interact with, and benefit from explainable systems. While existing reviews have discussed explanation formats and modalities, fewer studies have systematically examined how explanation interfaces are presented in empirical user studies. To address this gap, this paper reviewed interface design practices in explainability research and organized them into three dimensions: interactivity, modality, and interface type.

When it comes to interactivity, while interactive explanation interfaces have gained increasing interest in recent years, a substantial portion of user studies still depend on non- interactive, pregenerated prototypes, such as static screenshots, prepared texts, or mock- ups. This imbalance highlights the technical and resource- related challenges in developing fully functional interactive systems for experimental settings, as well as a reliance on tightly controlled study environments. The existing literature has acknowledged the critical role of interaction affordances in enhancing the effectiveness, transparency, and engagement of explainability interfaces [54]. Our findings reaffirm this importance while revealing a methodological inertia that hinders advancing toward user- controllable explanation systems.

Regarding modality, our review proposed a taxonomy for explanation formats, including text, graphical, and voice explanations. It also echoes previous reviews that have emphasized the importance of multimodal explanation designs in addressing diverse user needs and application contexts [157, 158, 50]. Our review found a clear preference among researchers for text- based and graphical explanations in empirical user studies, while video- based and voice- based explanations remain substantially underexplored. Video- based explanations are intuitive and engaging ways for illustrating dynamic, sequential processes and system behaviors; voice- based explanations hold promise for hands- free, mobile, or accessibility- focused contexts by providing users with real- time, natural language feedback in situations where visual or textual explanations may be impractical.

Future studies should actively explore how diverse modalities, especially mixed combinations of modalities, can be leveraged to enhance explanation clarity, thereby better accommodating diverse user needs and application fields.

Our analysis classified deployment platforms for explanation interfaces into nine categories: dashboards, desktop applications, games, chatbots, mobile apps, web- based tools, pop- up messages, virtual reality (VR) systems, and wearable devices. There was uneven attention to these environments. The majority of the included studies relied on graphical user interfaces, particularly dashboards, reflecting their practicality and familiarity in experimental settings. Notably, only a few studies have explored emerging interface types, such as voice user interfaces and virtual reality, despite their significant potential for providing embodied explanation experiences. When examining the pairing patterns between interface types and explanation modalities, our analyses reveal that certain interface types were more likely to be paired with certain modalities. For example, dashboards predominantly employ graphical- based explanations with supplementary text cues, while chatbots and pop- up messages rely almost exclusively on text- based explanations. These pairing patterns suggest an implicit alignment between certain interface types and explanation modalities, likely influenced by technical feasibility, interaction conventions, and user expectations within each environment. Future research should systematically explore how explanation modalities and interface types can be effectively combined to optimize user experience, task performance, and explanation outcomes across diverse interaction scenarios.

### 5.1.3 Explainability measurement

Building upon previous classifications [24, 159], measurements in explainability studies can be broadly categorized into two types: those assessing the qualities of explanations themselves and those examining their effects. A key motivation for this review lies in the need to clarify the former category in human- centered evaluations of explainable systems. However, a significant limitation in prior work is the insufficient differentiation of individual constructs. For example, several prior studies view understanding as a reflection of how well users comprehend the overall AI- assisted system or task outcome after interacting with explanations, as explanations constitute a type of decision- making assistance [160, 24]. In other cases, however, understanding was measured with respect to the explanation itself, that is, how well users grasp the content, structure, or reasoning process conveyed by the explanation [59]. By explicitly focusing on the measures of explainability itself, this review clarifies measurement ambiguity and offers a clearer, more structured mapping of what is being measured in user studies on interactive information systems.

A notable finding that emerged from the review is that it provided a comprehensive list of dimensions along which researchers investigate explanations. The Intrinsic category addresses the inherent properties of explanations. Prior research has consistently highlighted the importance of these attributes in explainable systems (e.g., [57, 24, 161]), and our review reveals inconsistencies in how they have been empirically measured. For instance, fidelity, which indicates how faithfully an explanation mirrors the underlying model, is sometimes inferred as a factor of trust [56, 162]. Empirical assessments of these attributes, particularly from a user- centered perspective, lack clear operationalization. In contrast, the evaluation technique known as the System Causability Scale is actively gaining traction within the research community [163]. This scale provides a structured and validated framework for assessing the quality of explanations from the user's viewpoint [147]. Its growing use reflects a promising step toward standardizing measurements and calls for an effort to develop well- defined instruments for other unspecified dimensions.

The format and presentation category captures how explanations are conveyed to users, and our review identified four primary measurement aspects within this dimension: visual aesthetics, modality, information organization, and information presence. Previous studies have noted the importance of designing and evaluating explainable interfaces [54], and some earlier research has explored individual aspects, for instance, examining modality preferences (e.g., text vs. graph) or types of information presented in the explanations [23, 164]. However, prior reviews typically treat these as a descriptive characteristic of the system or explanation design, rather than a measurable

construct in user evaluations. This highlights a methodological gap in the existing literature, which lacks evaluation of presentation features. In contrast, our review acknowledges format and presentation as an independent, user- centered evaluation focus, documenting how presentation- related constructs have been measured in practice.

Usability has consistently been a central focus in explainability research, with constructs such as understandability, readability, and ease of use widely adopted in user studies. For example, [58] and [59] both associated ease of use with usability. These measures typically assess users' subjective perceptions of how easily explanations can be comprehended and utilized within interactive systems. Additionally, we refined usability into measures of awareness and intrusiveness, both important yet underexplored aspects within this category. These focus on whether explanations appropriately catch user attention and whether they interrupt users during their interaction with systems. Balanced against other usability requirements, these two are crucial for ensuring that explanations not only facilitate decision- making but also integrate seamlessly into users' workflows without causing cognitive overload.

The ethics category addresses concerns regarding the responsible development and deployment of explainability- assisted systems, focusing on normative attributes such as transparency, scrutability, and trust. These measures emphasize the need to align explanations with broader ethical principles to ensure that users are well- informed, empowered to question decisions, and are able to develop calibrated trust in AI systems. While their conceptual significance is acknowledged, our review highlights that empirical efforts to capture these ethical dimensions systematically remain relatively sparse and inconsistent, indicating an important direction for future explainability research.

The Experiential category refers to users' subjective experiences and emotional responses when engaging with explanations. Notably, this category emerged as the most frequently measured in our review, underscoring the importance of user experience within the field of explainability research. In line with the previous review [58], satisfaction and usefulness were measured in  $82\%$  of the articles within this category, indicating their main role as experiential indicators. Conversely, dimensions such as behavioral intention, importance, and situational interest have received less attention, despite their potential to enhance our understanding of how explanations influence users' engagement, cognitive states, and future behaviors under varying tasks [165]. These findings highlight the need for more diverse and theoretically supported experiential measures in explainability evaluations.

Lastly, interaction with the explanation category includes behavioral indicators that reflect how users engage with explanations in interactive information systems. A necessary separation has to be made between measures assessing users' engagement with the explanations and those evaluating their subsequent outcomes, as the two often do not align. This issue has been largely ignored in previous research, resulting in a methodological gap. For instance, the widely adopted human- AI performance category (e.g., Human- AI Task Performance [28]) often blends measurements that target different aspects of explainability. Some studies emphasize metrics related to direct interaction with explanations themselves (e.g., [72, 74]), which reflect how users interact with or rely on explanations during user studies. Others assess whether viewing explanations enhances human performance on tasks, such as acceptance rate or answer accuracy [75, 133], thereby indirectly inferring the effectiveness of the explanations. While both types of measures are valuable, the lack of differentiation can obscure whether the observed behaviors stem from the explanation's inherent qualities or from its effects on system use and task outcomes.

## 5.2 Reflections and future directions

This review not only synthesized the current progress in the explainability of interactive information systems research but also uncovered gaps in the literature. Building on these findings, this section proposes several promising directions to guide future research and advance the research on explainability to support human- AI interaction.

### 5.2.1 Advancing the Quality of User Studies

5.2.1 Advancing the Quality of User StudiesOur review reveals that while explainability research increasingly adopts more human- centered approaches to assess the effectiveness of explanations, many user studies exhibit limitations in methodological rigor. In particular, issues such as missing data on participant characteristics (e.g., age, gender, and familiarity with AI), unclear research procedures, and unspecified evaluation measures often limit the replicability and generalizability of research findings and re- usability of research resources (e.g. corpora, user dialogue data, annotations and judgment labels) [166]. Given that explainability is intrinsically user/human- centered and its effects usually vary across different user groups and task scenarios, the above gaps might undermine the development of knowledge in the field. To address these limitations, future studies could enhance the quality of user research in two aspects. First, future research should adopt standardized reporting guidelines to ensure the sufficient disclosure of details about user studies. This would enhance the transparency of research processes and enable more systematic comparisons across studies. Second, researchers should create more diverse and theoretically grounded evaluation frameworks that incorporate a broader range of metrics. Furthermore, the explainability research would benefit from the establishment of standardized and validated measurement instruments, similar to the System Causability Scale [147], to promote methodological consistency, robustness in evaluation practices, and effective reuse of research resources (e.g. user interaction data, experimental tasks and hypothetical scenarios, study procedure, prototype interfaces) across different research communities [167, 168].

### 5.2.2 Pairing Definitions with Design and Evaluation Strategies

A key finding of this review is that there is a disconnect between how explainability is conceptualized, designed, and evaluated in interactive information systems. This observation highlights a central implication of our review: definitions of explainability should serve as the foundation for both system design and evaluation. Specifically, future research should begin by addressing the five dimensions of definition identified in this study - where, why, to whom, what, and how. These five dimensions provide a structured framework for defining the scope and purpose of explainability within a given context.

Once these definitional dimensions are established, the design of explanation interfaces should explicitly align with their corresponding requirements. For example, clarifying where explainability is required (e.g., during data preparation, model output presentation, or decision support) can help determine where explanations should be incorporated into user workflows. Similarly, defining to whom explanations are directed ensures that the style and content of explanations are tailored to the target audience's knowledge level and needs. The dimensions of why, what, and how can then guide design decisions about the explanation's intended goals, content, and delivery strategies. Moreover, these same definitional dimensions should also shape the selection of evaluation metrics and research designs. For instance, if a study defines explainability as fostering users' trust in AI systems, its evaluation should include validated trust scales, as well as behavioral indicators of trust, such as users' willingness to rely on or reject AI recommendations. Likewise, if explainability is intended to enhance users' understanding of system reasoning processes, evaluations can employ think- aloud protocols to assess users' mental models. This definition- driven approach—from definition to design to evaluation—holds great potential for advancing both theoretical insights and practical outcomes in the study of explainable human- AI interaction.

### 5.2.3 Explainability in LLM/GenAI Applications

5.2.3 Explainability in LLM/GenAI ApplicationsThe rise of large language models (LLMs) and generative AI (GenAI) systems introduces distinctive challenges and opportunities for explainability research [169]. GenAI is known for various shortcomings, ranging from causing hallucinations in outputs to generating biased or unfair responses. Therefore, explanations can help adjust and validate the output for GenAI [170]. Previous research has shown that explainability poses greater challenges for GenAI systems due to their open- ended, interactive, and context- sensitive nature, which makes explanations highly dynamic

and audience- sensitive [171, 172]. While recent reviews have begun to address explainability in GenAI contexts, such as outlining explainability methods (e.g., local analysis, global analysis) [173], identifying different stakeholders and deployment contexts for LLMs [174], and summarizing evaluation measures (e.g., plausibility, truthfulness, consistency) [175, 172], gaps remain in emphasizing the role of audience- centered XAI in LLMs. Building on our findings, we suggest that future research should move beyond system- focused computational explainability methods and prioritize audience- centered approaches for GenAI systems. Such studies will help deepen the understanding of the diverse in- situ and long- term needs, expectations and biases, and potential functional fixedness of various user groups [176, 177, 178], enabling the development of explanations for GenAI that reflect dynamic sense- making, accommodate varying user expertise, and support situated trust calibration [179, 180].

## 5.3 Limitations

Similar to other survey research, this work has some limitations as well as implications for future work. First, as our literature search concluded in February 2024, recent works, particularly those addressing explainability in most recent, domain- specific LLM- based systems, may have been missed. We acknowledge this limitation and strive to incorporate emerging trends into our discussions. Second, while our review systematically examined how explainability is measured, we primarily focused on the measurement of explainability itself. As a result, this survey offers less coverage on evaluating the outcomes or effects of explanations on user perceptions, decisions, and performance. Future research would benefit from a more thorough analysis that combines the evaluation of explanation and its effects on task completion and the fulfillment of user goals.

# 6 Conclusion

Explainability has become a vital property in the design of interactive information systems, supporting transparency, user trust, and responsible decision- making. To this end, a nuanced understanding of how users interpret and engage with explanations is crucial for advancing both theory and practice in explainable information system design and evaluation measures. Therefore, this review focuses on empirical user studies, systematically examining how explainability has been conceptualized, designed, and measured.

Following the PRISMA guideline, we synthesize 100 empirical studies and organize the literature according to three aspects: definition, interface design, and evaluation measures of explainability. Together, these contributions offer a consolidated understanding of the current landscape of user- centered explainability research. Conceptually, five dimensions have been identified to capture the multifaceted nature of explainability in interactive information systems. When it comes to design, explanation strategies have been systematically categorized based on their interactive affordances, presentation modalities, and interface type. Evaluation measures are grouped into six user- centered categories, reflecting the diverse ways explainability itself has been assessed in empirical research. Together, these contributions offer a consolidated understanding of the current landscape of user- centered explainability research. This study further confirms three practical avenues for future exploration: enhancing the quality and depth of user studies, systematically aligning definitions with design and evaluation strategies, and developing audience- centered explainability perspectives for LLM and GenAI applications. By pursuing these avenues, future research can assist AI in responsibly integrating into human- centered information environments.

# Acknowledgments

Special thanks to Young- Li Huang for her contribution in analyzing the interfaces. Jiqun Liu's participation in this project is partially supported by a Junior Faculty Summer Fellowship (JFSF) award from the Research Council at the University of Oklahoma.

# References

[1] Yongfeng Zhang, Xu Chen, et al. Explainable recommendation: A survey and new perspectives. Foundations and Trends® in Information Retrieval, 14(1):1- 101, 2020.

[2] Hao- Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O'connell, Terrance Gray, F Maxwell Harper, and Haiyi Zhu. Explaining decision- making algorithms through ui: Strategies to help non- expert stakeholders. In Proceedings of the 2019 chi conference on human factors in computing systems, pages 1- 12, 2019.

[3] Qingyao Ai, Yongfeng Zhang, Keping Bi, and W Bruce Croft. Explainable product search with a dynamic relation embedding model. ACM Transactions on Information Systems (TOIS), 38(1):1- 29, 2019.

[4] Peijie Sun, Le Wu, Kun Zhang, Yu Su, and Meng Wang. An unsupervised aspect- aware recommendation model with explanation text generation. ACM Transactions on Information Systems (TOIS), 40(3):1- 29, 2021.

[5] N. Chen, J. Liu, and T. Sakai. A reference- dependent model for Web search evaluation: Understanding and measuring the experience of boundedly rational users. pages 3396- 3405, New York, NY, 2023. ACM.

[6] Zhichao Xu, Hansi Zeng, Juntao Tan, Zuohui Fu, Yongfeng Zhang, and Qingyao Ai. A reusable model- agnostic framework for faithfully explainable recommendation and system scrutability. ACM Transactions on Information Systems, 42(1):1- 29, 2023.

[7] Lei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation. ACM Transactions on Information Systems, 41(4):1- 26, 2023.

[8] Seyedeh Neelufar Payrovnaziri, Zhaoyi Chen, Pablo Rengifo- Moreno, Tim Miller, Jiang Bian, Jonathan H Chen, Xiuwen Liu, and Zhe He. Explainable artificial intelligence models using real- world electronic health record data: a systematic scoping review. Journal of the American Medical Informatics Association, 27(7):1173- 1185, 2020.

[9] Andreas Holzinger, Georg Langs, Helmut Denk, Kurt Zatloukal, and Heimo Muller. Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4):e1312, 2019.

[10] Wei Jie Yeo, Wihan Van Der Heever, Rui Mao, Erik Cambria, Ranjan Satapathy, and Gianmarco Mengaldo. A comprehensive review on financial explainable ai. Artificial Intelligence Review, 58(6):1- 49, 2025.

[11] Guoshuai Zhao, Hao Fu, Ruihua Song, Tetsuya Sakai, Zhongxia Chen, Xing Xie, and Xueming Qian. Personalized reason generation for explainable song recommendation. ACM Transactions on Intelligent Systems and Technology (TIST), 10(4):1- 21, 2019.

[12] O- Joun Lee and Jason J Jung. Explainable movie recommendation systems by using story- based similarity. In Iui workshops, 2018.

[13] Chao Wang, Hengshu Zhu, Peng Wang, Chen Zhu, Xi Zhang, Enhong Chen, and Hui Xiong. Personalized and explainable employee training course recommendations: A bayesian variational approach. ACM Transactions on Information Systems (TOIS), 40(4):1- 32, 2021.

[14] Nadia Burkart and Marco F Huber. A survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research, 70:245- 317, 2021.

[15] Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Yunhao Liu, Anil Jain, and Jiliang Tang. Trustworthy ai: A computational perspective. ACM Transactions on Intelligent Systems and Technology, 14(1):1- 59, 2022.

[16] Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and Finale Doshi- Velez. How do humans understand explanations from machine learning systems? an evaluation of the human- interpretability of explanation. arXiv preprint arXiv:1802.00682, 2018.

[17] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267:1- 38, 2019.

[18] Q Vera Liao and Kush R Varshney. Human- centered explainable ai (xai): From algorithms to user experiences. arXiv preprint arXiv:2110.10790, 2021.

[19] Thu Nguyen and Jichen Zhu. Towards better user requirements: How to involve human participants in xai research. arXiv preprint arXiv:2212.03186, 2022.

[20] Upol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I.- Hsiang Lee, Michael Muller, and Mark O. Riedl. The Who in Explainable AI: How AI Background Shapes Perceptions of AI Explanations, July 2021. arXiv:2107.13509 [cs].

[21] Alison Smith- Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd- Graber, Daniel S Weld, and Leah Findlater. No explainability without accountability: An empirical study of explanations and feedback in interactive ml. In Proceedings of the 2020 chi conference on human factors in computing systems, pages 1- 13, 2020.

[22] Indu Panigrahi, Sunnie SY Kim, Amna Liaquat, Rohan Jinturkar, Olga Russakovsky, Ruth Fong, and Parastoo Abtahi. Interactivity x explainability: Toward understanding how interactivity can improve computer vision explanations. In Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 1- 9, 2025.

[23] Thu Nguyen, Alessandro Canossa, and Jichen Zhu. How Human- Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey, March 2024. arXiv:2403.14496 [cs].

[24] AKM Bahalul Haque, A. K. M. Najmul Islam, and Patrick Mikalef. Explainable Artificial Intelligence (XAI) from a user perspective: A synthesis of prior literature and problematizing avenues for future research. Technological Forecasting and Social Change, 186:122120, January 2023.

[25] Matthew J. Page, Joanne E. McKenzie, Patrick M. Bossuyt, Isabelle Boutron, Tammy C. Hoffmann, Cynthia D. Mulrow, Larissa Shanseer, Jennifer M. Tetzlaff, Elie A. Akl, Sue E. Brennan, Roger Chou, Julie Glanville, Jeremy M. Grimshaw, Asbjorn Hrobjartsson, Manoj M. Lalu, Tianjing Li, Elizabeth W. Loder, Evan Mayo- Wilson, Steve McDonald, Luke A. McGuinness, Lesley A. Stewart, James Thomas, Andrea C. Tricco, Vivian A. Welch, Penny Whiting, and David Moher. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. Systematic Reviews, 10(1):89, March 2021.

[26] Alejandro Barredo Arrieta, Natalia Diaz- Rodriguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil- Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58:82- 115, June 2020.

[27] Giulia Vilone and Luca Longo. Notions of explainability and evaluation approaches for explainable artificial intelligence. Information Fusion, 76:89- 106, December 2021.

[28] Sina Mohseni, Niloofar Zarei, and Eric D. Ragan. A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems. ACM Transactions on Interactive Intelligent Systems, 11(3- 4):1- 45, December 2021.

[29] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su- In Lee. From local explanations to global understanding with explainable AI for trees. Nature Machine Intelligence, 2(1):56- 67, January 2020.

[30] Scott M Lundberg and Su- In Lee. A Unified Approach to Interpreting Model Predictions. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.

[31] Tania Lombrozo. Explanation and categorization: How "why?" informs "what?". Cognition, 110(2):248- 253, February 2009.

[32] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135- 1144, San Francisco California USA, August 2016. ACM.

[33] Jo Vermeulen, Geert Vanderhulst, Kris Luyten, and Karin Coninx. PervasiveCrystal: Asking and Answering Why and Why Not Questions about Pervasive Computing Applications. In 2010 Sixth International Conference on Intelligent Environments, pages 271- 276, Kuala Lumpur, Malaysia, July 2010. IEEE.

[34] Rafal Kocielnik, Saleema Amershi, and Paul N. Bennett. Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End- user Expectations of AI Systems. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pages 1- 14, Glasgow Scotland Uk, May 2019. ACM.

[35] Brian Y. Lim, Anind K. Dey, and Daniel Avrahami. Why and why not explanations improve the intelligibility of context- aware intelligent systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 2119- 2128, Boston MA USA, April 2009. ACM.

[36] Carrie J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S. Corrado, Martin C. Stumpe, and Michael Terry. Human- Centered Tools for Coping with Imperfect Algorithms During Medical Decision- Making. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pages 1- 14, Glasgow Scotland Uk, May 2019. ACM.

[37] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, April 2014. arXiv:1312.6034 [cs].

[38] Himabindu Lakkaraju, Stephen H. Bach, and Jure Leskovec. Interpretable Decision Sets: A Joint Framework for Description and Prediction. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1675- 1684, San Francisco California USA, August 2016. ACM.

[39] Brad A. Myers, David A. Weitzman, Amy J. Ko, and Duen H. Chau. Answering why and why not questions in user interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 397- 406, Montreal Québec Canada, April 2006. ACM.

[40] Fred Hohman, Minsuk Kahng, Robert Pienta, and Duen Horng Chau. Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers. IEEE Transactions on Visualization and Computer Graphics, 25(8):2674- 2693, August 2019.

[41] Matthieu Bellucci, Nicolas Delestre, Nicolas Malandain, and Cecilia Zanni- Merk. Towards a terminology for a fully contextualized XAI. Procedia Computer Science, 192:241- 250, 2021.

[42] David Gunning and David W. Aha. DARPA's Explainable Artificial Intelligence Program. AI Magazine, 40(2):44- 58, June 2019.

[43] Vijay Arya, Rachel K. E. Bellamy, Pin- Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilović, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei, and Yunfeng Zhang. One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques, September 2019. arXiv:1909.03012 [cs].

[44] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. A Survey of Methods for Explaining Black Box Models. ACM Computing Surveys, 51(5):1- 42, September 2019.

[45] Sam Corbett- Davies, Johann D. Gaebler, Hamed Nilforoshan, Ravi Shroff, and Sharad Goel. The Measure and Mismeasure of Fairness. Journal of Machine Learning Research, 24(312):1- 117, 2023.

[46] Nuo Chen, Jiqun Liu, Hanpei Fang, Yuankai Luo, Tetsuya Sakai, and Xiao- Ming Wu. Decoy effect in search interaction: Understanding user behavior and measuring system vulnerability. ACM Transactions on Information Systems, 43(2):1- 58, 2025.

[47] Jiqun Liu. Toward a two- sided fairness framework in search and recommendation. In Proceedings of the 2023 Conference on Human Information Interaction and Retrieval, pages 236- 246, 2023.

[48] Valerie Beaudouin, Isabelle Bloch, David Boumie, Stéphan Clémençon, Florence d'Alché Buc, James Eagan, Winston Maxwell, Pavlo Mozharovskyi, and Jayneel Parekh. Flexible and Context- Specific AI Explainability: A Multidisciplinary Approach, March 2020. arXiv:2003.07703 [cs].

[49] Yingqiang Ge, Shuchang Liu, Zuohui Fu, Juntao Tan, Zelong Li, Shuyuan Xu, Yunqi Li, Yikun Xian, and Yongfeng Zhang. A survey on trustworthy recommender systems. ACM Transactions on Recommender Systems, 3(2):1- 68, 2024.

[50] Katharina Weitz, Dominik Schiller, Ruben Schlagowski, Tobias Huber, and Elisabeth André. "let me explain!": exploring the potential of virtual agents in explainable ai interaction design. Journal on Multimodal User Interfaces, 15(2):87- 98, 2021.

[51] Astrid Bertrand, Tiphaine Viard, Rafik Belloum, James R. Eagan, and Winston Maxwell. On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI '23, pages 1- 21, New York, NY, USA, April 2023. Association for Computing Machinery.

[52] Yvonne Rogers, Helen Sharp, and Jennifer Preece. Interaction Design: Beyond Human- Computer Interaction. John Wiley & Sons, New Jersey, 6 edition, 2023.

[53] Thilo Spinner, Udo Schlegel, Hanna Schäfer, and Mennatallah El- Assady. explainer: A visual analytics framework for interactive and explainable machine learning. IEEE transactions on visualization and computer graphics, 26(1):1064- 1074, 2019.

[54] Michael Chromik and Andreas Butz. Human- xai interaction: a review and design principles for explanation user interfaces. In Human- Computer Interaction- INTERACT 2021: 18th IFIP TC 13 International Conference, Bari, Italy, August 30- September 3, 2021, Proceedings, Part II 18, pages 619- 640. Springer, 2021.

[55] Vijay Arya, Rachel KE Bellamy, Pin- Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C Hoffman, Stephanie Houde, Q Vera Liao, Ronny Luss, Aleksandra Mojsilović, et al. One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques. arXiv preprint arXiv:1909.03012, 2019.

[56] Robert R. Hoffman, Shane T. Mueller, Gary Klein, and Jordan Litman. Metrics for Explainable AI: Challenges and Prospects, February 2019. arXiv:1812.04608 [cs].

[57] Helena Löfström, Karl Hammar, and Ulf Johansson. A Meta Survey of Quality Evaluation Criteria in Explanation Methods. In Jochen De Weerdt and Artem Polyvyanyy, editors, Intelligent Information Systems, Lecture Notes in Business Information Processing, pages 55- 63, Cham, 2022. Springer International Publishing.

[58] Ingrid Nunes and Dietmar Jannach. A systematic review and taxonomy of explanations in decision support and recommender systems. User Modeling and User- Adapted Interaction, 27(3):393- 444, December 2017.

[59] Yao Rong, Tobias Leemann, Thai- trang Nguyen, Lisa Fiedler, Peizhu Qian, Vaibhav Unhelkar, Tina Seidel, Gjergji Kasneci, and Enkelejda Kasneci. Towards Human- centered Explainable AI: A Survey of User Studies for Model Explanations, December 2023. arXiv:2210.11584 [cs].

[60] Essi Pietilä and Pedro A. Moreno- Sánchez. When an Explanation is not Enough: An Overview of Evaluation Metrics of Explainable AI Systems in the Healthcare Domain. In

Almir Badnjević and Lejla Gurbeta Pokvić, editors, MEDICON'23 and CMBEBIH'23, pages 573- 584, Cham, 2024. Springer Nature Switzerland.[61] Adrien Bibal and Benoît Frénay. Interpretability of machine learning models and representations: an introduction. In 24th european symposium on artificial neural networks, computational intelligence and machine learning, pages 77- 82. CIACO, 2016. [62] Finale Doshi- Velez and Been Kim. Considerations for evaluation and generalization in interpretable machine learning. Explainable and interpretable models in computer vision and machine learning, pages 3- 17, 2018. [63] Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlotterer, Maurice Van Keulen, and Christin Seifert. From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai. ACM Computing Surveys, 55(13s):1- 42, 2023. [64] Waddan Saeed and Christian Omlin. Explainable AI (XAI): A systematic meta- survey of current challenges and future opportunities. Knowledge- Based Systems, 263:110273, March 2023. [65] Adulam Jeyasothy, Thibault Laugel, Marie- Jeanne Lesot, Christophe Marsala, and Marcin Detyniecki. A general framework for personalising post hoc explanations through user knowledge integration. International Journal of Approximate Reasoning, 160:108944, 2023. [66] Rossitza Setchi, Maryam Banitalebi Dehkordi, and Juwairiya Siraj Khan. Explainable robotics in human- robot interactions. Procedia Computer Science, 176:3057- 3066, 2020. [67] Lei He, Nabil Aouf, and Bifeng Song. Explainable deep reinforcement learning for uav autonomous path planning. Aerospace science and technology, 118:107052, 2021. [68] Dina El- Zanfaly, Yiwei Huang, and Yanwen Dong. Sand- in- the- loop: Investigating embodied co- creation for shared understandings of generative ai. In Companion publication of the 2023 ACM designing interactive systems conference, pages 256- 260, 2023. [69] John W Creswell and Mariko Hirose. Mixed methods and survey research in family medicine and community health. Family Medicine and Community Health, 7(2):e000086, March 2019. [70] Vivian Lai, Chacha Chen, Q. Vera Liao, Alison Smith- Renner, and Chenhao Tan. Towards a Science of Human- AI Decision Making: A Survey of Empirical Studies, December 2021. arXiv:2112.11471 [cs].[71] Saranya A. and Subhashini R. A systematic review of Explainable Artificial Intelligence models and applications: Recent developments and future trends. Decision Analytics Journal, 7:100230, June 2023. [72] Chun- Hua Tsai and Peter Brusilovsky. Evaluating Visual Explanations for Similarity- Based Recommendations: User Perception and Performance. In Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization, UMAP '19, pages 22- 30, New York, NY, USA, June 2019. Association for Computing Machinery.[73] Viktor Eisenstadt, Christian Espinoza- Stapelfeld, Ada Mikyas, and Klaus- Dieter Althoff. Explainable Distributed Case- Based Support Systems: Patterns for Enhancement and Validation of Design Recommendations. In Michael T. Cox, Peter Funk, and Shahina Begum, editors, Case- Based Reasoning Research and Development, volume 11156, pages 78- 94. Springer International Publishing, Cham, 2018. Series Title: Lecture Notes in Computer Science.[74] Cristina Conati, Oswald Barral, Vanessa Putnam, and Lea Rieger. Toward personalized XAI: A case study in intelligent tutoring systems. Artificial Intelligence, 298:103503, September 2021. [75] Christos Sardianos, Iraklis Varlamis, Christos Chronis, George Dimitrakopoulos, Abdullah Alsalemi, Yassine Himeur, Faycal Bensaali, and Abbes Amira. The emergence of explainability of intelligent systems: Delivering explainable and personalized recommendations for energy efficiency. International Journal of Intelligent Systems, 36(2):656- 680, February 2021.

[76] Molika Meas, Ram Machlev, Ahmet Kose, Aleksei Tepljakov, Lauri Loo, Yoash Levron, Eduard Petlenkov, and Juri Belikov. Explainability and Transparency of Classifiers for Air- Handling Unit Faults Using Explainable Artificial Intelligence (XAI). Sensors, 22(17):6338, August 2022.

[77] Benedikt Leichtmann, Christina Humer, Andreas Hinterreiter, Marc Streit, and Martina Mara. Effects of Explainable Artificial Intelligence on trust and human behavior in a high- risk decision task. Computers in Human Behavior, 139:107539, February 2023.

[78] Monika Westphal, Michael Vössing, Gerhard Satzger, Galit B. Yom- Tov, and Anat Rafaeli. Decision control and explanations in human- AI collaboration: Improving user perceptions and compliance. Computers in Human Behavior, 144:107714, July 2023.

[79] Marko Tešić and Ulrike Hahn. Can counterfactual explanations of AI systems' predictions skew lay users' causal intuitions about the world? If so, can we correct for that? Patterns, 3(12):100635, December 2022.

[80] Maria Riveiro and Serge Thill. "That's (not) the output I expected!" On the role of end user expectations in creating explanations of AI systems. Artificial Intelligence, 298:103507, September 2021.

[81] Giuseppe Desolda, Joseph Aneke, Carmelo Ardito, Rosa Lanzilotti, and Maria Francesca Costabile. Explanations in warning dialogs to help users defend against phishing attacks. International Journal of Human- Computer Studies, 176:103056, August 2023.

[82] Anjana Wijekoon and Nirmalie Wiratunga. A user- centred evaluation of DisCERN: Discovering counterfactuals for code vulnerability detection and correction. Knowledge- Based Systems, 278:110830, October 2023.

[83] Francisco Cruz, Charlotte Young, Richard Dazeley, and Peter Vamplew. Evaluating Human- like Explanations for Robot Actions in Reinforcement Learning Scenarios, July 2022. arXiv:2207.03214 [cs].

[84] Thrupthi Ann John, Vineeth N Balasubramanian, and C V Jawahar. Explaining Deep Face Algorithms Through Visualization: A Survey. IEEE Transactions on Biometrics, Behavior, and Identity Science, pages 1- 1, 2023.

[85] Yazan Mualla, Igor Tchappi, Timotheus Kampik, Amro Najjar, Davide Calvaresi, Abdeljalil Abbas- Turki, Stéphane Galland, and Christophe Nicolle. The quest of parsimonious XAI: A human- agent architecture for explanation formulation. Artificial Intelligence, 302:103573, January 2022.

[86] Sam Hepenstal, Leishi Zhang, Neesha Kodagoda, and B. I. william Wong. Developing Conversational Agents for Use in Criminal Investigations. ACM Transactions on Interactive Intelligent Systems, 11(3- 4):25:1- 25:35, September 2021.

[87] Joe Collenette, Katie Atkinson, and Trevor Bench- Capon. Explainable AI tools for legal reasoning about cases: A study on the European Court of Human Rights. Artificial Intelligence, 317:103861, April 2023.

[88] Aditya Dey, Chandan Radhakrishna, Nishitha Nancy Lima, Suraj Shashidhar, Sayantan Polley, Marcus Thiel, and Andreas Nurnberger. Evaluating Reliability in Explainable Search. In 2021 IEEE 2nd International Conference on Human- Machine Systems (ICHMS), pages 1- 4, Magdeburg, Germany, September 2021. IEEE.

[89] Vincent Lully, Philippe Laublet, Milan Stankovic, and Filip Radulovic. Enhancing explanations in recommender systems with knowledge graphs. Procedia Computer Science, 137:211- 222, 2018.

[90] Sameen Maruf, Ingrid Zukerman, Ehud Reiter, and Gholamreza Haffari. Influence of context on users' views about explanations for decision- tree predictions. Computer Speech & Language, 81:101483, June 2023.

[91] Kanika Goel, Renuka Sindhgatta, Sumit Kalra, Rohan Goel, and Preeti Mutreja. The effect of machine learning explanations on user trust for automated diagnosis of COVID- 19. Computers in Biology and Medicine, 146:105587, July 2022.

[92] Mauro Dragoni, Ivan Donadello, and Claudio Eccher. Explainable AI meets persuasiveness: Translating reasoning results into behavioral change advice. Artificial Intelligence in Medicine, 105:101840, May 2020.

[93] Julia Graefe, Selma Paden, Doreen Engelhardt, and Klaus Bengler. Human Centered Explainability for Intelligent Vehicles - A User Study. In Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI '22, pages 297- 306, New York, NY, USA, September 2022. Association for Computing Machinery.

[94] Google. Components.

[95] Severino Ribecca. The Data Visualisation Catalogue.

[96] Christian Meske and Enrico Bunde. Design Principles for User Interfaces in AI- Based Decision Support Systems: The Case of Explainable Hate Speech Detection. Information Systems Frontiers, 25(2):743- 773, April 2023.

[97] Sumedha Singla, Motahhare Eslami, Brian Pollack, Stephen Wallace, and Kayhan Batmanghelich. Explaining the black- box smoothly- - A counterfactual approach. Medical Image Analysis, 84:102721, February 2023.

[98] Valdemar Darry, Pat Pataranutaporn, Yaoli Mao, and Pattie Maes. Wearable Reasoner: Towards Enhanced Human Rationality Through A Wearable Device With An Explainable AI Assistant. In Proceedings of the Augmented Humans International Conference, pages 1- 12, Kaiserslautern Germany, March 2020. ACM.

[99] Cataldo Musto, Fedelucio Narducci, Pasquale Lops, Marco De Gemmis, and Giovanni Semeraro. Linked open data- based explanations for transparent recommender systems. International Journal of Human- Computer Studies, 121:93- 107, January 2019.

[100] Sara Tandon and Jennifer Wang. Surfacing AI Explainability in Enterprise Product Visual Design to Address User Tech Proficiency Differences. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, CHI EA '23, pages 1- 8, New York, NY, USA, April 2023. Association for Computing Machinery.

[101] Mahsan Nourani, Donald R. Honeycutt, Jeremy E. Block, Chiradeep Roy, Tahrima Rahman, Eric D. Ragan, and Vibhav Gogate. Investigating the Importance of First Impressions and Explainable AI with Interactive Video Analysis. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1- 8, Honolulu HI USA, April 2020. ACM.

[102] Hariharan Subramonyam, Colleen Seifert, and Eytan Adar. ProtoAI: Model- Informed Prototyping for AI- Powered Interfaces. In 26th International Conference on Intelligent User Interfaces, pages 48- 58, College Station TX USA, April 2021. ACM.

[103] Sarath Sreedharan, Siddharth Srivastava, and Subbarao Kambhampati. Using state abstractions to compute personalized contrastive explanations for AI agent behavior. Artificial Intelligence, 301:103570, December 2021.

[104] Jinglu Jiang, Surinder Kahai, and Ming Yang. Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty. International Journal of Human- Computer Studies, 165:102839, September 2022.

[105] Minjung Kim, Saebyeol Kim, Jinwoo Kim, Tae- Jin Song, and Yuyoung Kim. Do stakeholder needs differ? - Designing stakeholder- tailored Explainable Artificial Intelligence (XAI) interfaces. International Journal of Human- Computer Studies, 181:103160, January 2024.

[106] Carina Manger, Jakob Peintner, Marion Hoffmann, Mirella Probst, Raphael Wennmacher, and Andreas Riener. Providing Explainability in Safety- Critical Automated Driving Situations

through Augmented Reality Windshield HMIs. In Adjunct Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, pages 174- 179, Ingolstadt Germany, September 2023. ACM.[107] Richard Y. Wang and Diane M. Strong. Beyond Accuracy: What Data Quality Means to Data Consumers. Journal of Management Information Systems, 12(4):5- 33, 1996. [108] Luisa Pumplun, Felix Peters, Joshua Gawlitzal and Peter Buxmann. Bringing Machine Learning Systems into Clinical Practice: A Design Science Approach to Explainable Machine Learning- Based Clinical Decision Support Systems. Journal of the Association for Information Systems, 24(4):953- 979, January 2023. [109] Jiao Sun, Q. Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde, Kartik Talamadupula, and Justin D. Weisz. Investigating Explainability of Generative AI for Code through Scenario- based Design. In 27th International Conference on Intelligent User Interfaces, pages 212- 228, Helsinki Finland, March 2022. ACM.[110] Adil Mukhtar, Birgit Hofer, Dietmar Jannach, and Franz Wotawa. Explaining software fault predictions to spreadsheet users. Journal of Systems and Software, 201:111676, July 2023. [111] Sven Schultze, Ani Withoft, Larbi Abdenebaoui, and Susanne Boll. Explaining Image Aesthetics Assessment: An Interactive Approach. In Proceedings of the 2023 ACM International Conference on Multimedia Retrieval, pages 20- 28, Thessaloniki Greece, June 2023. ACM.[112] Tim Draws, Karthikeyan Natesan Ramamurthy, Ioana Baldini, Amit Dhurandhar, Inkit Padhi, Benjamin Timmermans, and Nava Tintarev. Explainable Cross- Topic Stance Detection for Search Results. In Proceedings of the 2023 Conference on Human Information Interaction and Retrieval, pages 221- 235, Austin TX USA, March 2023. ACM.[113] Matthew L. Olson, Roli Khanna, Lawrence Neal, Fuxin Li, and Weng- Keen Wong. Counterfactual state explanations for reinforcement learning agents via generative deep learning. Artificial Intelligence, 295:103455, June 2021. [114] Mohammad Naiseh, Dena Al- Thani, Nan Jiang, and Raian Ali. How the different explanation classes impact trust calibration: The case of clinical decision support systems. International Journal of Human- Computer Studies, 169:102941, January 2023. [115] Jasminko Novak, Tina Maljur, and Kalina Drenska. Transferring AI Explainability to User- Centered Explanations of Complex COVID- 19 Information. In Jessie Y. C. Chen, Gino Fragomeni, Helmut Degen, and Stavroula Ntoa, editors, HCI International 2022 - Late Breaking Papers: Interacting with eXtended Reality and Artificial Intelligence, Lecture Notes in Computer Science, pages 441- 460, Cham, 2022. Springer Nature Switzerland.[116] Raphaela Butz, Renee Schulz, Arjen Hommersom, and Marko van Eekelen. Investigating the understandability of XAI methods for enhanced user experience: When Bayesian network users became detectives. Artificial Intelligence in Medicine, 134:102438, December 2022. [117] Sule Anjomshoea, Daniel Omeiza, and Lili Jiang. Context- based image explanations for deep neural networks. Image and Vision Computing, 116:104310, December 2021. [118] Chun- Hua Tsai and Peter Brusilovsky. The effects of controllability and explainability in a social recommender system. User Modeling and User- Adapted Interaction, 31(3):591- 627, July 2021. [119] Pedro Sequeira and Melinda Gervasio. Interestingness elements for explainable reinforcement learning: Understanding agents' capabilities and limitations. Artificial Intelligence, 288:103367, November 2020. [120] Sophia Schulze- Weddige and Thorsten Zylowski. User Study on the Effects Explainable AI Visualizations on Non- experts. In Matthias Wölfel, Johannes Bernhardt, and Sonja Thiel, editors, ArtsIT, Interactivity and Game Creation, Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering, pages 457- 467, Cham, 2022. Springer International Publishing.

[121] Daricia Wilkinson, Öznur Alkan, Q. Vera Liao, Massimiliano Mattetti, Inge Vejsbjerg, Bart P. Knijnenburg, and Elizabeth Daly. Why or Why Not? The Effect of Justification Styles on Chatbot Recommendations. ACM Transactions on Information Systems, 39(4):1- 21, October 2021.

[122] Milad Moradi and Matthias Samwald. Post- hoc explanation of black- box classifiers using confident itemsets. Expert Systems with Applications, 165:113941, March 2021.

[123] Wei Du, Qiang Yan, Wenping Zhang, and Jian Ma. Leveraging online behaviors for interpretable knowledge- aware patent recommendation. Internet Research, 32(2):568- 587, March 2022.

[124] Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation, June 2023. arXiv:2306.01220 [cs].

[125] Eoin M. Kenny, Courtney Ford, Molly Quinn, and Mark T. Keane. Explaining black- box classifiers using post- hoc explanations- by- example: The effect of explanations and error- rates in XAI user studies. Artificial Intelligence, 294:103459, May 2021.

[126] Zekun Yang and Juan Feng. Explainable multi- task convolutional neural network framework for electronic petition tag recommendation. Electronic Commerce Research and Applications, 59:101263, May 2023.

[127] Sangyeon Kim, Sanghyun Choo, Donghyun Park, Hoonseok Park, Chang S. Nam, Jae- Yoon Jung, and Sangwon Lee. Designing an XAI interface for BCI experts: A contextual design for pragmatic explanation interface based on domain knowledge in a specific context. International Journal of Human- Computer Studies, 174:103009, June 2023.

[128] Murat Dikmen and Catherine Burns. The effects of domain knowledge on trust in explainable AI and task performance: A case of peer- to- peer lending. International Journal of Human- Computer Studies, 162:102792, June 2022.

[129] Jasper Van Der Waa, Elisabeth Nieuwburg, Anita Cremers, and Mark Neerincx. Evaluating XAI: A comparison of rule- based and example- based explanations. Artificial Intelligence, 291:103404, February 2021.

[130] Chun- Hua Tsai and Peter Brusilovsky. Explaining recommendations in an interactive hybrid social recommender. In Proceedings of the 24th International Conference on Intelligent User Interfaces, pages 391- 396, Marina del Ray California, March 2019. ACM.

[131] Lukasz Gorski and Shashishekar Ramakrishna. Explainable artificial intelligence, lawyer's perspective. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law, pages 60- 68, Sao Paulo Brazil, June 2021. ACM.

[132] Jasper van der Waa, Tjeerd Schoonderwoerd, Jurriaan van Diggelen, and Mark Neerincx. Interpretable confidence measures for decision support systems. International Journal of Human- Computer Studies, 144:102493, 2020.

[133] Xuejiao Zhao, Huanhuan Chen, Zhenchang Xing, and Chunyan Miao. Brain- Inspired Search Engine Assistant Based on Knowledge Graph. IEEE Transactions on Neural Networks and Learning Systems, 34(8):4386- 4400, August 2023.

[134] Tjeerd A.J. Schoonderwoerd, Wiard Joritsma, Mark A. Neerincx, and Karel Van Den Bosch. Human- centered XAI: Developing design patterns for explanations of clinical decision support systems. International Journal of Human- Computer Studies, 154:102684, October 2021.

[135] Roberto Confalonieri, Tillman Weyde, Tarek R. Besold, and Fermín Moscoso Del Prado Martín. Using ontologies to enhance human understandability of global post- hoc explanations of black- box models. Artificial Intelligence, 296:103471, July 2021.

[136] Andrew Silva, Mariah Schrum, Erin Hedlund- Botti, Nakul Gopalan, and Matthew Gombolay. Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of

xAI on Human- Agent Interaction. International Journal of Human- Computer Interaction, 39(7):1390- 1404, April 2023.

[137] Theodore Evans, Carl Orge Retzlaff, Christian Geißler, Michaela Kargl, Markus Plass, Heimo Müller, Tim- Rasmus Kiehl, Norman Zerbe, and Andreas Holzinger. The explainability paradox: Challenges for xAI in digital pathology. Future Generation Computer Systems, 133:281- 296, August 2022.

[138] Oskar Wysocki, Jessica Katharine Davies, Markel Vigo, Anne Caroline Armstrong, Dónal Landers, Rebecca Lee, and André Freitas. Assessing the communication gap between AI models and healthcare professionals: Explainability, utility and trust in AI- driven clinical decision- making. Artificial Intelligence, 316:103839, March 2023.

[139] Shi- Jun Luo, Hyoil Han, Qiong Chang, and Jun Miyazaki. Automatic Extraction of Effective Relations in Knowledge Graph for a Recommendation Explanation System. In Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing, SAC '23, pages 1754- 1761, New York, NY, USA, June 2023. Association for Computing Machinery.

[140] Sarath Sreedharan, Tathagata Chakraborti, and Subbarao Kambhampati. Foundations of explanations as model reconciliation. Artificial Intelligence, 301:103558, December 2021.

[141] Panagiotis Symeonidis, Lidija Kirjackaja, and Markus Zanker. Session- based recommendation along with the session style of explanation. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 404- 420. Springer, 2022.

[142] Ines Neves, Duarte Folgado, Sara Santos, Marilia Barandas, Andrea Campagner, Luca Ronzio, Federico Cabitza, and Hugo Gamboa. Interpretable heartbeat classification using local model- agnostic explanations on ECGs. Computers in Biology and Medicine, 133:104393, June 2021.

[143] Doha Kim, Yeosol Song, Songyie Kim, Sewang Lee, Yanqin Wu, Jungwoo Shin, and Daeho Lee. How should the results of artificial intelligence be explained to users? - Research on consumer preferences in user- centered explainable artificial intelligence. Technological Forecasting and Social Change, 188:122343, March 2023.

[144] Krisztian Baog, Filip Radlinski, and Shushan Arakelyan. Transparent, Scrutable and Explainable User Models for Personalized Recommendation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 265- 274, Paris France, July 2019. ACM.

[145] Mohammad Naiseh, Reem S. Al- Mansoori, Dena Al- Thani, Nan Jiang, and Raian Ali. Nudging through Friction: An Approach for Calibrating Trust in Explainable AI. In 2021 8th International Conference on Behavioral and Social Computing (BESC), pages 1- 5, Doha, Qatar, October 2021. IEEE.

[146] Donghee Shin. The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. International Journal of Human- Computer Studies, 146:102551, February 2021.

[147] Andreas Holzinger, André Carrington, and Heimo Müller. Measuring the Quality of Explanations: The System Causability Scale (SCS): Comparing Human and Machine Explanations. KI - Künstliche Intelligenz, 34(2):193- 198, June 2020.

[148] Kacper Sokol and Peter Flach. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 56- 67, Barcelona Spain, January 2020. ACM.

[149] Heidi Vainio- Pekka, Mamia Ori- Otse Agbese, Marianna Jantunen, Ville Vakkuri, Tommi Mikkonen, Rebekah Rousi, and Pekka Abrahamsson. The role of explainable ai in the research field of ai ethics. ACM Transactions on Interactive Intelligent Systems, 13(4):1- 39, 2023.

[150] Anna Jobin, Marcello Ienca, and Effy Vayena. The global landscape of ai ethics guidelines. Nature machine intelligence, 1(9):389- 399, 2019.

[151] John A McDermid, Yan Jia, Zoe Porter, and Ibrahim Habli. Artificial intelligence explainability: the technical and ethical dimensions. Philosophical Transactions of the Royal Society A, 379(2207):20200363, 2021.

[152] Pearl Pu, Li Chen, and Rong Hu. Evaluating recommender systems from the user's perspective: survey of the state of the art. User Modeling and User- Adapted Interaction, 22:317- 355, 2012.

[153] John A. McDermid, Yan Jia, Zoe Porter, and Ibrahim Habli. Artificial intelligence explainability: the technical and ethical dimensions. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 379(2207):20200363, August 2021. Publisher: Royal Society.

[154] Yi Yu, Kazunari Sugiyama, and Adam Jatowt. Domain counterfactual data augmentation for explainable recommendation. ACM Transactions on Information Systems, 43(3):1- 30, 2025.

[155] Vagner Figueredo De Santana, Ana Fucs, Vinicius Segura, Daniel Brugnaro De Moraes, and Renato Cerqueira. Predicting the need for XAI from high- granularity interaction data. International Journal of Human- Computer Studies, 175:103029, July 2023.

[156] Kamran Alipour, Arijit Ray, Xiao Lin, Jurgen P. Schulze, Yi Yao, and Giedrius T. Burachas. The Impact of Explanations on AI Competency Prediction in VQA. In 2020 IEEE International Conference on Humanized Computing and Communication with Artificial Intelligence (HCCAI), pages 25- 32, Irvine, CA, USA, September 2020. IEEE.

[157] Giulia Vilone and Luca Longo. Classification of explainable artificial intelligence methods through their output formats. Machine Learning and Knowledge Extraction, 3(3):615- 661, 2021.

[158] Giulia Vilone and Luca Longo. Explainable artificial intelligence: a systematic review. arXiv preprint arXiv:2006.00093, 2020.

[159] Michael Chromik and Martin Schuessler. A taxonomy for human subject evaluation of black- box explanations in xai. Exss- atec@ iui, 1:1- 7, 2020.

[160] Vivian Lai, Chacha Chen, Alison Smith- Renner, Q Vera Liao, and Chenhao Tan. Towards a science of human- ai decision making: An overview of design space in empirical human- subject studies. In Proceedings of the 2023 ACM conference on fairness, accountability, and transparency, pages 1369- 1385, 2023.

[161] Hongyu Lu, Weizhi Ma, Yifan Wang, Min Zhang, Xiang Wang, Yiqun Liu, Tat- Seng Chua, and Shaoping Ma. User perception of recommendation explanation: Are your explanations what users need? ACM Transactions on Information Systems, 41(2):1- 31, 2023.

[162] Jiun- Yin Jian, Ann M Bisantz, and Colin G Drury. Foundations for an empirically determined scale of trust in automated systems. International journal of cognitive ergonomics, 4(1):53- 71, 2000.

[163] Arun Das and Paul Rad. Opportunities and challenges in explainable artificial intelligence (xai): A survey. arXiv preprint arXiv:2006.11371, 2020.

[164] Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng- Keen Wong. Too much, too little, or just right? ways explanations impact end users' mental models. In 2013 IEEE Symposium on visual languages and human centric computing, pages 3- 10. IEEE, 2013.

[165] Jiqun Liu, Matthew Mitsui, Nicholas J Belkin, and Chirag Shah. Task, information seeking intentions, and user behavior: Toward a multi- level understanding of web search. In Proceedings of the 2019 ACM SIGIR Conference on Human Information Interaction and Retrieval, pages 123- 132, 2019.

[166] Jiqun Liu. Toward cranfield- inspired reusability assessment in interactive information retrieval evaluation. Information Processing & Management, 59(5):103007, 2022.

[167] Tianji Jiang, Wenqi Li, and Jiqun Liu. The landscape of data reuse in interactive information retrieval: Motivations, sources, and evaluation of reusability. Journal of the Association for Information Science and Technology, 2024. 

[168] Jiqun Liu. Deconstructing search tasks in interactive information retrieval: A systematic review of task dimensions and predictors. Information Processing & Management, 58(3):102522, 2021. 

[169] Sebastian Lubos, Thi Ngoc Trang Tran, Alexander Felfernig, Seda Polat Erdemiz, and VietMan Le. Llm- generated explanations for recommender systems. In Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization, pages 276- 285, 2024. 

[170] Johannes Schneider. Explainable generative ai (genxai): A survey, conceptualization, and research agenda. Artificial Intelligence Review, 57(11):289, 2024. 

[171] Gesina Schwalbe and Bettina Finzel. A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts. Data Mining and Knowledge Discovery, 38(5):3043- 3101, 2024. 

[172] Francisco Herrera. Making sense of the unensible: Reflection, survey, and challenges for xai in large language models toward human- centered ai. arXiv preprint arXiv:2505.20305, 2025. 

[173] Haoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large language models. arXiv preprint arXiv:2401.12874, 2024. 

[174] Christian Meske, Enrico Bunde, Johannes Schneider, and Martin Gersch. Explainable artificial intelligence: objectives, stakeholders, and future research opportunities. Information systems management, 39(1):53- 63, 2022. 

[175] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and Technology, 15(2):1- 38, 2024. 

[176] Jiqun Liu, Jamshed Karimnazarov, and Ryen W White. Trapped by expectations: Functional fixedness in llm- enabled chat search. arXiv preprint arXiv:2504.02074, 2025. 

[177] Ben Wang and Jiqun Liu. Understanding users' dynamic perceptions of search gain and cost in sessions: An expectation confirmation model. Journal of the Association for Information Science and Technology, 75(9):937- 956, 2024. 

[178] Jiqun Liu and Jiangen He. Boundedly rational searchers interacting with medical misinformation: Characterizing context- dependent decoy effects on credibility and usefulness evaluation in sessions. In Proceedings of the 2025 ACM SIGIR Conference on Human Information Interaction and Retrieval, pages 154- 167, 2025. 

[179] Agathe Balayn, Mireia Yurrita, Fanny Rancourt, Fabio Casati, and Ujwal Gadiraju. Unpacking trust dynamics in the llm supply chain: An empirical exploration to foster trustworthy llm production & use. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 1- 20, 2025. 

[180] Nina Corvelo Benz and Manuel Rodriguez. Human- aligned calibration for ai- assisted decision making. Advances in Neural Information Processing Systems, 36:14609- 14636, 2023.

# A Appendix

Table 7: Database and search query  

<table><tr><td></td><td>Database</td><td>Field</td><td>Search query</td><td>Search Results</td></tr><tr><td rowspan="11">Query 1</td><td>The ACM full-text col-lection</td><td>Abstract</td><td>&quot;search*&quot; OR &quot;recommendation*&quot; AND &quot;user*&quot; AND &quot;explainability&quot;</td><td>77</td></tr><tr><td>ScienceDirect</td><td>Title, abstract or author-specified keywords</td><td>(&quot;search&quot; OR &quot;recommendation&quot;) AND &quot;user&quot; AND &quot;explainability&quot;</td><td>536</td></tr><tr><td>PsycInfo</td><td>Abstract</td><td>AB (&quot;search*&quot; OR &quot;recommendation*&quot; ) AND AB (&quot;user*&quot; AND &quot;explainability*&quot;)</td><td>11</td></tr><tr><td>Wiley Online Library</td><td>Abstract</td><td>(&quot;search*&quot; OR &quot;recommendation*&quot;) AND &quot;user*&quot; AND &quot;explainability&quot;</td><td>3</td></tr><tr><td>Web of Science</td><td>Abstract</td><td>AB=&quot;(&quot;search*&quot; OR &quot;recommendation*&quot; ) AND &quot;user*&quot; AND &quot;explainability&quot;</td><td>174</td></tr><tr><td>Google Scholar</td><td>Full text</td><td>user explainability search OR recommendation</td><td>top200</td></tr><tr><td>IEEE</td><td>Title, abstract or author-specified keywords</td><td>(&quot;All Metadata&quot;:user) AND (&quot;All Metadata&quot;:explainability) AND (&quot;All Meta-data&quot;:search) OR (&quot;All Metadata&quot;:recommendation)</td><td>top200</td></tr><tr><td>MIS Quarterly (from Google Scholar)</td><td>Full text</td><td>user explainability search OR recommendation source:MIS source:Quarterly</td><td>8</td></tr><tr><td>IS research (from Google Scholar)</td><td>Full text</td><td>user explainability search OR recommendation source:Information source:Systems source: Research</td><td>12</td></tr><tr><td>Management science (from Google Scholar)</td><td>Full text</td><td>user explainability search OR recommendation source:Management source:Science</td><td>23</td></tr><tr><td>Google Scholar</td><td>Full text</td><td>explanability AI OR search OR &quot;information retrieval&quot; OR recommendation &quot;user explanation&quot;</td><td>top200</td></tr><tr><td rowspan="11">Query 2</td><td>The ACM full-text col-lection</td><td>Abstract</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation)</td><td>147</td></tr><tr><td>Web of Science</td><td>Abstract</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation)</td><td>234</td></tr><tr><td>The ACM full-text col-lection</td><td>Abstract</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation OR conversational agent OR conversational system)</td><td>168</td></tr><tr><td>Web of Science</td><td>Abstract</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation OR conversational agent OR conversational system)</td><td>232</td></tr><tr><td>PsycInfo</td><td>Abstract</td><td>AB (&quot;user&quot; AND &quot;explainability&quot;) AND AB (&quot;AI&quot; OR &quot;search&quot; OR &quot;information retrieval&quot; OR &quot;recommendation&quot; OR &quot;conversational agent&quot; OR &quot;conversational system&quot;), allow relevant terms</td><td>18</td></tr><tr><td>Wiley Online Library</td><td>Abstract</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation)</td><td>0</td></tr><tr><td>Wiley Online Library</td><td>Abstract</td><td>&quot;user&quot; AND &quot;explainability&quot;</td><td>10</td></tr><tr><td>ScienceDirect</td><td>Title, abstract or author-specified keywords</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation OR conversational agent OR conversational system)</td><td>764</td></tr><tr><td>Google Scholar</td><td>Full text</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation OR conversational agent OR conversational system)</td><td>top100</td></tr><tr><td>IEEE Xplore</td><td>Abstract</td><td>user explainability OR explainability&quot; AND (AI OR search OR information retrieval OR recommendation OR conversational agent OR conversational system)</td><td>95</td></tr><tr><td>MIS Quarterly (from Google Scholar)</td><td>Full text</td><td>&quot;user&quot; AND &quot;explainability&quot; AND (AI OR search OR information retrieval OR recommendation OR conversational agent OR conversational system)</td><td>4</td></tr></table>

Table 8: A full list of keyword frequency of the definitions in the three categories  

<table><tr><td>Keyword</td><td>Frequency</td><td>Example Context</td></tr><tr><td>XAI</td><td>45</td><td>XAI is a branch of AI intended to explain actions and decisions to humans</td></tr><tr><td>system</td><td>21</td><td>XAI refers to a system to produce an explanation for the user&#x27;s understanding of the model itself and individual decisions of the model</td></tr><tr><td>understand</td><td>15</td><td>At a high level, XAI is often defined as an initiative or effort made to improve AI transparency and users&#x27; understanding of AI</td></tr><tr><td>decision</td><td>13</td><td>Explainable Artificial Intelligence (XAI) is a field of study aimed at reliably and efficiently capturing the AI decision-making process, and interpreting and reporting it to a human audience</td></tr><tr><td>model</td><td>13</td><td>With artificial intelligence dominating in major fields, it becomes imperative to create AI models that are transparent in the sense that the user is presented with an explanation of why the model generated a certain output or made a specific decision, all while preserving the high performance and accuracy of the model</td></tr><tr><td>user</td><td>13</td><td>Explainable AI (XAI) is a means of AI design where recommendations are supported by explanations to facilitate users&#x27; trust calibration process. Explanations provide decisionmakers with insights into how the machine derived a recommendation. Explanations are supposed to help humans identify situations where AI recommendations can be incorrect in specific contexts and cases</td></tr><tr><td>human</td><td>8</td><td>Explainable Artificial Intelligence (XAI) aims at explaining the algorithmic decisions of AI solutions with non-technical terms in order to make these decisions trusted and easily understandable by humans</td></tr><tr><td>transparency</td><td>8</td><td>Explainable AI (XAI) is seen as a way to achieve such transparency and interpretability</td></tr><tr><td>recommendation</td><td>6</td><td>eXplainable AI (XAI) refers to an AI component that explains AI recommendations to humans receiving them.</td></tr><tr><td>interpretable</td><td>5</td><td>Interpretable machine learning is a subfield of explainable AI, with many different surveys aiming merely to give an overview of inherently interpretable models vs black-box models, while others dive more deeply into the different views and perspectives associated with explainable AI</td></tr><tr><td>blackbox</td><td>4</td><td>The research field of explainable AI (XAI) tackles the black box problem by introducing transparent models as well as techniques for generating different types of explanations for black box models</td></tr><tr><td>ML</td><td>4</td><td>explaining the predictions made by ML models</td></tr><tr><td>specific</td><td>4</td><td>An eXplainable RL (XRL) system that enables humans to correctly understand the agent&#x27;s aptitude in a specific task</td></tr><tr><td>trust</td><td>4</td><td>Explainable AI (XAI) is a means of AI design where recommendations are supported by explanations to facilitate users&#x27; trust calibration process</td></tr><tr><td>help</td><td>3</td><td>It helps people understand the decision-making process of AI algorithms by bringing transparency and accountability into AI systems</td></tr><tr><td>prediction</td><td>3</td><td>Explainable AI (XAI) has emerged as a field to address this need for AI systems&#x27; predictions to be followed by explanations of these predictions</td></tr><tr><td>result</td><td>3</td><td>XAI can be defined as the field that aims to make AI systems results more understandable to humans</td></tr><tr><td>technology</td><td>3</td><td>Explainable Artificial Intelligence (XAI) aims at explaining the algorithmic decisions of AI solutions with non-technical terms in order to make these decisions trusted and easily understandable by humans</td></tr><tr><td>inner workings</td><td>3</td><td>Explainable AI (XAI) suggests that having AI systems explain their inner workings to their end users</td></tr><tr><td>accuracy</td><td>2</td><td>With artificial intelligence dominating in major fields, it becomes imperative to create AI models that are transparent in the sense that the user is presented with an explanation of why the model generated a certain output or made a specific decision, all while preserving the high performance and accuracy of the model</td></tr><tr><td>algorithm</td><td>2</td><td>Explainable Artificial Intelligence (XAI) aims at explaining the algorithmic decisions of AI solutions with non-technical terms in order to make these decisions trusted and easily understandable by humans</td></tr><tr><td>improve</td><td>2</td><td>At a high level, XAI is often defined as an initiative or effort made to improve AI transparency and users&#x27; understanding of AI (Adadi and Berrada, 2018).</td></tr><tr><td>output</td><td>2</td><td>A major goal of Explainable Artificial Intelligence (XAI) is to have AI-systems construct explanations for their own output.</td></tr><tr><td>reason</td><td>2</td><td>Explainable models attempt to provide reason and causality behind their decisions</td></tr><tr><td>reveal</td><td>2</td><td>Explainable models as methods that reveal how AI decisions are formed, helping users to understand and appropriately trust AI systems</td></tr></table>

<table><tr><td colspan="3">Explanation</td></tr><tr><td>decision</td><td>10</td><td>A useful explanation model would help users to understand the recommendation reasoning process, which allows the users to make a better decision or persuade them to accept the suggestions from a system</td></tr><tr><td>recommendation</td><td>10</td><td>Explanations are generally initiated by an information provider to clarify or justify the recommendation and convince the recipient to comply with it</td></tr><tr><td>model</td><td>7</td><td>Explanation: an interface between human and system that accurately approximates the model of the system and is comprehensible to the human</td></tr><tr><td>understand</td><td>7</td><td>how the reasoning process of an intelligent information system should be made understandable to the user</td></tr><tr><td>user</td><td>7</td><td>We consider explanations in the framework of counterfactual reasoning, where a user who is confused by the agent&#x27;s activity (or proposed activity) presents alternative behavior that they would have expected the agent to execute.</td></tr><tr><td>reason</td><td>6</td><td>An explanation could be “a summary of some reasoning or provenance for facts”</td></tr><tr><td>feature</td><td>5</td><td>Explanations may simply offer insight into how the decision was reached (e.g., highlighting important features, presenting answer probabilities, etc.)</td></tr><tr><td>help</td><td>5</td><td>An explanation is sometimes a justification of why items have been recommended, while sometimes an item description which helps users to understand the qualities of the item well enough to decide whether it is relevant for them or not</td></tr><tr><td>item</td><td>4</td><td>Generally, explanations “seek to show how a recommended item relates to a user’s preferences”</td></tr><tr><td>system</td><td>4</td><td>Explanation: an interface between human and system that accurately approximates the model of the system and is comprehensible to the human</td></tr><tr><td>describe</td><td>3</td><td>a contrastive explanation describes “why event A occurred as opposed to some alternative event B”</td></tr><tr><td>human</td><td>3</td><td>“explanation”, a term that in this context denotes any indication that could help the human decision-maker (in our case, the ECG reader) understand the output of the decision support (in our case an ML classifier)</td></tr><tr><td>information</td><td>3</td><td>Explanations are generally initiated by an information provider to clarify or justify the recommendation and convince the recipient to comply with it</td></tr><tr><td>insight</td><td>3</td><td>One popular type of explanation, the justification, offers insight as to why a decision is a good one without necessarily describing the algorithmic details</td></tr><tr><td>justification</td><td>3</td><td>An explanation is sometimes a justification of why items have been recommended, while sometimes an item description which helps users to understand the qualities of the item well enough to decide whether it is relevant for them or not</td></tr><tr><td>present</td><td>3</td><td>A thorough explanation should explain what imaging features are present in those important locations, and how changing such features modifies the classification decision</td></tr><tr><td>alternative</td><td>2</td><td>We consider explanations in the framework of counterfactual reasoning, where a user who is confused by the agent’s activity (or proposed activity) presents alternative behavior that they would have expected the agent to execute in an informal way, an explanation can be seen as a narrative answering the question</td></tr><tr><td>answer</td><td>2</td><td>of why particular facts (e.g., an event or a decision) are happening. Such a narrative may explain things in terms of causality</td></tr><tr><td>behavior</td><td>2</td><td>a global explanation where the common behaviours of the tool are explained</td></tr><tr><td>causality</td><td>2</td><td>Explanations aim to reach a high level of causability, which can be defined as the extent to which users understand an explanation in an efficient, effective, and satisfying manner</td></tr><tr><td>classification</td><td>2</td><td>A thorough explanation should explain what imaging features are present in those important locations, and how changing such features modifies the classification decision</td></tr><tr><td>quality</td><td>2</td><td>An explanation is sometimes a justification of why items have been recommended, while sometimes an item description which helps users to understand the qualities of the item well enough to decide whether it is relevant for them or not</td></tr><tr><td>relate</td><td>2</td><td>Generally, explanations “seek to show how a recommended item relates to a user’s preferences”</td></tr><tr><td>specific</td><td>2</td><td>a local explanation where feature attribution explains why a specific code snippet is predicted to be vulnerable</td></tr></table>

<table><tr><td>human</td><td>9</td><td>explainability requires an accurate description of a decision-maker and defines how comprehensible the decisions are to humans</td></tr><tr><td>model</td><td>9</td><td>In a broader view, explainability encompasses everything that makes ML models transparent and understandable, also including information about the data, performance, etc</td></tr><tr><td>system</td><td>9</td><td>explainability emphasizes making ML systems comprehensible to humans through explanations</td></tr></table>

<table><tr><td>understand</td><td>9</td><td>Explainability is crucial for users to understand why certain results are being presented to them, particularly in the case of health information where the consequences of acting on incorrect or misleading information can be severe</td></tr><tr><td>user</td><td>8</td><td>Explainability focuses on making the recommendation process and the reasons behind specific recommendation more clear to the users</td></tr><tr><td>ability</td><td>7</td><td>Explainability includes the ability of humans to understand the explanation</td></tr><tr><td>information</td><td>7</td><td>Explainability is the term applied to the concept whereby a user is provided with sufficient information to be able to reconstruct why an AI-driven system made a prediction</td></tr><tr><td>decision</td><td>6</td><td>explainability methods that aid humans in verifying models&#x27; decisions or help them make better decisions</td></tr><tr><td>AI</td><td>5</td><td>Explainability has been identified as a requirement to promote reliability and trust in the AI output and also to ensure humans remain in control</td></tr><tr><td>item</td><td>4</td><td>the system&#x27;s ability to be able to explain to users why specific items are recommended</td></tr><tr><td>recommendation</td><td>4</td><td>explainability, i.e., the system may just need to justify why the recommendation was presented</td></tr><tr><td>xxx-based</td><td>3</td><td>Common approaches to (post hoc) explainability of specific predictions of AI systems include feature importance, saliency maps, and example-based methods</td></tr><tr><td>interpretable</td><td>3</td><td>Interpretability is a related term and can be defined as the level to which the user understands and can make use of the explanations given by the system and the information provided</td></tr><tr><td>ML</td><td>3</td><td>Explainability emerged as a research area aiming to address interpretability bottlenecks in ML models resulting from the fact that many of them can be effective in predicting an outcome, but not in explaining their underlying reasoning, which limits their practical application in critical areas</td></tr><tr><td>output</td><td>3</td><td>Explainability has been identified as a requirement to promote reliability and trust in the AI output and also to ensure humans remain in control</td></tr><tr><td>prediction</td><td>3</td><td>Explainability is the term applied to the concept whereby a user is provided with sufficient information to be able to reconstruct why an AI-driven system made a prediction</td></tr><tr><td>specific</td><td>3</td><td>the system&#x27;s ability to be able to explain to users why specific items are recommended</td></tr><tr><td>clearer</td><td>2</td><td>the ability of a model to make its functioning clearer to an audience</td></tr><tr><td>comprehensible</td><td>2</td><td>explainability requires an accurate description of a decision-maker and defines how comprehensible the decisions are to humans</td></tr><tr><td>everything</td><td>2</td><td>Explainability or Explainable AI (XAI) can be defined as everything that makes AI more understandable to human beings.</td></tr><tr><td>feature</td><td>2</td><td>Common approaches to (post hoc) explainability of specific predictions of AI systems include feature importance, saliency maps, and example-based methods</td></tr><tr><td>functioning</td><td>2</td><td>the ability of a model to make its functioning clearer to an audience</td></tr><tr><td>internal</td><td>2</td><td>Explainability is the extent to which the internal mechanics of algorithmic journalism can be explained in understandable human language</td></tr><tr><td>need</td><td>2</td><td>explainability, i.e., the system may just need to justify why the recommendation was presented</td></tr><tr><td>present</td><td>2</td><td>Explainability is crucial for users to understand why certain results are being presented to them, particularly in the case of health information where the consequences of acting on incorrect or misleading information can be severe</td></tr><tr><td>reason</td><td>2</td><td>Explainability focuses on making the recommendation process and the reasons behind specific recommendation more clear to the users</td></tr></table>