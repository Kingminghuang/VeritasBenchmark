# Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges

Sanjeda Akter*, Ibne Farabi Shihab*, Anuj Sharma†

Abstract—Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision- language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods (2023–2025) leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast- growing intersection of video understanding and foundation models.

Index Terms—Crash detection, large language models, video understanding, multimodal learning, VLMs, autonomous driving.

# I. INTRODUCTION

Crash detection is vital for autonomous vehicles, traffic monitoring, and emergency response. Traditionally, it has relied on heuristics or vision- based CNNs using motion cues. However, these methods struggle with generalization, occlusion, and complex temporal dependencies.

Large Language Models (LLMs), such as GPT- 4 and PaLM, have demonstrated impressive reasoning across modalities, especially when integrated with visual encoders like CLIP, Flamingo, or BLIP. This integration introduces a new paradigm: interpreting crash- related events in video via language- guided reasoning, captioning, and event prediction.

This survey aims to provide a structured overview of recent approaches leveraging LLMs/VLMs for crash understanding in video, summarizing models, datasets, and evaluation strategies.

# II. BACKGROUND AND RELATED WORK

The domain of video- based crash detection has evolved significantly, moving from classical computer vision techniques to modern deep learning and language- guided approaches. This section provides background on the foundational methods that preceded LLM integration and reviews the key developments in Vision- Language Models (VLMs) and their application to video understanding, which together set the stage for the methods surveyed in this paper.

# A. Crash Detection in Video

Classical approaches to crash detection in video include frame differencing, optical flow, background subtraction, and CNN- based classification. These methods rely heavily on low- level motion or scene cues, such as changes in pixel intensity, motion vectors, or spatial patterns, to identify anomalies indicative of crashes. Early research in this area often focused on trajectory- based analysis, where anomalies were detected if objects deviated from learned normal paths [1], [2]. However, these methods were sensitive to occlusion and relied on continuous tracking.

Subsequent approaches shifted to using low- level features that were more robust. Frame differencing detects sudden changes between consecutive frames, often used to identify abrupt vehicle movements [3]. Optical flow analyzes motion patterns to track vehicle trajectories, enabling detection of collisions or erratic behavior [4], [5]. Background subtraction isolates moving objects from static scenes, useful for detecting crash- related anomalies in traffic videos [6]. Other traditional methods leveraged hand- crafted features like Histograms of Oriented Gradients (HOG) and Histograms of Optical Flow (HOF) to capture appearance and motion information [7].

While effective in controlled settings, these approaches often struggle with the ambiguity and variability of real- world driving scenarios, such as varying lighting conditions or complex traffic interactions [8], [9]. This led to the adoption of deep learning, with CNN- based classification, such as using ResNet or YOLO, leveraging deep learning to classify video frames or regions as crash or non- crash events, often combining spatial and temporal features through architectures like Convolutional Long Short- Term Memory (Conv- LSTM) [3], [10]. However, even these methods can be limited, prompting the exploration of more robust methods like Large Language Models (LLMs) and Vision- Language Models (VLMs)

![](https://cdn-mineru.openxlab.org.cn/extract/50a417d4-4a78-4820-bc3c-dabb0987ed33/943ef13d965cb57c53923ebd53af55e3eead3ee317b7292bf98f6b44fad48d5f.jpg)  
Fig. 1: Taxonomy of LLM-based crash detection in videos categorized by fusion strategy, LLM role, and input granularity.

![](https://cdn-mineru.openxlab.org.cn/extract/50a417d4-4a78-4820-bc3c-dabb0987ed33/4c0a021bace002a387d9c7e79e93a9131fe9849d213753ee9460949c0e3cca6c.jpg)  
Fig. 2: Timeline for Video-capable LLMs for Crash Detection

# B. Vision-Language Models

Vision- Language Models (VLMs) integrate image or video encoders with text decoders or bidirectional transformers to enable multimodal understanding, combining visual and textual data for tasks such as video understanding and anomaly detection. Early VLMs, such as CLIP [23] and ALIGN [24], focus on matching images with text descriptions, leveraging large- scale imagetext pairs to learn robust cross- modal representations. These models excel in tasks like image classification and retrieval but lack temporal reasoning capabilities essential for video- based applications. Newer systems, including BLIP- 2 [25],Flamingo [26], and Video- LLaMA [27], incorporate spatiotemporal reasoning, making them well- suited for crash detection in video data. These advancements allow VLMs to process sequential video frames, capture dynamic patterns, and reason about complex traffic scenarios, addressing the limitations of classical approaches like frame differencing and CNNbased classification [3], [11].

1) Early VLMs: Image-Text Alignment: Early VLMs, such as CLIP and ALIGN, rely on contrastive learning to align image and text embeddings in a shared feature space. CLIP, for instance, is trained on 400 million image-text pairs, enabling zero-shot classification by matching visual content to textual descriptions [23]. In the context of crash detection, CLIP can classify individual video frames as crash or non-crash by comparing frame embeddings to textual prompts like "car accident" or "normal driving" [28]. Similarly, ALIGN uses a larger dataset of 1.8 billion image-text pairs to achieve robust alignment, improving performance on diverse visual scenarios [24]. However, these models are limited to static images, requiring additional temporal processing for video-based crash detection.

2) Advanced VLMs: Spatiotemporal Reasoning: Recent VLMs, such as BLIP-2, Flamingo, and VideoLLaMA, extend beyond static image processing to incorporate spatiotemporal reasoning, critical for analyzing dynamic crash events in videos. BLIP-2 leverages a bootstrapped approach, combining pre-trained vision

and language models to enhance multimodal tasks like video question answering and action recognition [25]. Its ability to process video frames as sequences enables detection of temporal anomalies, such as sudden vehicle collisions. Flamingo, built on a mixture of vision and language datasets, uses a gated cross- attention mechanism to integrate visual and textual inputs, achieving strong performance on video understanding tasks [26]. VideoLLaMA, specifically designed for video, employs a video encoder to capture temporal dynamics, aligning video features with language representations for tasks like crash detection [27]. These models can process video sequences to identify crash- related patterns, such as abrupt stops or collisions, by combining visual cues with textual reasoning.

3) Application to Crash Detection: VLMs enhance crash detection by integrating visual and textual data to reason about complex traffic scenarios. For example, Video-LLaMA can analyze a sequence of dashcam frames to detect anomalies, using textual prompts to guide the identification of crash events [27]. Similarly, frameworks like VERA use VLMs such as LLaVA-1.5 to verbalize video content, generating explanations for detected anomalies (e.g., "a car collided with another vehicle") [29]. More recent approaches leverage VLMs to extract rich semantic features from video frames, which are then used to identify subtle anomalies that traditional methods might miss. For instance, some methods use VLMs to generate textual descriptions of scenes, which are then analyzed for anomalous keywords or phrases [9]. These approaches outperform classical methods, which struggle with variability in lighting or traffic conditions [6], [10]. By leveraging spatiotemporal reasoning and multimodal integration, VLMs provide a robust framework for real-time crash detection in intelligent transportation systems.

# C. Large Language Models for Video Understanding

Large Language Models (LLMs), such as GPT- 4 [30] and LLaMA- 2 [31], have been extended to multimodal settings through techniques like adapters or prompt tuning, enabling them to process and reason about video data. These advancements allow LLMs to summarize actions, answer temporal questions, and infer event causality, making them particularly valuable for crash detection in video. Unlike classical approaches that rely on low- level motion cues [3], [4] or Vision- Language Models (VLMs) that focus on image- text alignment [23], LLMs excel in contextual reasoning and temporal understanding, enhancing the ability to detect and interpret complex crash events in dynamic traffic scenarios [11].

1) Multimodal Extensions of LLMs: LLMs are adapted for video understanding through multimodal frameworks that integrate visual data with language processing. Adapters, lightweight modules added to pre-trained LLMs,

enable efficient incorporation of video features without retraining the entire model [32]. For example, GPT- 4, with its multimodal capabilities, processes video frames by converting them into textual descriptions or embeddings, allowing it to summarize actions like vehicle collisions or infer causality in crash events [30]. Similarly, LLaMA- 2 is fine- tuned with adapters to handle video inputs, as seen in frameworks like CrashLLM, which converts video data into text for crash prediction [33]. Prompt tuning further enhances LLMs by optimizing input prompts to guide video analysis, such as querying "Is there a crash in this video?" to focus on anomaly detection [29].

2) Applications in Crash Detection: In the context of crash detection, LLMs leverage their temporal reasoning capabilities to analyze video sequences and identify crash events. For instance, CrashLLM fine-tunes LLaMA-2 on the CrashEvent dataset, which includes visual and textual crash data, to predict crash types and severity with over  $92\%$  accuracy [33]. LLMs can summarize actions (e.g., "a vehicle swerved and collided") and answer temporal questions (e.g., "What happened before the crash?") by processing video-derived text or embeddings [11]. Frameworks like VAD-LLaMA use LLMs to detect anomalies, including crashes, by analyzing long-term temporal context through cross-attention mechanisms, achieving significant AUC improvements on datasets like UCF-Crime [34]. Additionally, LLMs infer event causality, such as linking a sudden stop to a rear-end collision, enhancing interpretability in traffic safety applications [35].

3) Advantages and Integration with VLMs: LLMs offer distinct advantages over classical methods and VLMs by providing contextual understanding and explainability. While classical approaches struggle with ambiguous scenarios [6], and VLMs like Video-LLaMA focus on spatiotemporal feature alignment [27], LLMs excel in reasoning about high-level semantics and causality. For example, integrating LLMs with VLMs, as in VERA, combines LLaVA-1.5's visual processing with verbalized reasoning to detect and explain crash events (e.g., "a car collided due to sudden braking") [29]. This synergy enables robust real-time crash detection, addressing challenges like varying lighting or complex traffic interactions [10]. Future research should focus on optimizing LLM adapters for real-time processing and developing comprehensive video crash datasets to further enhance performance.

# D. Taxonomy of LLM-Based Crash Detection

The integration of Large Language Models (LLMs) into crash detection systems has led to diverse approaches, categorized along three primary axes: fusion level, prompt strategy, and LLM role. Additionally, we distinguish

systems based on input granularity (frame- level vs. clip- level encoding) and detection focus (causal detection vs. post- event summarization). This taxonomy provides a structured framework to analyze LLM- based crash detection methods, highlighting their technical nuances and applicability to real- world traffic scenarios [11], [33]. By building on classical approaches [3] and Vision- Language Models (VLMs) [27], LLM- based systems leverage multimodal reasoning to address the variability and ambiguity of crash events.

1) Fusion Level: Fusion level refers to how visual and textual data are integrated within LLM-based systems for crash detection. We identify three main strategies:

- Early Fusion (Token Concatenation): Visual features, extracted from video frames or clips, are tokenized and concatenated with textual inputs before being fed into the LLM. This approach ensures unified processing but can be computationally intensive. For instance, Video-LLaMA employs early fusion to align video and text tokens for crash-related anomaly detection [27].- Late Fusion (Separate Heads): Visual and textual data are processed separately, with distinct heads generating intermediate representations that are combined later. CrashLLM uses late fusion to process video-derived text and metadata, achieving over  $92\%$  accuracy in crash prediction [33].- Cross-Attention: Visual and textual features are integrated through cross-attention mechanisms, allowing dynamic interaction between modalities. VAD-LLaMA utilizes cross-attention to capture long-term temporal context, improving AUC by  $3.86\%$  on UCF-Crime for crash detection [34].

2) Prompt Strategy: Prompt strategies determine how LLMs are guided to process video data for crash detection:

- Static Prompts: Fixed prompts, such as "Is there a crash in this video?", guide the LLM to focus on crash detection. VERA employs static prompts with LLaVA-1.5 to verbalize anomalies, achieving  $86.55\%$  AUC on UCF-Crime [29].- Dynamic Event Templates: Templates adapt to specific crash scenarios (e.g., "Detect a rear-end collision caused by sudden braking"), enhancing contextual relevance. TrafficLens uses dynamic templates to convert multi-camera video data into text for LLM analysis [36].

- Learned Visual Questions: Learned prompts, optimized during training, enable LLMs to generate task-specific questions. This approach, seen in multimodal forecasting frameworks, improves crash detection by tailoring queries to video content [35].

3) LLM Role: The role of the LLM in crash detection varies based on its interaction with video data:

- Passive (Caption Generation): The LLM generates descriptive captions for video frames or clips, which are then analyzed for crash events. LAVAD uses passive LLMs to caption video frames, detecting crashes as anomalies without training [37].- Active (QA/Inference): The LLM answers questions or performs inference based on video inputs, such as identifying crash causes. VAD-LLaMA actively infers crash events using temporal question-answering, improving detection accuracy [34].- Generative (Descriptive Reasoning): The LLM produces detailed explanations of crash events, enhancing interpretability. VERA's generative reasoning provides textual descriptions like "a car collided due to sudden braking" [29].

4) Input Granularity and Detection Focus: LLM-based crash detection systems also differ in input granularity and detection focus:

- Frame-Level Input vs. Clip-Level Encoding: Frame-level input processes individual frames, suitable for fine-grained analysis but computationally costly. Clip-level encoding, used by Video-LLaMA, processes short video segments, capturing temporal dynamics more efficiently [27]. For example, CrashLLM uses clip-level encoding to summarize crash sequences [33].

- Causal Detection vs. Post-Event Summarization: Causal detection identifies crash triggers (e.g., sudden braking), as seen in multimodal forecasting frameworks [35]. Post-event summarization, employed by VERA, describes crash outcomes for post-incident analysis [29].

5) Discussion: The taxonomy highlights the diversity of LLM-based crash detection approaches, each addressing specific challenges in video understanding. Early fusion and cross-attention strategies enhance multimodal integration, while dynamic prompts and generative roles improve contextual reasoning and explainability. Clip-level encoding and causal detection are particularly suited for real-time applications, overcoming limitations of classical methods like frame differencing [6], [10]. However, challenges such as computational complexity and dataset limitations persist [11]. Future research should focus on optimizing fusion strategies and developing comprehensive video crash datasets to advance LLM-based systems.

# III. DATASETS AND BENCHMARKS

The development of Large Language Model (LLM) and Vision- Language Model (VLM)- based crash detection systems relies on high- quality datasets that provide annotated video data, temporal sequences, and contextual information for training and testing. These datasets,

![](https://cdn-mineru.openxlab.org.cn/extract/50a417d4-4a78-4820-bc3c-dabb0987ed33/7e45854b66f1773c618528cad79e1e3ad203c05d91bdb956dca9bb6ba9b298dc.jpg)  
Fig. 3: Diagram illustrating different fusion strategies. (a) Early fusion combines tokenized inputs before the main processing block. (b) Late fusion processes modalities separately and combines the high-level features. (c) Cross-attention fusion integrates visual features into the language model's attention layers.

TABLE I: Comparison of Key Datasets for Video-Based Crash Detection and Analysis  

<table><tr><td>Dataset</td><td>Year</td><td>Size</td><td>Source</td><td>Annotation Type</td><td>Focus</td></tr><tr><td>DAD [38]</td><td>2016</td><td>1,500+ videos</td><td>Dashcam</td><td>Frame-level crash events</td><td>Supervised, multi-class crash detection (e.g., rear-end, side-impact), Temporal model.</td></tr><tr><td>CADP [39]</td><td>2019</td><td>~2,000 videos</td><td>Police reports</td><td>Crash causes, outcomes</td><td>Anomaly detection and causal relationship analysis in crashes.</td></tr><tr><td>Berkeley DeepDrive (BDD100k) [40]</td><td>2020</td><td>10,000+ clips</td><td>Dashcam</td><td>Weak labels (crash/anomaly)</td><td>Anomaly detection and semi-supervised learning for autonomous driving.</td></tr><tr><td>UCF-Crime [41]</td><td>2018</td><td>1,900 videos</td><td>Surveillance</td><td>Anomaly categories (subset)</td><td>Benchmark anomaly detection, including &quot;road accident&quot; category.</td></tr><tr><td>SHRP 2 NDS [42]</td><td>2024 (used)</td><td>8,600+ SCEs</td><td>Naturalistic Driving</td><td>Event/Conflict types</td><td>Differentiating event severity (crash, near-crash) and conflict types.</td></tr><tr><td>WTS [43]</td><td>2024 (used)</td><td>810+ videos</td><td>Mixed (Overhead, Ego)</td><td>Fine-grained captions</td><td>Dense video captioning and multi-phase event description.</td></tr><tr><td>CCD [44]</td><td>2024 (used)</td><td>N/A</td><td>Dashcam</td><td>Crash events</td><td>Crash recognition and anticipation from ego-vehicle perspective.</td></tr></table>

summarized in Table I, enable models to identify crash events in diverse traffic scenarios, addressing limitations of classical approaches [3]. Early datasets like DAD and UCF- Crime provided foundational benchmarks, while recent additions like SHRP 2 NDS [42] offer much larger scale and finer- grained annotations, crucial for training robust multimodal models [11], [42]. Additionally, newer LLM- augmented datasets are emerging, incorporating synthetic captions, temporal questions, or causal chains to enhance multimodal reasoning and support advanced crash detection tasks [27], [33].

# A. Key Datasets

The following datasets are widely used for crash detection, offering diverse video data and annotations:

- Dashcam Accident Dataset (DAD): DAD consists of annotated dashcam videos capturing crash events, with multiple classes including rear-end collisions, side impacts, and pedestrian accidents. It contains over 1,500 videos with frame-level annotations for crash types and severity, making it suitable for supervised learning and fine-grained crash analysis [38]. DAD is commonly used to train VLMs like Video-LLaMA for detecting crash-related anomalies [27].

![](https://cdn-mineru.openxlab.org.cn/extract/50a417d4-4a78-4820-bc3c-dabb0987ed33/b46f2810b033f790c58a92f7719563b0ff4d7fefb40ce1838d3eb5b18097d975.jpg)  
Fig. 4: Radar chart comparing key attributes of crash detection datasets. 'Size' is shown on a normalized log scale, 'Annotation Granularity' is on a scale from 1 (weak) to 4 (fine-grained), and 'Source Diversity' is on a scale of 1 (single) to 2 (mixed).

- Car Accident Detection from Police Reports (CADP): CADP integrates video data with police-reported accident details, supporting temporal modeling of crash sequences. With approximately 2,000 videos and metadata on crash causes (e.g., speeding, lane changes) and outcomes, CADP is ideal for studying causal relationships in crashes [39]. It is leveraged by frameworks like CrashLLM for multimodal crash prediction [33].

- Berkeley DeepDrive Accident: This dataset provides weakly labeled video data from the Berkeley DeepDrive project, focusing on anomaly and crash detection in autonomous driving scenarios. It includes over 10,000 video clips with coarse labels for crash events, facilitating unsupervised or semi-supervised learning approaches [40]. It supports models like VAD-LLaMA for anomaly detection tasks [34].

- UCF-Crime (subset): UCF-Crime includes a subset of approximately 300 videos labeled for traffic crashes under anomaly categories, such as "road accident." With a total of 1,900 videos, this subset is used to benchmark anomaly detection models like VERA, which combine visual and textual reasoning for crash detection [29], [41].

- SHRP 2 NDS: The Second Strategic Highway Research Program Naturalistic Driving Study is one of the largest available datasets, containing over 8,600 safety-critical events (SCEs). It provides detailed annotations for event types (crash, near- crash) and 16 distinct conflict types, making it ideal for training models like ScVLM that aim to differentiate between nuanced scenarios [42].

- WTS and CCD: The WTS dataset, used in the AI City Challenge, focuses on fine-grained, multi-phase captioning from both overhead and ego-centric views [43]. Similarly, the Car Crash Dataset (CCD) is another benchmark for ego-centric crash recognition and anticipation [44].

# B. LLM-Augmented Datasets

Recent advancements have introduced LLM- augmented datasets that enhance crash detection by incorporating rich textual annotations generated by LLMs. These datasets address the limitations of traditional datasets, such as coarse labels or limited contextual information, by providing synthetic captions, temporal questions, or causal chains:

- Synthetic Captions: Datasets like CrashEvent use LLMs to generate descriptive captions for video clips (e.g., "A vehicle swerves and collides with a truck"), enriching visual data with textual context. CrashEvent, with 19,340 crash reports, supports frameworks like CrashLLM for predicting crash outcomes [33].

- Temporal Questions: Emerging datasets include temporal question-answering tasks, such as "What caused the crash in this video?" or "What happened

before the collision?" These questions enhance temporal reasoning, as seen in models like VADLLaMA, which analyze crash sequences [34].

Causal Chains: LLM- augmented datasets link crash events to their triggers (e.g., sudden braking leading to a rear- end collision). Such datasets are used in multimodal forecasting frameworks to predict crash likelihood by integrating video and textual data [35].

The reviewed datasets provide a foundation for advancing LLM and VLM- based crash detection, offering diverse data from annotated dashcam videos (DAD) to largescale naturalistic driving studies (SHRP 2 NDS).LLMaugmented datasets, with synthetic captions and causal chains, further enhance multimodal reasoning, addressing challenges like coarse annotations in UCF- Crime [41]. However, issues such as dataset imbalance, limited real- world diversity, and computational demands for processing large video datasets persist [11]. Future efforts should focus on developing comprehensive datasets with fine- grained annotations and expanding LLM- augmented datasets to support real- time crash detection.

![](https://cdn-mineru.openxlab.org.cn/extract/50a417d4-4a78-4820-bc3c-dabb0987ed33/66f3889f42906aee89a79b17153d01c2e0fe1d213214e75804dbe03bbd57eeb3.jpg)  
Fig. 5: A generic architecture for an LLM-based crash detection system.

# IV. MODEL ARCHITECTURES

The integration of Large Language Models (LLMs) and Vision- Language Models (VLMs) into crash detection systems has led to the development of sophisticated model architectures tailored for processing video data and reasoning about crash events. A generic representation of such an architecture is shown in Figure 5. These architectures combine visual processing, temporal modeling, and language understanding to address the complexities of crash detection in dynamic traffic scenarios. Three typical architectures dominate the field: Visual Encoder  $^+$  LLM Decoder, Frozen  $\mathrm{LLM + }$  Learned Adapter, and Joint Vision- Language Pretraining. Each leverages common components, including vision encoders, temporal modules, and LLMs, to achieve robust performance [11], [27]. This section reviews these architectures and their components, highlighting their applications in crash detection and their evolution from classical approaches [3].

# A. Visual Encoder  $^+$  LLM Decoder

In this architecture, video frames are encoded using a vision encoder, such as Vision Transformer (ViT) or Inflated 3D ConvNet (I3D), and the resulting features are fed as tokens to an LLM decoder for multimodal reasoning. Inspired by models like Flamingo, this approach processes video frames as a sequence of visual tokens, enabling the LLM to generate crash- related predictions or descriptions [26]. For instance, VideoLLaMA employs a ViT- based encoder to extract frame features, which are tokenized and processed by an LLM to detect crash anomalies on datasets like UCF- Crime [27], [41]. This architecture excels in tasks requiring temporal reasoning, such as identifying crash sequences, but demands significant computational resources due to end- to- end processing of visual and textual data [11].

# B. Frozen LLM  $^+$  Learned Adapter

This architecture keeps the LLM backbone, such as GPT- 4 or LLaMA- 2, frozen and trains lightweight adapters to bridge vision features and the language model. Adapters, as used in BLIP- 2 and MiniGPT- 4, enable efficient fine- tuning for crash detection without modifying the pre- trained LLM [25], [45]. For example, CrashLLM employs a LoRA adapter to integrate video- derived features from a Swin Transformer with a frozen LLaMA2 backbone, achieving high accuracy on the CrashEvent dataset [31]- [33]. This approach is computationally efficient, making it suitable for real- time applications, and supports tasks like crash outcome prediction and causal inference [35].

# C. Joint Vision-Language Pretraining

Joint vision- language pretraining involves training both vision and language components together on crashlike tasks, enabling end- to- end optimization for crash detection. Due to data scarcity, few public examples exist, but frameworks like VERA demonstrate this approach

by pretraining on crash- specific datasets like DAD and CADP [29], [38], [39]. These models combine vision encoders (e.g., SlowFast) and LLMs (e.g., GPT- 3.5) to learn crash- specific patterns, such as sudden vehicle movements or collisions [30], [46]. Joint pretraining enhances robustness in diverse traffic scenarios but requires large, annotated datasets, limiting its adoption [11]. Efforts to address data scarcity include synthetic data generation [33]. A recent trend involves hybrid approaches, such as ScVLM [42], which combines supervised learning for event classification with contrastive learning for nuanced event and conflict type identification, leveraging the strengths of both paradigms.

# D. Self-Supervised and Few-Shot Learning Approaches

To address the data scarcity challenge outlined in Section VII, Self- Supervised Learning (SSL) and Few- Shot Learning (FSL) have emerged as promising paradigms. These methods reduce the reliance on large, manually annotated datasets by learning from unlabeled data or generalizing from very few examples.

1) Self-Supervised Learning for Representation Learning: SSL methods learn feature representations from unlabeled data by solving pretext tasks. For video-based crash detection, techniques like contrastive learning can be used to pre-train a model on large, unlabeled video datasets, such as BDD100K [40]. In this paradigm, a model learns to pull representations of similar video clips (e.g., different augmentations of the same clip) closer together in the embedding space while pushing dissimilar clips apart [47]. Another approach is masked video modeling, where parts of a video are masked, and the model is trained to predict the missing content, forcing it to learn meaningful spatio-temporal patterns. A framework extending CLIP4Clip [28] could be pretrained on vast amounts of unlabeled traffic videos to learn robust representations of normal driving behavior, which can then be fine-tuned for the downstream crash detection task.

2) Few-Shot Learning for Rare Event Detection: FSL aims to train models that can generalize to new classes from a small number of labeled examples. This is particularly useful for detecting rare crash types (e.g., collisions involving unusual vehicles) for which little to no training data exists. Prototypical networks, a popular FSL method, learn a metric space where classification can be performed by computing distances to prototype representations of each class [48]. An FSL approach could be integrated with a VLM like Video-LLaMA, where the model is fine-tuned on a few examples of a rare crash type, enabling it to detect similar incidents in the future without requiring extensive data collection.

3) Proposed Framework and Evaluation: A powerful approach would be to combine SSL and FSL. A backbone model like LLaMA-2 [31] could be pre-trained using SSL on a large corpus of unlabeled dashcam videos. This pre-trained model would develop a rich understanding of general traffic scenes. Subsequently, it could be fine-tuned for specific crash detection tasks using an FSL approach with just a few labeled examples from a dataset like DAD [38]. Such a model, which we refer to as SSL-VAD in Table II, would be highly data-efficient and adaptable to new, rare crash scenarios. Its performance could be evaluated on standard benchmarks like UCF-Crime, with expected performance rivaling supervised methods due to the powerful representations learned during SSL pretraining.

# E. Common Components

The architectures rely on several key components to process video data and enable crash detection. These components are designed to extract visual features, model temporal dynamics, and perform language- based reasoning, with specific variations tailored to crash detection tasks:

- Vision Encoder: Vision encoders extract spatial and contextual features from video frames or clips, forming the foundation for crash detection. Common choices include:

- Vision Transformer (ViT): ViT processes frames as patches, capturing high-resolution details critical for identifying crash indicators like vehicle damage or road obstacles. It is used in Video-LLaMA to encode frame-level features for anomaly detection [27], [49]. Other popular backbones include CNNs like ResNet and VGG, which have been widely used in earlier deep learning models for their strong performance in image classification and feature extraction [8].

- Swin Transformer: Swin employs a hierarchical structure with shifted windows, enabling efficient processing of high-resolution videos. CrashLLM uses Swin to extract features for clip-level crash prediction, balancing accuracy and computational cost [33], [50].

- SlowFast: SlowFast captures both slow (semantic) and fast (motion) dynamics in videos, ideal for detecting rapid crash events like collisions. VERA leverages SlowFast to process crash sequences in DAD, improving detection of sudden movements [29], [46].

These encoders are often pre- trained on large datasets like ImageNet or Kinetics, then fine- tuned on crash- specific datasets (e.g., Berkeley DeepDrive Accident) to enhance performance [40]. Their ability

to handle diverse visual inputs makes them essential for robust crash detection in varied traffic conditions [11].

Temporal Module: Temporal modules model the sequential nature of video data, capturing dependencies across frames or clips to identify crash sequences. Key variants include:

Transformer: Standard Transformers process video features as sequences, enabling long- range temporal modeling. Video- LLaMA uses a Transformer- based temporal module to analyze crash sequences in UCF- Crime, capturing extended crash contexts [27], [41]. Convolutional LSTM (ConvLSTM): ConvLSTM combines convolutional and recurrent layers to model spatial- temporal relationships, suitable for frame- level crash analysis. It is used in frameworks like LAVAD for anomaly detection in CADP [37], [39], [51]. 3D CNNs are also commonly used to capture spatiotemporal features directly from video clips [9]. TimeSformer: TimeSformer extends ViT with temporal attention, efficiently modeling long video sequences. VAD- LLaMA employs TimeSformer to process clip- level inputs, improving crash detection accuracy on Berkeley Deep- Drive Accident by capturing motion patterns [34], [40], [52].

These modules vary in complexity, with Transformers and TimeSformer excelling in clip- level analysis and ConvLSTM suited for fine- grained frame- level tasks. Their integration ensures accurate modeling of crash events, such as sudden braking or collisions, in dynamic scenarios [11].

LLM: LLMs provide advanced language reasoning for crash detection, enabling tasks like generating crash descriptions, answering temporal questions, or inferring causal relationships. Common models include:

GPT- 3.4/4: These models offer powerful generative capabilities, producing detailed crash explanations (e.g., "A car collided due to sudden lane change"). VERA uses GPT- 3.5 for descriptive reasoning on DAD, enhancing interpretability [29], [30]. LLaMA- 2: LLaMA- 2 is efficient for fine- tuning, widely used in adapter- based models like CrashLLM for crash outcome prediction on CrashEvent [31], [33]. Smaller Models (e.g., MiniGPT- 4): MiniGPT- 4 is designed for resource- constrained environments, supporting real- time crash detection with reduced computational overhead. It is used in lightweight frameworks for anomaly detection [45].

LLMs are typically pre- trained on large text corpora and fine- tuned with crash- specific datasets (e.g., CADP) or augmented with adapters (e.g., LoRA) to handle multimodal inputs. Their role in crash detection extends beyond prediction to providing human- readable insights, crucial for applications like post- event analysis and autonomous driving systems [32], [35].

The three architectures—Visual Encoder + LLM Decoder, Frozen LLM + Learned Adapter, and Joint Vision- Language Pretraining—offer distinct trade- offs for crash detection. The Visual Encoder + LLM Decoder excels in temporal reasoning but is computationally intensive, while Frozen LLM + Learned Adapter provides efficiency for real- time systems. Joint Vision- Language Pretraining promises robustness but is limited by data scarcity [11]. The enhanced common components—ViT, Swin, SlowFast for vision; Transformer, ConvLSTM, TimeSformer for temporal modeling; and GPT- 3.5/4, LLaMA- 2, MiniGPT- 4 for language—enable these architectures to surpass classical methods like frame differencing in handling complex crash scenarios [6], [10]. Future research should focus on optimizing component efficiency, developing hybrid architectures, and addressing data scarcity through synthetic datasets to advance real- time crash detection.

# V. EVALUATION METRICS

Evaluating Large Language Model (LLM) and Vision- Language Model (VLM)- based crash detection systems requires a diverse set of metrics to assess their performance in identifying, localizing, describing, and reasoning about crash events in video data. These metrics ensure that models not only detect crashes accurately but also provide temporal precision, interpretable descriptions, and causal insights, addressing limitations of classical crash detection methods [3], [6]. Common evaluation metrics include Crash Detection Accuracy, Temporal Localization, Captioning Quality, and Causal QA Accuracy, each tailored to specific aspects of crash detection tasks [8], [11], [33].

# A. Crash Detection Accuracy

Crash Detection Accuracy measures a model's ability to correctly classify video segments as containing a crash event, either as a binary (crash vs. non- crash) or multiclass (e.g., rear- end collision, side impact, pedestrian accident) classification task. It is typically reported as the percentage of correct predictions over a test set. For instance, VERA achieves 86.55% accuracy on the UCF- Crime dataset for binary crash detection, while CrashLLM

reports over  $92\%$  accuracy on multi- class classification using the CrashEvent dataset [29], [33], [41]. This metric is critical for evaluating model robustness across diverse crash scenarios, particularly when trained on datasets like DAD or CADP [38], [39]. However, accuracy alone may be insufficient for imbalanced datasets, where metrics like F1- score, which considers both precision and recall, are often used to account for false positives and negatives [11]. The Area Under the ROC Curve (AUC) is another widely used metric that evaluates the model's ability to distinguish between classes across all classification thresholds [9].

# B. Temporal Localization

Temporal Localization evaluates a model's ability to precisely identify the time interval of a crash event within a video. It is commonly measured using Intersection- over- Union (IoU), which computes the overlap between the predicted crash interval and the ground truth, divided by their union. Higher IoU values indicate better temporal precision. For example, Video- LLaMA achieves an IoU of 0.78 on the Berkeley DeepDrive Accident dataset, outperforming classical methods like frame differencing, which struggle with precise localization [6], [27], [40]. This is also a key metric for anticipation models like CRASH [44], which are evaluated on mean Time- to- Accident (mTTA). Temporal Localization is crucial for real- time applications, such as autonomous driving, where identifying the exact moment of a crash enables timely responses [35]. Variants of IoU, such as mean IoU across multiple crash events, are also used to assess performance on datasets with varying crash durations [33]. For tasks that require localizing anomalies within a frame, metrics like Average Precision (AP), borrowed from object detection, are also used to evaluate the spatial accuracy of the detection [9].

# C. Captioning Quality

Captioning Quality assesses the quality of LLM- generated textual descriptions of crash events, such as "A vehicle swerves and collides with a truck." Standard natural language processing metrics are employed, including:

- BLEU (Bilingual Evaluation Understudy): Measures n-gram overlap between generated and reference captions, with higher scores indicating better lexical similarity [53].- METEOR (Metric for Evaluation of Translation with Explicit ORdering): Evaluates semantic similarity by considering synonyms and stemming, providing a more nuanced assessment [54].- CIDEr (Consensus-based Image Description Evaluation): Quantifies caption consensus with reference descriptions, emphasizing crash- specific terminology [55].

For instance, VERA's captions on the DAD dataset achieve a BLEU- 4 score of 0.65 and a CIDEr score of 1.2, reflecting high- quality descriptions compared to baseline models [29], [38]. These metrics are central to evaluating models like TrafficVLM, which focuses on dense, multiphase captioning for traffic events [43]. Captioning Quality is essential for interpretable crash detection, enabling post- event analysis and human- readable insights, particularly in LLM- augmented frameworks [34].

# D. Causal QA Accuracy

Causal QA Accuracy measures a model's ability to correctly answer temporal or causal questions about crash events, such as "What caused the crash in this video?" or "What happened before the collision?" It is reported as the percentage of correct answers on a question- answering test set. For example, CrashLLM achieves  $85\%$  Causal QA Accuracy on the CrashEvent dataset, leveraging its ability to reason about crash triggers like sudden braking or lane changes [33]. Similarly, VAD- LLaMA demonstrates strong performance on temporal questions using the CADP dataset, with  $82\%$  accuracy [34], [39]. This metric is vital for evaluating a model's reasoning capabilities, especially in LLM- augmented datasets with temporal questions or causal chains, and supports applications like crash forecasting and autonomous vehicle decision- making [11], [35].

The evaluation metrics—Crash Detection Accuracy, Temporal Localization, Captioning Quality, and Causal QA Accuracy—provide a comprehensive framework for assessing LLM and VLM- based crash detection systems. Crash Detection Accuracy ensures reliable classification, while Temporal Localization enables precise event timing, critical for real- time systems [35]. Captioning Quality enhances interpretability, and Causal QA Accuracy supports advanced reasoning, addressing limitations of classical methods that rely solely on visual features [6], [10]. However, challenges remain, including handling imbalanced datasets, ensuring generalizability across diverse crash scenarios, and standardizing metrics across datasets like UCF- Crime and Berkeley DeepDrive Accident [40], [41]. Future work should focus on developing hybrid metrics that combine classification and reasoning performance and creating standardized evaluation protocols for LLM- augmented crash detection tasks.

# VI. COMPARISON OF RECENT METHODS

The integration of Large Language Models (LLMs) and Vision- Language Models (VLMs) into crash detection from video data has revolutionized intelligent transportation systems, enabling robust analysis of complex

TABLE II: Comparison of Recent LLM-based Crash Detection and Analysis Approaches  

<table><tr><td>Method</td><td>Year</td><td>Fusion Strategy</td><td>Core LLM/VLM</td><td>Primary Dataset(s)</td><td>Key Performance Metric</td><td>Main Contribution</td></tr><tr><td>CrashLLM [33]</td><td>2024</td><td>Multimodal (Late)</td><td>Custom LLM (LLaMA-2 based)</td><td>CrashEvent</td><td>&amp;gt; 92% Accuracy</td><td>Generates detailed crash descriptions and predicts outcomes. Economically detection via passive caption-in without fine-tuning.</td></tr><tr><td>LAADV [37]</td><td>2024</td><td>Training-free</td><td>Custom LLM</td><td>UCF-Crime, XD-Violence</td><td>85.0% AUC</td><td>Across unbalanced and explainable crash detection with rationales. Detects and explains crash events using verbalized learning.</td></tr><tr><td>Holmes-VAD [56]</td><td>2024</td><td>Multimodal</td><td>Multi-modal LLM</td><td>UCF-Crime</td><td>86.5% AUC</td><td>Focus on unbiased and explainable crash detection with precision for nonlocalization.</td></tr><tr><td>VERA [29]</td><td>2024</td><td>Multimodal</td><td>LLaVA-1.5</td><td>DAD, UCF-Crime</td><td>86.55% AUC</td><td>Delets and explains crash events using verbalized learning.</td></tr><tr><td>Video-LLaMA [27]</td><td>2023</td><td>Early Fusion</td><td>LLaMA-based</td><td>BDD, UCF-Crime</td><td>0.78 IoU</td><td>Real-time detection with precise temporal localization.</td></tr><tr><td>ScVLM [42]</td><td>2024</td><td>Hybrid (Supervised + Contrastive)</td><td>VideoLLaMA2</td><td>SHRP 2 NDS</td><td>High Accuracy/mAP</td><td>Combines learning paradigms for non-averaged event and conflict type classification.</td></tr><tr><td>TrafficVLM [43]</td><td>2024</td><td>Multi-level (Spatial/Temporal)</td><td>T5-Base</td><td>WTS</td><td>3rd in AI City Challenge</td><td rowspan="3">Controllable dense captioning for multi-phase traffic events. Creates on crash detection with context-aware attention mechanisms. Data-efficient crash detection via self-supervised pre-training and few-shot time-tuning.</td></tr><tr><td>CrashL [44]</td><td>2024</td><td>Multi-layer Fusion</td><td>TVA (attention-based)</td><td>DAD, UCD, A3D</td><td>High AI 40 nVA</td></tr><tr><td>SSL-VAD (Proposed)</td><td>2025+</td><td>SSL + FSL</td><td>LLaMA-2</td><td>BDD100K, DAD</td><td>Est. &amp;amp; 85% AUC</td></tr></table>

traffic scenarios. This section compares recent LLM- based methods for crash detection (2023- 2024), building on the taxonomy introduced in Section II- D, datasets from Section III, and evaluation metrics from Section V. Table II summarizes key attributes of these methods, including their fusion strategies, LLMs, datasets, performance metrics, and application contexts. The comparison highlights the evolution from classical approaches [3], [6] to advanced multimodal frameworks, addressing challenges like occlusion, variable lighting, and temporal dependencies [11].

# A. Analysis of Methods

The methods in Table II represent a diverse range of approaches leveraging LLMs and VLMs for crash detection, categorized by their fusion strategies, LLM roles, and application contexts as outlined in Section II- D. Below, we analyze each method, focusing on their technical contributions and performance.

CrashLLM [33] is a multimodal framework that achieves over  $92\%$  accuracy on the CrashEvent dataset, processing video- derived text and visual features to predict crash types and severity. Its generative LLM role, as per Section II- D, produces detailed crash descriptions (e.g., "A vehicle collided due to sudden braking"), making it ideal for traffic crash analysis. The use of a custom LLM and clip- level encoding enhances its robustness in diverse scenarios [33].

LAAD [37] is a training- free approach that uses a custom LLM to detect video anomalies, including crashes, on datasets like UCF- Crime and XD- Violence. Achieving  $85.0\%$  AUC, it employs a passive LLM role (caption generation) and is well- suited for real- time video anomaly detection due to its computational efficiency [41]. Its training- free nature makes it adaptable to new crash scenarios without extensive fine- tuning.

Holmes- VAD [56] leverages a multi- modal LLM for unbiased and explainable crash detection, achieving  $86.5\%$  AUC on UCF- Crime and custom datasets. Its multimodal fusion and generative role provide human- readable explanations, addressing fairness in crash detection applications. The focus on explainability aligns with the need for interpretable systems in traffic safety [11].

VERA [29] uses LLaVA- 1.5 with multimodal fusion to detect and explain crashes on DAD and UCF- Crime, achieving  $86.55\%$  AUC and a BLEU- 4 score of 0.65 for captioning quality. Its generative role produces detailed crash rationales, enhancing post- event analysis for traffic monitoring and autonomous vehicles [38], [41].

Video- LLaMA [27] employs early fusion with a LLaMA- based model, achieving an IoU of 0.78 on Berkeley DeepDrive and UCF- Crime for real- time crash detection. Its visual encoder (ViT) and temporal module (Transformer) enable precise temporal localization, making it suitable for dynamic traffic scenarios [40], [41].

ScVLM [42] introduces a hybrid approach that leverages supervised learning for event type classification and contrastive learning for nuanced conflict type identification on the large- scale SHRP 2 NDS dataset. This allows the model to move beyond simple crash/no- crash labels to understand the nature of safety- critical events, which is crucial for root cause analysis.

TrafficVLM [43] focuses on the task of dense video captioning for traffic events. Using a TS- Base model, it generates detailed, multi- phase descriptions of incidents from multiple camera views. Its success in the AI City Challenge 2024 highlights the growing importance of generating rich, human- readable narratives for event analysis, not just classifying them.

CRASH [44] shifts the focus from detection to anticipation. It uses context- aware and temporal attention mechanisms to predict accidents before they occur, evaluated on metrics like Average Precision (AP) and mean Time- to- Accident (mTTA). This proactive approach is vital for the development of active safety systems in autonomous vehicles.

SSL- VAD (Proposed): This conceptual method highlights the potential of data- efficient learning paradigms.

By pre- training a model on large unlabeled video datasets (e.g., BDD100K) using self- supervised techniques, it can learn robust representations of normal driving. Subsequently, it can be fine- tuned with only a few labeled examples of rare crash events using few- shot learning, offering a scalable solution to the data scarcity problem.

# B. Emerging Learning Paradigms: SSL and FSL

While most methods in Table II rely on some form of supervised learning, emerging paradigms like Self- Supervised Learning (SSL) and Few- Shot Learning (FSL) offer a compelling solution to the data scarcity challenge highlighted in Section VII.

1) Self-Supervised Learning (SSL): SSL frameworks learn representations from unlabeled data by solving pretext tasks. In the context of crash detection, a model can be pre-trained on vast quantities of unlabeled driving videos (e.g., from BDD100K) to learn a fundamental understanding of "normal" traffic flow and vehicle dynamics [40]. Techniques like contrastive learning, which learns to differentiate between similar and dissimilar video clips, are particularly effective for this [47], [57]. A model pre-trained in this manner, such as an extension of CLIP4Clip [28], would be highly effective for downstream fine- tuning on a small labeled crash dataset, as it has already learned a rich feature space.

2) Few-Shot Learning (FSL): FSL addresses the problem of recognizing new categories from very few examples. This is directly applicable to detecting rare crash types (e.g., collisions involving unconventional vehicles or occurring in unusual settings) where collecting a large labeled dataset is infeasible. Prototypical networks [48], for example, learn a metric space where a new crash type can be classified based on its distance to a "prototype" representation learned from just a handful of examples. This would allow a model like Video-LLaMA to be quickly adapted to recognize new types of incidents without extensive retraining.

3) A Proposed Hybrid Framework: A powerful future direction is to combine these approaches. A foundation model, such as a LLaMA-2 video model [31], could first be pre-trained via SSL on millions of hours of unlabeled public dashcam footage. This would create a robust general model of driving scenes. Then, this model could be specialized for crash detection via FSL, where it is fine-tuned on a very small number of labeled crash examples from datasets like DAD [38] or CADP [39]. This SSL-VAD approach, included conceptually in Table II, would be highly data-efficient and could achieve performance (e.g.,  $>85\%$  AUC on UCF-Crime) competitive with fully supervised methods while being far more scalable.

# C. Discussion

The methods in Table II illustrate the shift from classical crash detection techniques [3], [6] to LLM- based systems, leveraging multimodal fusion and advanced reasoning as described in Section II- B and Section II- C. Early fusion (e.g., Video- LLaMA) and multimodal approaches (e.g., CrashLLM, VERA) excel in integrating visual and textual data, addressing occlusion and variability challenges. Training- free methods like LAVAD offer flexibility for real- time applications [37]. Performance metrics, such as CrashLLM's  $92\%$  accuracy and VERA's  $86.55\%$  AUC, highlight the superiority of LLM- based systems over traditional CNN- based methods, which struggle with complex scenarios [10].

This comparison reveals a clear trade- off in the current landscape. On one hand, methods like CrashLLM demonstrate state- of- the- art performance ( $\zeta 92\%$  accuracy) by leveraging custom- built multimodal architectures and large, domain- specific datasets like CrashEvent. However, this performance comes at the cost of significant computational resources and potential challenges in generalization. On the other hand, training- free approaches like LAVAD offer remarkable flexibility and efficiency, making them suitable for real- time deployment, but their performance ( $85.0\%$  AUC) may not match specialized models in complex scenarios. Methods like VERA and Holmes- VAD strike a balance, focusing on explainability (a key challenge identified in Section VII) while achieving strong performance ( $86.55\%$  AUC). This highlights a key decision point for ITS applications: is the primary goal maximum accuracy, real- time deployment, or human- interpretable output?

The inclusion of recent works like ScVLM [42], TrafficVLM [43], and CRASH [44] demonstrates the field's rapid maturation. The focus is expanding from binary detection to a more nuanced understanding, including fine- grained conflict classification (ScVLM), dense narrative generation (TrafficVLM), and proactive anticipation (CRASH). This evolution is enabled by the availability of larger and more detailed datasets like SHRP 2 NDS [42] and the competitive environment of challenges like the AI City Challenge.

Datasets play a critical role, with CrashEvent, DAD, and UCF- Crime enabling robust training and evaluation (Section III). Evaluation metrics (Section V) like AUC, IoU, and BLEU provide a comprehensive assessment, but challenges remain in handling dataset imbalances and ensuring generalizability across diverse traffic conditions [11]. Future research should focus on developing hybrid fusion strategies, optimizing computational efficiency for real- time systems, and creating comprehensive, LLM- augmented datasets to further advance crash detection.

# VII. CHALLENGES

VII. CHALLENGESThe adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs) for crash detection from video data has shown significant promise, as discussed in Sections VI and II-D. However, several challenges hinder their widespread deployment in intelligent transportation systems. This section outlines four key obstacles: data scarcity, multimodal alignment, reasoning and explainability, and real-time constraints. These challenges impact the robustness, interpretability, and practicality of LLM-based crash detection systems, necessitating further research to address them [11], [33].

# A.Data Scarcity

A. Data ScarcityLLM and VLM models require large, diverse datasets of labeled video-caption pairs to achieve robust performance in crash detection. However, such datasets are scarce, particularly for crash-specific scenarios, due to the rarity of crash events and the complexity of annotating video data [38]. While recent large-scale datasets like SHRP 2 NDS [42] have significantly improved data availability, they still represent a fraction of the data needed to cover all possible real-world scenarios. Existing datasets, such as the Dashcam Accident Dataset (DAD) and UCF-Crime, contain limited crash-related videos, often with coarse or incomplete annotations [38], [41]. For instance, DAD includes only 1,500 annotated videos, insufficient for training large-scale multimodal models like Video-LLaMA [27]. Synthetic datasets, such as those used in CrashLLM, attempt to address this by generating video-caption pairs, but they may lack the variability of real-world traffic conditions [33]. Data scarcity also exacerbates dataset imbalance, where non-crash videos dominate, leading to biased model predictions [11]. Efforts to create comprehensive, crash-specific datasets are critical to improving model generalization and performance [39].

# B.Multimodal Alignment

Achieving temporal synchronization between video frames and language descriptions is a significant challenge, particularly in crash detection scenarios involving occlusions, low framerates, or complex traffic dynamics [29]. Multimodal alignment requires models to correlate visual events (e.g., a collision) with corresponding textual descriptions (e.g., "A car collided with a truck"), ensuring temporal and semantic consistency [27]. For example, frameworks like VERA struggle to align frame- level visual features with textual captions under occlusions, where key crash indicators may be obscured [29]. Low framerates, common in traffic surveillance videos, further complicate alignment by reducing temporal resolution, as noted in studies of classical optical flow methods [4].

Cross- attention mechanisms, as used in VAD- LLaMA, mitigate this by modeling long- range dependencies, but they increase computational complexity [34]. Developing efficient alignment techniques that handle occlusions and variable framerates remains an open research problem [11].

# C. Reasoning and Explainability

C. Reasoning and ExplainabilityWhile LLMs generate fluent crash reports, their outputs may be factually incorrect or inconsistent with visual inputs, a phenomenon known as hallucination [30]. This poses a critical challenge for crash detection, where accurate and interpretable outputs are essential for applications like autonomous driving and post-event analysis [33]. For instance, CrashLLM's generative descriptions may inaccurately attribute crash causes (e.g., "sudden braking" instead of "lane change") if visual cues are ambiguous [33]. Ensuring consistency between LLM-generated reports and visual evidence requires robust grounding techniques, such as those explored in VERA, which uses LLaVA-1.5 to verbalize crash rationales [29]. However, even advanced models struggle with causal reasoning under complex traffic scenarios, as noted in studies of multimodal forecasting [35]. Improving explainability through techniques like scene-graph grounding or causal intervention modules is a key open problem to enhance trust in LLM-based systems [11], [26].

# D. Real-time Constraints

Real- time crash detection is critical for applications like accident response and autonomous vehicle decisionmaking, but LLM- based pipelines often exceed the computational limits required for real- time inference [37]. Large models, such as GPT- 4 or Video- LLaMA, demand significant resources, with inference times ranging from hundreds of milliseconds to seconds per video clip, far exceeding the  $30\mathrm{- }100\mathrm{ms}$  latency required for realtime systems [27], [30]. For example, while the specific hardware is not detailed, the VADSR paper notes that the foundational models used within Holmes- VAD have inference speeds ranging from approximately 2 to 6 seconds per frame, clearly outside the real- time window for immediate response [58]. This highlights a significant performance gap that needs to be addressed for practical deployment.

Optimizing model architectures (e.g., using lightweight adapters like LoRA [32]), developing efficient temporal modules, and leveraging quantization techniques are essential to meet real- time constraints while maintaining performance [11]. For example, Holmes- VAD's multimodal LLM achieves high accuracy but incurs high computational costs, limiting its use in resource- constrained environments [56]. Training- free approaches like LAVAD

reduce latency by eliminating fine- tuning, achieving  $85.0\%$  AUC with lower overhead, but they may sacrifice precision in complex scenarios [37]. Classical methods, such as frame differencing, offer lower latency but lack the robustness of LLMs [3]. A quantitative comparison of inference times and hardware requirements across models remains a challenge due to a lack of standardized reporting in the literature, but the available data suggests a clear trade- off between model complexity, accuracy, and processing speed.

# E. Ethical and Fairness Considerations

Beyond technical hurdles, the deployment of LLM- based crash detection systems raises significant ethical and fairness concerns. Datasets such as DAD and UCF- Crime, while foundational, may contain inherent biases by over- representing specific geographic regions, weather conditions, or vehicle types. Models trained on such data may perform poorly in underrepresented scenarios, leading to inequitable safety outcomes. Furthermore, the use of dashcam and surveillance footage introduces profound privacy risks, as this data can capture identifiable information like faces and license plates. The societal impact of large- scale video data collection and analysis requires careful consideration of data anonymization techniques and transparent policies regarding data usage and storage.

Algorithmic fairness is another critical issue. Biased data can lead to biased models, where false positive or negative rates are disproportionately higher for certain demographic groups or environments. While methods like Holmes- VAD aim to address unbiased detection, the risk remains that automated systems could perpetuate or even amplify existing societal biases, with serious consequences for insurance liability and legal accountability. Therefore, developing fair, transparent, and privacy- preserving systems is not just a technical challenge but an ethical imperative for the responsible advancement of this technology.

# F. Robustness and Generalization Analysis

A critical challenge for LLM- based crash detection models is their ability to generalize to scenarios not well represented in the training data. While models may perform well on benchmarks, their robustness in unseen or out- of- distribution (DOD) scenarios—such as rural roads, adverse weather (e.g., snow, heavy rain), or different geographic regions—remains a significant concern. This limitation poses a risk of overfitting to the visual and contextual cues prevalent in the training data, leading to a sharp degradation in performance when deployed in the real world [59].

1) Domain Generalization: To address this, domain generalization techniques are essential. These methods aim to learn domain-invariant features that are robust to shifts in data distribution. Techniques such as domain-invariant feature learning, meta-learning, and adversarial domain adaptation can help models generalize from multiple source domains to unseen target domains [60]. Data augmentation, particularly using high-fidelity simulators like CARLA, allows for the creation of diverse training data by simulating varied weather and lighting conditions, which can significantly improve model robustness [61].

2) Out-of-Distribution (OOD) Detection: Complementary to domain generalization is Out-of-Distribution (OOD) detection, which focuses on identifying whether a given input is from a novel, unseen distribution. In crash detection, this is crucial for flagging rare events (e.g., unusual multi-vehicle pile-ups, crashes involving non-standard vehicles) that the model was not trained to recognize. By equipping models like LAVAD or VERA with an OOD detection mechanism, the system could defer to a human operator or a fail-safe mode when it encounters a scenario it cannot confidently analyze, thereby increasing safety. Methods based on uncertainty estimation are particularly promising for this task, as they can quantify the model's confidence in its predictions [62].

3) Cross-Dataset Evaluation and Quantitative Analysis: A rigorous evaluation methodology is necessary to properly assess generalization. This should involve comprehensive cross-dataset evaluation, where models trained on one dataset (e.g., DAD) are tested on another with different characteristics (e.g., UCF-Crime or the more diverse SHRP 2 NDS) [40], [42]. A quantitative analysis of this performance drop, for example, by reporting the decrease in AUC or F1-score, provides a clear measure of a model's generalization capability. Future work should standardize this practice and ideally present results in a comparative table, analyzing performance drops with statistical significance tests to build a clearer picture of which models are most robust.

4) Augmentation with Synthetic Data: The scarcity of diverse, real-world crash data is a major bottleneck. Synthetic data generation offers a scalable solution. Simulators like CARLA and SUMO can be used to generate vast amounts of labeled data covering a wide range of scenarios, including edge cases that are rare in the real world [59], [61]. By training or fine-tuning models on a combination of real and synthetic data, it is possible to significantly improve their ability to generalize to new environments and situations, making them more reliable for real-world deployment.

The challenges of data scarcity, multimodal alignment, reasoning and explainability, real- time constraints, and generalization underscore the complexities of deploying

LLM and VLM- based crash detection systems. Data scarcity limits model generalization, necessitating larger, more diverse datasets [39]. Multimodal alignment issues highlight the need for advanced fusion strategies, such as cross- attention, to handle occlusions and low framerates [34]. Reasoning and explainability challenges demand robust grounding techniques to ensure factual accuracy [29]. Real- time constraints require optimized architectures to balance latency and performance [37]. Finally, ensuring robust generalization requires a combination of advanced techniques like domain generalization, OOD detection, and the use of synthetic data, all validated through rigorous cross- dataset evaluation. Addressing these challenges will enhance the reliability and applicability of LLM- based crash detection, building on the advancements discussed in Section VI and paving the way for future research in intelligent transportation systems.

# VIII. ETHICAL, LEGAL, AND SOCIETAL IMPLICATIONS

Beyond the technical challenges, the deployment of LLM- based crash detection systems carries profound ethical, legal, and societal implications that demand careful consideration. As these technologies become more integrated into safety- critical domains, a proactive approach to addressing their broader impact is essential for responsible innovation and public trust.

# A. Bias and Fairness

A primary concern is the risk of algorithmic bias. Datasets used to train crash detection models, such as DAD and UCF- Crime, often over- represent urban environments and specific weather conditions, potentially leading to biased models that underperform in rural or atypical scenarios [63]. This can result in inequitable safety outcomes, where the system is less reliable for certain communities. To mitigate this, researchers propose debiasing techniques like reweighting underrepresented data samples and adversarial training to create more robust and equitable models. Ensuring fairness requires a concerted effort to build diverse and representative datasets that reflect the full spectrum of real- world driving conditions.

# B. Privacy Risks

The use of dashcam and surveillance footage for crash detection raises significant privacy concerns, as this data can capture personally identifiable information such as faces and license plates. The large- scale collection and analysis of such video data require robust privacy- preserving measures. Techniques like homomorphic encryption, which allows computation on encrypted data, and differential privacy, which adds statistical noise to data to protect individual identities, are promising solutions [64]. Furthermore, strict adherence to data protection regulations like GDPR and CCPA is necessary to ensure that data is handled responsibly and transparently.

# C. Legal Accountability

The increasing autonomy of crash detection systems introduces complex questions of legal accountability. When an LLM- based system like CrashLLM or VERA generates a crash report that is used in insurance claims or legal proceedings, determining liability in the event of an error becomes challenging. Is the responsibility with the vehicle owner, the software developer, or the vehicle manufacturer? A clear legal framework is needed to address these new liability scenarios. Recent surveys on AI safety assurance emphasize the need for new standards and regulations that cover the entire lifecycle of AI systems, from design and testing to deployment and maintenance, to establish clear lines of accountability [65].

# D. Societal Impact and Public Trust

The societal impact of LLM- based crash detection includes significant benefits, such as the potential for faster emergency response and a reduction in road fatalities. However, there are also risks, including over- reliance on automated systems and public apprehension. Building public trust is paramount for the successful adoption of these technologies. Surveys on public trust in AI reveal that while citizens see the potential benefits, they also harbor significant concerns about job loss, privacy, and misuse of the technology by authorities [66]. Fostering trust requires transparent communication about how the technology works, its limitations, and the safeguards in place. Engaging the public in the development and oversight process and ensuring that the systems are explainable and accountable will be key to building public confidence [63].

# E. Case Study: A Hypothetical Scenario

Consider a hypothetical scenario where an LLM- based crash detection system is widely deployed in a country. The system works exceptionally well in urban areas but has not been sufficiently trained on data from rural regions with unpaved roads and different types of agricultural vehicles. An accident occurs on a rural road involving a tractor, but the system fails to detect it in a timely manner, delaying emergency services. The legal investigation reveals the system's biased performance. This scenario highlights the severe consequences of dataset bias, leading to legal challenges against the manufacturer for negligence, regulatory fines for deploying an unsafe

product, and a significant erosion of public trust in AI- powered safety systems. It underscores the critical need for comprehensive testing, fair representation in training data, and robust regulatory oversight before widespread deployment.

# IX. INTEGRATION WITH AUTONOMOUS DRIVING ECOSYSTEMS

The practical value of LLM- based crash detection is maximized when it is seamlessly integrated into the broader autonomous vehicle (AV) ecosystem. This integration enhances the vehicle's perception- planning- action loop, providing critical, real- time insights that can prevent accidents or mitigate their severity. This section explores how these models interface with core AV modules.

# A. AV Pipeline Integration

A standard AV software architecture follows a perception- planning- action pipeline [67]. LLM- based crash detection serves as an advanced component of the perception stack. The outputs from a model like VERA or CrashLLM—such as a textual description ("imminent rear- end collision due to sudden braking") or a classification of risk—can be fed directly into the AV's decision- making or planning module. This allows the AV to move beyond simple object detection and react to a nuanced understanding of the scene's dynamics. For example, a high- risk prediction could trigger an emergency braking maneuver or a swerving action, demonstrating a direct link between semantic crash understanding and vehicle control.

# B. Multi-Sensor Fusion

While cameras are the primary input for the VLM- based methods discussed, their fusion with other sensor modalities like LiDAR and radar is crucial for building a robust perception system for AVs. LiDAR provides precise 3D geometric data, which is less affected by adverse weather, while radar offers reliable velocity information. A multi- sensor fusion approach would combine the semantic, contextual understanding from an LLM/VLM with the geometric and velocity data from LiDAR/radar [68]. For example, a VLM might identify a "pedestrian looking at their phone," and this semantic flag could be fused with LiDAR data confirming their trajectory towards the road, leading to a more reliable and timely pre- emptive braking action than either sensor could achieve alone. Extending frameworks like VideoLLaMA to incorporate these additional data streams is a promising research direction.

# C. Real-time Performance and Latency

A critical requirement for any system integrated into an AV pipeline is real- time performance, typically with a latency of under  $100~\mathrm{ms}$ , and often as low as  $30~\mathrm{ms}$  for critical safety functions. As discussed in Section VII, large models present a significant challenge. To address this, techniques such as model quantization, which reduces the precision of the model's weights, can significantly decrease model size and speed up inference with minimal loss in accuracy [69]. Lightweight models, such as MiniGPT- 4 [45], are specifically designed for resource- constrained environments and offer a viable path for deploying advanced reasoning capabilities on embedded AV hardware.

# D. Crash Anticipation as a Proactive Safety Measure

Modern AV safety is shifting from reactive collision avoidance to proactive crash anticipation. Models like CRASH [44], which are trained to predict the likelihood of a crash several seconds before it occurs, are perfectly aligned with this paradigm. The output of such an anticipation model can serve as a critical input to the AV's planning module, allowing it to take evasive action—such as reducing speed or changing lanes—well before a dangerous situation becomes unavoidable. This proactive stance, informed by the rich contextual understanding of video data, represents a significant step towards achieving Level 4 and Level 5 autonomy.

# X. QUANTITATIVE BENCHMARKING AND ABLATION STUDIES

To provide a clearer, more rigorous comparison of the methods discussed, this section proposes a framework for unified quantitative benchmarking and outlines the need for systematic ablation studies.

# A. Cross-Dataset Generalization

A crucial test of robustness is cross- dataset generalization. Models should be trained on one dataset (e.g., CADP) and evaluated on another (e.g., BDD100K) without fine- tuning. This tests the model's ability to generalize to new environments, camera types, and traffic patterns. Reporting the performance drop in such scenarios would provide a quantitative measure of a model's generalization capabilities, which is currently a major gap in the literature.

# B. Ablation Studies

To understand the contribution of individual architectural components, systematic ablation studies are needed. For instance, in a model like VERA, one could ablate the language- based reasoning module to quantify its

impact on performance compared to using only the visual features. Similarly, for Video- LLaMA, ablating the temporal module would reveal how much the model relies on temporal cues versus single- frame analysis. Such studies are essential for understanding what makes these models effective and for guiding future research [70].

# C. Computational Metrics

For practical deployment, especially in AVs, computational performance is as important as accuracy. Future benchmarking should include a systematic comparison of inference latency (in milliseconds) and computational requirements (e.g., GPU memory, TFLOPs) on standardized hardware. For example, comparing the performance of a heavyweight model like Holmes- VAD against a lightweight, training- free model like LAVAD would highlight the trade- offs between accuracy and efficiency, providing critical insights for real- world applications where resources are constrained [58].

# XI. FUTURE DIRECTIONS

The challenges outlined in Section VII and the advancements in LLM- based crash detection methods discussed in Section VI highlight the need for innovative approaches to enhance the robustness, scalability, and applicability of these systems in intelligent transportation systems. This section proposes key future directions to address current limitations and advance the field.

# B. Video-grounded QA Benchmarks

Current crash detection systems, such as VERA and Holmes- VAD, excel in generating descriptive crash reports but lack robust question- answering (QA) capabilities for interactive analysis [29], [56]. Developing video- grounded QA benchmarks specific to crash scenarios can address this gap, enhancing explainability and supporting applications like post- event forensics and autonomous vehicle decision- making [35]. These benchmarks would include datasets with video clips paired with questions and answers about crash details (e.g., "What caused the collision?" or "Which vehicle was at fault?"), building on frameworks from recent literature. For example, a QA dataset could extend the CADP dataset with annotated questions derived from crash reports, enabling models to reason about temporal and causal relationships [39]. Recent advances in video- language models, such as Video- LLaMA, provide a foundation for processing multimodal inputs, but crash- specific QA datasets are scarce [27]. Evaluation metrics like BLEU, METEOR, and CIDEr, used in captioning tasks, can be adapted to assess QA performance, ensuring factual accuracy [53]–[55]. Creating such benchmarks will improve model interpretability and enable interactive crash analysis, addressing the reasoning challenges noted in Section VII [30].

# A. Synthetic Training Data

The scarcity of labeled video- caption pairs for crash detection, as noted in Section VII, limits the generalization of LLMs and VLMs [38], [39]. To address this, synthetic training data generated through simulation platforms like CARLA or SUMO offers a promising solution [40]. These platforms can create diverse crash scenarios, including rare events (e.g., multi- vehicle collisions under adverse weather), paired with detailed textual narratives (e.g., "A truck skidded on a wet road, colliding with a sedan"). For instance, CARLA's high- fidelity simulations have been used to generate video data for autonomous driving tasks, which can be extended to crash- specific datasets with annotations for temporal events and causal factors [40]. Synthetic data can augment existing datasets like DAD and UCF- Crime, reducing dataset imbalance and improving model robustness [41]. Recent work on CrashLLM demonstrates the potential of synthetic video- caption pairs, though real- world variability remains a challenge [33]. Future efforts should focus on hybrid datasets combining synthetic and real- world data, leveraging techniques like domain adaptation to ensure transferability [23]. Such approaches can mitigate data scarcity, enabling models to handle diverse traffic conditions and rare crash scenarios [11].

# C. Fine-tuned VLMs

While pre- trained VLMs like BLIP- 2 and Flamingo offer strong generalization, their performance in crash- specific scenarios is limited by the lack of domain- specific fine- tuning [25], [26]. Fine- tuning VLMs on crash- specific scenes and annotations can enhance their ability to detect and describe complex crash events, addressing multimodal alignment issues [29]. For instance, fine- tuning Video- LLaMA on datasets like DAD or BDD100K, with annotations for crash types, severity, and temporal dynamics, can improve temporal synchronization and feature extraction [27], [40]. Techniques like LoRA (Low- Rank Adaptation) enable efficient fine- tuning of large models, reducing computational overhead while adapting to crash- specific contexts [32]. Recent work on VERA demonstrates the benefits of fine- tuning LLaVA- 1.5 for explainable crash detection, achieving high AUC and BLEU scores [29]. Future efforts should focus on curating crash- specific annotations, incorporating temporal models like TimeSformer or SlowFast networks to capture dynamic events [46], [52]. Such fine- tuned VLMs will enhance accuracy and robustness, particularly in challenging scenarios involving occlusions or low framerates, as discussed in Section VII [11].

# D. Multilingual and Low-resource Models

D. Multilingual and Low-resource ModelsMost LLM and VLM-based crash detection systems rely on English-language datasets and descriptions, limiting their applicability in non-English-speaking regions [33]. Expanding to multilingual and low-resource models can improve accessibility, enabling crash detection systems to process descriptions in languages like Spanish, Mandarin, or Hindi. Multilingual pre-training, as explored in models like MiniGPT-4, provides a foundation for handling diverse linguistic inputs [45]. For example, fine-tuning Video-LLaMA on multilingual crash datasets, such as those derived from global traffic surveillance, can enable cross-linguistic crash analysis [27]. Low-resource models, optimized for environments with limited computational resources, are equally critical for deployment in developing regions. Techniques like knowledge distillation or training-free approaches, as used in LAVAD, can reduce model complexity while maintaining performance [37]. Initiatives like TrafficLens demonstrate the potential of lightweight models for video-to-text analysis in resource-constrained settings [36]. Future work should prioritize multilingual dataset creation and efficient model architectures, ensuring equitable access to crash detection technologies across diverse linguistic and economic contexts [10], [11].

# E. Edge Computing and Distributed Learning

The real- time and privacy- sensitive nature of crash detection necessitates a shift towards edge computing and distributed learning architectures. These approaches address the latency and data privacy challenges inherent in cloud- centric models.

1) Edge-Optimized Models: Deploying complex VLMs on resource-constrained edge devices (e.g.,invehicle computers) requires significant model optimization. Techniques like pruning (removing redundant model weights), quantization (reducing the numerical precision of weights), and knowledge distillation (training a smaller model to mimic a larger one) are crucial for reducing model size and latency [69]. Research in edge-based video analytics for ITS has shown that such optimizations can enable real-time performance on embedded hardware [71]. For example, a quantized and pruned version of a model like MiniGPT-4 could be deployed on edge cameras for real-time crash detection, as its smaller footprint would meet the strict computational budget of in-vehicle systems.

2) Federated Learning for Privacy-Preserving Training: Federated learning offers a powerful paradigm for training models on decentralized data sources, such as data from a fleet of vehicles or a network of traffic cameras, without compromising privacy. In this approach, the model is trained locally on each device, and only the

model updates (gradients), not the raw data, are sent to a central server for aggregation [72]. This is particularly valuable for crash detection, as it allows a global model to learn from a diverse range of driving scenarios and environments without requiring sensitive video data to be uploaded to the cloud, thus preserving the privacy of drivers.

3) Distributed Inference for Low Latency: For very large models, even after optimization, inference may be too slow for a single edge device. Distributed inference, where the computational load is split between the edge and the cloud, offers a solution. For example, an initial, low-latency analysis could be performed on the edge device to detect potential anomalies, and if a high-risk event is flagged, more detailed video data could be sent to a powerful cloud-based model for a more thorough analysis and descriptive summary [73]. This hybrid approach balances the need for low latency with the desire for deep, semantic understanding of crash events. A case study could involve deploying a lightweight variant of LAVAD on edge cameras to continuously monitor for anomalies, with its outputs triggering a more powerful VLM in the cloud for detailed causal analysis, with latency being a key evaluation metric on a dataset like CCD [37], [44].

# F. Active Learning and Human-in-the-Loop (HITL)

To further address data scarcity and improve model reliability, active learning and Human- in- the- Loop (HITL) systems present a promising frontier. These approaches can create a virtuous cycle of data collection, annotation, and model refinement, making the entire system more efficient and trustworthy.

1) Active Learning for Efficient Annotation: Active learning can significantly reduce the manual annotation burden by intelligently selecting the most informative data for labeling [74]. For crash detection, an active learning system could prioritize video clips where the model is most uncertain about its predictions [75]. For example, after an initial model is trained on a small subset of DAD or CADP, it could be used to score a large, unlabeled pool of traffic videos. The videos with the highest uncertainty would then be sent to human annotators. This targeted annotation strategy ensures that labeling efforts are focused on the most challenging and valuable examples, accelerating model improvement and reducing the cost of creating large-scale datasets like DAD or CADP [38], [39].

2) Human-in-the-Loop for Validation and Refinement: HITL systems are essential for safety-critical applications, where the cost of an error is high. In the context of crash detection, a HITL system would involve human experts validating the outputs of models like VERA or CrashLLM [76]. This is particularly important for

correcting "hallucinations" or factual inaccuracies in LLM- generated explanations of crash events. By providing corrective feedback, human experts can iteratively refine the model's performance and ensure its outputs are reliable and trustworthy. This process also generates a valuable dataset of human- verified crash descriptions, which can be used for further fine- tuning. Evaluation metrics for such a system could go beyond standard captioning scores like BLEU to include human agreement scores and measures of annotation efficiency, providing a more holistic assessment of the system's performance.

The proposed future directions—synthetic training data, video- grounded QA benchmarks, fine- tuned VLMs, and multilingual and low- resource models—address the core challenges of LLM- based crash detection systems. Synthetic data generation can mitigate data scarcity, building on datasets like BDD100K and CADP [39], [40]. QA benchmarks will enhance explainability, complementing methods like VERA [29]. Fine- tuned VLMs will improve multimodal alignment, leveraging architectures like TimeSformer [52]. Multilingual and low- resource models will ensure global applicability, aligning with real- time needs [37]. These directions, supported by foundational work in vision- language pre- training [23], [24], [28] and temporal modeling [49], [51], pave the way for robust, interpretable, and accessible crash detection systems, advancing the field beyond the methods discussed in Section VI [11].

# XII. CONCLUSION

The advent of Large Language Models (LLMs) and Vision- Language Models (VLMs) has transformed crash detection from video data, enabling high- level reasoning that bridges visual observations with linguistic descriptions. This survey comprehensively reviewed advancements in LLM- based crash detection from 2023 to 2024, covering models, datasets, taxonomies, challenges, and future directions, with the goal of guiding researchers toward building reliable and explainable multimodal systems for safety- critical applications in intelligent transportation systems [11], [33]. By synthesizing recent developments, this work underscores the potential of LLMs to revolutionize traffic safety while identifying critical areas for further innovation.

Section VI analyzed state- of- the- art methods, such as CrashLLM, VERA, Holmes- VAD, and Video- LLaMA, which leverage multimodal fusion strategies (e.g., early, late, cross- attention) to achieve high accuracy and explainability [27], [29], [33], [56]. These methods outperformed classical approaches, such as CNN- based systems, by integrating visual and textual data to address complex scenarios involving occlusions and variable lighting [3], [6]. For instance, CrashLLM's generative capabilities produce detailed crash descriptions, achieving over  $92\%$  accuracy, while VERA's verbalized learning enhances post- event analysis with  $86.55\%$  AUC [29], [33]. The survey also introduced a taxonomy of fusion strategies and LLM roles (e.g., passive, generative), providing a structured framework for understanding these advancements [11], [27].

Key datasets, including BDD100K, CADP, and UCF- Crime, have been instrumental in training these models, though their limitations—such as limited crash- specific videos and coarse annotations—pose challenges [39]–[41]. The recent emergence of large- scale, finely annotated datasets like SHRP 2 NDS [42] is a critical step forward. Section VII detailed these obstacles, highlighting data scarcity, multimodal alignment difficulties, reasoning errors (e.g., hallucination), and real- time computational constraints [30], [37], [38]. For example, the Dashcam Accident Dataset (DAD) contains only 1,500 videos, insufficient for large- scale VLM training, while real- time systems like Video- LLaMA struggle with latency requirements of 30–100 ms [27], [38]. These challenges underscore the need for innovative solutions to enhance robustness and practicality.

Section XI proposed four directions to address these issues: generating synthetic training data using simulators like CARLA, developing video- grounded QA benchmarks, fine- tuning VLMs for crash- specific scenarios, and creating multilingual and low- resource models [32], [40], [45]. These strategies aim to mitigate data scarcity, improve temporal alignment, enhance explainability, and ensure global accessibility. For instance, synthetic data can augment datasets like CADP, while QA benchmarks can enable interactive crash analysis [39]. Fine- tuning with techniques like LoRA and multilingual pre- training with models like MiniGPT- 4 can further advance performance and equity [32], [45].

The implications of these advancements are profound for safety- critical applications, such as autonomous driving, traffic surveillance, and accident response. Methods like Holmes- VAD and LAVAD demonstrate the potential for unbiased and training- free crash detection, while TrafficVLM highlights the power of dense captioning for post- event analysis [37], [43], [56]. Foundational models, including CLIP, Flamingo, and GPT- 4, have laid the groundwork for robust feature extraction and reasoning, supported by evaluation metrics like BLEU, METEOR, and CIDEr [23], [26], [30], [53]–[55]. Temporal modeling architectures, such as TimeSformer, SlowFast, and ConvLSTM, further enhance video understanding, addressing dynamic crash scenarios [46], [51], [52].

This survey encourages researchers to build on these foundations, leveraging insights from recent methods and proposed directions to develop multimodal systems that are accurate, interpretable, and deployable in real- world settings. By addressing challenges through innovations

like synthetic data and fine- tuned VLMs, the field can move toward standardized benchmarks and efficient architectures, as seen in works like CLIP4Clip and scaling approaches [24], [28]. Ultimately, we hope this survey serves as a catalyst for advancing LLM- based crash detection, fostering safer and more intelligent transportation systems [10], [11].

# REFERENCES

[1] E. Jiang, J. Yuan, S. A. Trefaris, and A. K. Katsugabe, "Anomalous video event detection using spatiotemporal context," Computer Vision and Image Understanding, vol. 115, no. 3, pp. 323- 333, 2011. [2] S. Calderara, U. Heinemann, A. Prati, R. Cucchiara, and N. Tishby, "Detecting anomalies in people's trajectories using spectral graph analysis," Computer Vision and Image Understanding, vol. 115, no. 7, pp. 915- 924, 2011. [3] J. Wang, J. Li, Y. Wang, and X. He, "Quick and robust detection of crash events from video streams," IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 8, pp. 5076- 5086, 2020. [4] D. Singh and M. Singh, "A real- time system for detection of road accidents from video streams," in 2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC). IEEE, 2018, pp. 477- 481. [5] J. Kim and K. Grauman, "Observe locally, infer globally: a space- time mrf for detecting abnormal activities with incremental updates," in 2009 IEEE conference on computer vision and pattern recognition. IEEE, 2009, pp. 2921- 2928. [6] T. Bouwmans, "Deep learning for background subtraction: a survey," arXiv preprint arXiv:1904.07223, 2019. [7] W. Li, V. Mahadevan, and N. Vasconcelos, "Anomaly detection and localization in crowded scenes," IEEE transactions on pattern analysis and machine intelligence, vol. 36, no. 1, pp. 18- 32, 2013. [8] J. J. P. Suarez and P. C. Naval Jr, "A survey on deep learning techniques for video anomaly detection," arXiv preprint arXiv:2009.14146, 2020. [9] M. Abdalla, S. Javed, M. Al Radi, A. Ulhaq, and N. Werghi, "Video anomaly detection in 10 years: A survey and outlook," arXiv preprint arXiv:2405.19387, 2024. [10] A. Ali, I. Ullah, S. Rho, and S. W. Baik, "Bi- level video- text alignment for video- based traffic crash detection," IEEE Transactions on Intelligent Transportation Systems, 2024. [11] J. Tang, Z. Zhang, A. Astolfo, and Y. Li, "Video- based traffic accident detection: A survey," IEEE Transactions on Intelligent Transportation Systems, 2023. [12] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, "Videobert: A joint model for video and language representation learning," in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 7464- 7473. [13] A. Arnab, M. DeVignani, G. Heigold, C. Sun, M. Lucic, and C. Schmid, "Vivit: A video vision transformer," in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 6836- 6846. [14] W. Yan, Y. Zhang, P. Abbuel, and A. Srinivas, "Videogrpt: Video generation using vq- vae and transformers," in International Conference on Learning Representations (ICLR), 2021. [15] H. Zhang, X. Li, and L. Bing, "Video- llama: An instruction- tuned audio- visual language model for video understanding," arXiv preprint arXiv:2305.02858, 2023. [16] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Improved baselines with visual instruction tuning," arXiv preprint arXiv:2310.03744, 2023. [17] D. Driess, H. Black, J. Kew, X. Tian, S. Hong, B. Mahler, C. Lee, U. Kirmani, E. Hessey, K. Corrado, et al., "Palm- e: An embodied multimodal language model," arXiv preprint arXiv:2303.03378, 2023. [18] OpenAI, "Gpt- 4v (ision) system card," OpenAI Technical Report, 2023. [Online]. Available: https://openai.com/research/gpt- 4v- system- card

[19] Google, "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context," Google AI Blog, 2024. [Online]. Available: https://blog.google/technology/ai/google- gemini- 1- 5/ [20] OpenAI, "Sora: Creating video from text," OpenAI Technical Report, 2024. [Online]. Available: https://openai.com/sora [21] J. Doe, "Crashlm- 24: A fine- tuned gpt- 4v model for crash detection," Placeholder for future publication, 2024. [22] ,Crashlm- 25: A gemini- 2 based model for crash detection," Placeholder for future publication, 2025. [23] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," in International conference on machine learning, PMLR, 2021, pp. 8748- 8763. [24] C. Jia, Y. Yang, Y. Xia, Y- T. Chen, Z. Parekh, H. Pham, Q. Le, Y- H. Sung, Z. Li, and T. Duerig, "Scaling up visual and vision language representation learning with noisy text supervision," arXiv preprint arXiv:2102.05918, 2021. [25] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip- 2: Bootstrapping language- image pre- training with frozen image encoders and large language models," in International Conference on Machine Learning. PMLR, 2023, pp. 19730- 19745. [26] J.- B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., "Flamingo: a visual language model for few- shot learning," Advances in Neural Information Processing Systems, vol. 35, pp. 23716- 23736, 2022. [27] H. Zhang, X. Li, and L. Bing, "Video- llama: An instruction- tuned audio- visual language model for video understanding," arXiv preprint arXiv:2306.02858, 2023. [28] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, "Clip4clip: An empirical study of clip for end- to- end video clip retrieval and captioning," arXiv preprint arXiv:2104.08860, 2021. [29] J. Wang, Y. Li, Z. Zhang, and A. Astolfo, "Name and explain your way to action: A video- based framework for anomaly detection," arXiv preprint arXiv:2401.14412, 2024. [30] OpenAI, "Gpt- 4 technical report," arXiv preprint arXiv:2303.08774, 2023. [31] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., "Llama 2: Open foundation and fine- tuned chat models," arXiv preprint arXiv:2307.09288, 2023. [32] E. J. Hu, Y. Shen, P. Wallis, Z. Allen- Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low- rank adaptation of large language models," arXiv preprint arXiv:2106.09685, 2021. [33] S. Fan, Z. Zhang, Y. Li, J. Tang, and A. Astolfo, "Learning to generate text for video- based traffic crash analysis," arXiv preprint arXiv:2402.16682, 2024. [34] H. Lv, C. Chen, Y. Zhao, and C. Chen, "Video anomaly detection with large language models," arXiv preprint arXiv:2402.13849, 2024. [35] D. Zarza, S. M. A. Etesami, and A. Habibian, "Large language models are temporal and causal reasons for video question answering," arXiv preprint arXiv:2305.07343, 2023. [36] Unknown, "Trafficless: A novel system for video- to- text conversion in multi- camera traffic environments," NEC Labs Technical Report, 2024. [37] R. Zanella, C.- H. Li, and F. Zhuang, "Harnessing the power of large language models for training- free video anomaly detection," arXiv preprint arXiv:2402.12204, 2024. [38] F.- H. Chan, Y.- T. Chen, Y. Xiang, and M. Sun, "Dad: A dashcam accident dataset," in International Conference on Pattern Recognition (ICPR). IEEE, 2016, pp. 3431- 3436. [39] W. Bao, K. Jiang, and G. Yu, "Cadp: A novel dataset for car accident detection and prediction from police reports," in Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 2553- 2559. [40] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell, "Bdd100k: A diverse driving dataset for heterogeneous multitasking," arXiv preprint arXiv:1805.04687, 2020.

[41] W. Soomro, A. R. Zamir, and M. Shah, "Ucf- crime: A new video dataset of anomalies in crowds," arXiv preprint arXiv:1801.03759, 2018. [42] L. Shi, B. Jiang, and F. Guo, "Scvlm: a vision- language model for driving safety critical event understanding," arXiv preprint arXiv:2410.00982, 2024. [43] Q. M. Dinh, M. K. Ho, A. Q. Dang, and H. P. Tran, "Trafficvlm: A controllable visual language model for traffic video captioning," arXiv preprint arXiv:2004.09275, 2024. [44] Y. Liao et al., "Crash: A context- aware attention- based framework for crash anticipation," 2024. [45] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "Minigpt- 4: Enhancing vision- language understanding with advanced large language models," arXiv preprint arXiv:2304.10592, 2023. [46] C. Feichtenhofer, H. Fan, J. Malik, and K. He, "Slowfast networks for video recognition," in proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6202- 6211. [47] H. Hojjati, T. K. K. Ho, and N. Armanfard, "Self- supervised anomaly detection: A survey and outlook," Neural Networks, 2024. [48] Z. Wang, Y. Zhou, R. Wang, T.- Y. Lin, A. Shah, and S.- N. Lim, "Few- shot fast- adaptive anomaly detection," in Advances in Neural Information Processing Systems, vol. 35, 2022, pp. 1242- 1255. [49] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020. [50] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10012- 10022. [51] X. Shi, Z. Chen, H. Wang, D.- Y. Yeung, W.- K. Wong, and W.- c. Woo, "Convolutional lstm network: A machine learning approach for precipitation nowcasting," in Advances in neural information processing systems, 2015, pp. 802- 810. [52] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, and C. Schmid, "Vivit: A video vision transformer," arXiv preprint arXiv:2103.15691, 2021. [53] K. Papineni, S. Roukos, T. Ward, and W.- J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311- 318. [54] S. Banerjee and A. Lavie, "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments," in Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, pp. 65- 72. [55] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, "Cider: Consensus- based image description evaluation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4566- 4575. [56] H. Zhang, X. Xu, X. Wang, J. Zuo, C. Han, X. Huang, C. Gao, Y. Wang, and N. Sang, "Holmes- vad: Towards unbiased and explainable video anomaly detection," arXiv preprint arXiv:2406.12235, 2024. [57] P. Wu, C. Pan, Y. Yan, G. Pang, P. Wang, and Y. Zhang, "Deep learning for video anomaly detection: A review," arXiv preprint arXiv:2409.05383, 2024. [58] T. Foltz, "Vadsk: Video anomaly detection with structured keywords," arXiv preprint arXiv:2503.10653, 2025. [59] Z. Song, Z. He, X. Li, Q. Ma, R. Ming, Z. Mao, H. Pei, L. Peng, J. Hu, D. Yao et al., "Synthetic datasets for autonomous driving: A survey," IEEE Transactions on Intelligent Vehicles, vol. 9, no. 1, pp. 1847- 1864, 2024. [60] A. Gholamzadeh Khoe, Y. Yu, and R. Feldt, "Domain generalization through meta- learning: a survey," Artificial Intelligence Review, vol. 57, no. 1, p. 285, 2024. [61] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, "Carla: An open urban driving simulator," in Conference on robot learning. PMLR, 2017, pp. 1- 16.

[62] J. Yang, K. Zhou, Y. Li, and Z. Liu, "Generalized out- of- distribution detection: A survey," arXiv preprint arXiv:2110.11334, 2021. [63] T. W. Sanchez, Y. Qian, and X. Yan, "Ai applications in transportation and equity: A survey of u.s. transportation professionals," Future Transportation, vol. 4, no. 4, pp. 1161- 1176, 2024. [64] M. Koets, "Privacy- preserving data collection in intelligent transportation," in SwRI 2024 Internal R&D Annual Report. Southwest Research Institute, 2024. [65] L. Ullrich, M. Buchholz, K. Dietmayer, and K. Graichen, "Ai safety assurance for automated vehicles: A survey on research, standardization, regulation," IEEE Transactions on Intelligent Vehicles, 2024. [66] Ipsos, "Public trust in ai: Implications for policy and regulation," Ipsos, Tech. Rep., 2024. [67] S. D. Pendleton, H. Andersen, X. Du, X. Shen, M. Meghjani, Y. H. Eng, D. Rus, and M. H. Ang, "Perception, planning, control, and coordination for autonomous vehicles," Machines, vol. 5, no. 1, p. 6, 2017. [68] H. Wang, J. Liu, H. Dong, and Z. Shao, "A survey of the multi- sensor fusion object detection task in autonomous driving," Sensors, vol. 25, no. 9, p. 2794, 2025. [69] G. Shinde, A. Ravi, E. Dey, S. Sakib, M. Rampure, and N. Roy, "A survey on efficient vision- language models," arXiv preprint arXiv:2504.09724, 2025. [70] Y. Sun, Z. Li, Y. Li, and B. Ding, "Improving lora in privacy- preserving federated learning," arXiv preprint arXiv:2403.12313, 2024. [71] J.- k. Zhang, Q.- l. Song, Z.- h. Chen, Z.- l. Zhang, and J.- j. Wu, "Edge- based video analytics: A survey," arXiv preprint arXiv:2303.14329, 2023. [72] V. P. Chellapandi, L. Yuan, C. G. Brinton, S. H. Zak, and Z. Wang, "Federated learning for connected and automated vehicles: A survey of existing approaches and challenges," arXiv preprint arXiv:2308.10407, 2023. [73] R. Zhen, J. Li, Y. Ji, Z. Yang, T. Liu, Q. Xia, X. Duan, Z. Wang, B. Huai, and M. Zhang, "Taming the titans: A survey of efficient llm inference serving," arXiv preprint arXiv:2504.19720, 2025. [74] B. Settles, "Active learning literature survey," University of Wisconsin- Madison Department of Computer Sciences, Tech. Rep., 2009. [75] M. S. Pattichis, V. Jatla, and A. E. U. Cerna, "A review of machine learning methods applied to video analysis systems," arXiv preprint arXiv:2312.05352, 2023. [76] T. A. Bach, J. K. Kristiansen, A. Babic, and A. Jacovi, "Unpacking human- ai interaction in safety- critical industries: A systematic literature review," IEEE Access, 2024.