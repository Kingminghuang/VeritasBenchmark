# Transformer-based EEG Decoding: A Survey

Haodong Zhang, Hongqi Li* Member, IEEE

Abstract—Electroencephalography (EEG) is one of the most common signals used to capture the electrical activity of the brain, and the decoding of EEG, to acquire the user intents, has been at the forefront of brain- computer/machine interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods with machine learning, the advent of deep learning approaches have gradually revolutionized the field by providing an end- to- end long- cascaded architecture, which can learn more discriminative features automatically. Among these, Transformer is renowned for its strong handling capability of sequential data by the attention mechanism, and the application of Transformers in various EEG processing tasks is increasingly prevalent. This article delves into a relevant survey, summarizing the latest application of Transformer models in EEG decoding since it appeared. The evolution of the model architecture is followed to sort and organize the related advances, in which we first elucidate the fundamentals of the Transformer that benefits EEG decoding and its direct application. Then, the common hybrid architectures by integrating basic Transformer with other deep learning techniques (convolutional/recurrent/graph/spiking neural networks, generative adversarial networks, diffusion models, etc.) is overviewed in detail. The research advances of applying the modified intrinsic structures of customized Transformer have also been introduced. Finally, the current challenges and future development prospects in this rapidly evolving field are discussed. This paper aims to help readers gain a clear understanding of the current state of Transformer applications in EEG decoding and to provide valuable insights for future research endeavors.

Index Terms—EEG decoding, feature fusion, transformer, convolutional neural network, signal processing.

# I. INTRODUCTION

A S a cutting- edge technology to directly bridge the hu- man mind with external devices, brain- computer/machine interfaces (BCIs/BMIs) have garnered more and more attention in both rehabilitation engineering and non- medical domains [1], [2]. The non- invasive electroencephalography (EEG) records the activity of cortical neurons through electrodes placed on the scalp, providing millisecond- level temporal resolution that allows for tracking rapid changes in brain activity, and has been widely used in BCIs [3]. Accordingly, the efficient EEG decoding through the analysis to capture the underlying user intents is of great significance, which generally follows a set of steps with signal acquisition, preprocessing, feature extraction, and classification.

Traditionally, machine learning methods are used in the above procedures, where researchers usually start by extracting features manually of three main dimensions, i.e., frequency, temporal, and spatial domains from raw EEG signals, which are then fed into specific classifiers for decoding and recognition. Specifically, the features of the frequency, time- frequency, and spatial- frequency have been designed first based on the specific expertise and extracted with the common spatial pattern CSP).Fast Fourier Transform (FFT), and Wavelet Transform, etc. Then, supervised classification algorithms such as the support vector machine, random forest, and linear discriminant analysis, or unsupervised learning methods such as K- nearest neighbor analysis (KNN) are employed. However, manual feature design and extraction rely heavily on expert experience and is a time- consuming and complex process.

Conversely, deep learning (DL) methods have recently achieved great success in the field of EEG decoding due to their powerful automatic feature extraction and representation learning capabilities. Briefly, DL is inspired by a hierarchical structure based on the visual cortex of the human brain, which mainly represents a long cascade of information extraction architecture with multiple processing layers to reflect its depth. Since it allows for increasingly abstract but more representative features through the constant transfer of information from the lowest layer to the highest, DL techniques such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short- term memory (LSTM), transfer learning, have already been successfully applied for the EEG decoding. Nevertheless, many of above DL models struggle to parallelly model the complex temporal patterns and high- dimensional feature spaces of non- stationary EEG data.

As another classical type of DL algorithm, Transformer was introduced by Google in 2017 to revolutionize sequence processing tasks [4]. The core innovation of Transformer is the multi- head attention mechanism (MHA), which calculates the relationships between elements in the input sequence and assigns different importance weights to each element. To this end, it enables the model to capture complex temporal and spatial patterns, particularly those long- range dependencies entailed in EEG signals. The positional encoding embedded in the model ensures accurate temporal positioning, which is essential for identifying intricate relationships across different segments of EEG sequences. Moreover, multiple attention heads also allow the Transformer to process information in parallel across different subspaces, enhancing both efficiency and expressive power. In a nutshell, the Transformer architecture bears the benefits of parallelization, scalability, and flexibility, which has shown a promising future in various domains [5]. Since 2019, such a model has gradually appeared in EEG speech recognition [6], epilepsy detection [7], and classification tasks [8], and the resulting significant advances have led to a new wave of EEG research.

interfaces (BCIS/BMIS) have garnered much attention over the past decades due to their outstanding ability to convert

the users' brain activity into machine- readable intentions or commands [1]. Among various BCI modalities, noninvasive electroencephalograph (EEG) has the advantages of adequate temporal resolution, non- surgical electrode placements, and low cost, thus leading to its widest application in the fields of rehabilitation engineering [2], [3], cognitive science [4], neuroscience, and psychology [5].

However, although nearly two hundred papers have been published to date, few comprehensive review articles were found to clarify the trends of the Transformers application in the EEG field. Chen et al. [9] reported the use of fundamental Transformer model in the field of brain science, emphasizing on its broad variety of applications of the brain disease diagnosis, brain age prediction, brain anomaly detection, etc. More focally, in [10], Abibullaev et al. concluded the model's foundational principles, technical advantages, and potential challenges applied to BCIs. Further, the importance of data limitations and augmentation in Transformer- based BCI systems has been highlighted in [11]. However, to the best of our knowledge, there is no detailed overview of the specific evolution and variants of the relevant models to shed light on current trends in technological progress under the explosive growth of Transformers in EEG decoding. The urgent need for a thorough comprehensive summary of the recent advances gives rise to our current work.

# A. Contribution and Overview

We searched the literature from databases of IEEE Xplore, Science Direct, Web of Science, Google Scholar, and arXiv. The cut- off date of the screening was Jun. 29th, 2025, and the main keywords were "Electroencephalogram/EEG", and "Transformer/vision transformer/ViT/self- attention/multi- head attention", and "decoding/classification/detection/recognition". After the search, the following screening criteria were applied:

- Studies focused on non-EEG signals (e.g., EOG, EMG) were excluded, except for involved multimodal methods.- Papers only mentioned Transformers in the title/abstract but were unrelated to the main text or lacked innovative methods were eliminated.- Studies with unclear design, insufficient result analysis, or lacking comparative experiments were disqualified.- Priority was given to influential studies validated using diverse standard datasets.

As a result, about 165 papers related to the application of Transformers in EEG decoding since 2020 were screened and compiled, to provide a comprehensive summary and in- depth analysis of recent advancements. The main contributions of this review are: 1) Provide readers with the most thorough overview to date on the applicability of Transformer model in the field of EEG decoding. The distillation carried out via this work facilitates clarifying the evolving lineage with relevant research and helps to excite the attention of potential researchers; 2) We classify the Transformer- based variants for EEG processing tasks in terms of direct Transformer, its combination with other DL approaches, and customization. Further, for the customized ones, we sort out for the first time the specific categories of enhanced intrinsic structures while offering clear research entry points; 3) Challenges and future directions of the Transformer within the EEG decoding are discussed in explicit detail, especially those applicable models related to specific decoding tasks and datasets, to inspire the further dialectical rethinking about the development of the field and flourish the community.

# B. Paper Organization

Overall, for a variety of EEG tasks (classification, generation, and denoising, etc.), the Transformer is exploited by means of the backbone models (used independently), hybrid models (combined with other DL- based networks), and customized Transformer (variants based on modified intrinsic structures). Hence, the remainder of paper is organized as follows. Section II provides an overview of Transformer basics and the direct application of backbone models to EEG decoding. Afterwards, along with the line of model evolution, current research are systematically reviewed, including the hybrid architectures of base Transformer with other DL models in Section III, and customized Transformer- based networks in Section IV. Finally, Section V discusses current challenges and future directions, while Section VI concludes the current work.

# II. TRANSFORMER BASICS AND DIRECT APPLICATIONS

The basic Transformer model optimizes the sequence- to- sequence tasks with its Encoder- Decoder structure and multihead self- attention mechanism. These features have enabled the Transformer to effectively capture subtle patterns and dynamic changes of EEG, thus recognizing intricate patterns.

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/fda0d769b604ad030e97805e4176078c921c4d8bff1c4f45eded1235191e290f.jpg)  
Fig. 1 General structure of Transformer backbone model.

Fig.1 shows the general structure of Transformer backbone model, starting from Input Embedding, which provides a numerical representation of raw data by converting it into dense vectors. Then, Positional Encoding adds unique position- based information to preserve sequence order. Two core parts of the Transformer Encoder and Decoder consist of multiple layers to construct learning models, where the self- attention mechanism of Encoder processes the entire sequence context and Decoder employs masked attention to preserve causality in sequence generation tasks. The core contents of self- attention are:

$$
Q = X_{P}W_{Q},K = X_{P}W_{K},V = X_{P}W_{V} \tag{1}
$$

$$
Attention(Q,K,V) = Softmax(QK^{T} / \sqrt{D_{k}})V \tag{2}
$$

where  $Q, K, V$  are defined as the input of the self- attention, which are the query, key, and value, respectively.  $\sqrt{D_{k}}$  serves as a scaling factor to prevent SoftMax function from entering

regions where it has extremely small gradients. The multi- head attention mechanism enhances the model's ability to extract features by running several attention processes in parallel:

$$
\begin{array}{r}M u l t i H e a d(Q,K,V) = C o n c a t(h e a d_{1},\dots h e a d_{h})W^{O}\\ H e a d_{i} = S e l f A t t e n t i o n(Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{W}) \end{array} \tag{3}
$$

where each head of  $Head_{i}$  captures different aspects of the input, and Wo is the weight matrix for the output linear transformation that combines the outputs of all heads. During the decoding, encoded data and previously generated tokens are processed using self- attention to predict the next token. The final outputs are produced through a Softmax layer, mapping predictions to probabilities for tasks like text generation or classification [12].

The simplest way to use a Transformer in EEG processing is to apply the above backbone model directly. In this regard, the Encoder is predominantly used due to its ability to effectively capture the spatiotemporal characteristics of EEG, while the Decoder is less commonly employed, as tasks typically do not involve sequence generation. Overall, conventional temporal and spatial features formed by interactions between electrode channels, and spectral features are key EEG features. Studies have used the basic transformer encoder to extract these various features or their combinations. For instance, spatiotemporal sequence structures were designed to stack features [13], time- frequency branch structures were developed for feature fusion [14], and study [15] applied different features to build separate models. Considering the classification based solely on partial dimensional features is often insufficient, models to combine spatio- temporal, label- label correlation features [16], and temporal- spatial- spectral features [17], [18], are developed. Especially, AMDET proposed in [19] converted EEG data into three- dimensional temporal- spatial- spectral representations, utilizing Transformer to extract spectral and temporal features, and thus enhancing emotion recognition. Apart from the feature extraction, Transformer model can also be used in applications of EEG source imaging (ESI) [20], semi- supervised domain adaption [21], and transfer learning using self- supervised pre- training methods [22]. Moreover, similar to machine translation, the complete encoder- decoder structure has been attempted in Seq2seq tasks, such as the reconstruction of EEG- limb data [23].

As a milestone applied in visual domain, Vision Transformer (ViT) has become a common specialized Transformer that processes images through a series of steps to achieve efficient classification [24]. Since the EEG can be transformed into image- like formats, the general ViT backbone structure has been introduced in EEG decoding. As shown in Fig. 2, during the Patch Embedding step, ViT divides the image into fixed- size patches and converts each patch into an embedding vector through linear projection. A [CLS] token is introduced for classification purposes. Position encodings are then added to each embedding vector to preserve the spatial relationships between patches. These embedding vectors are fed into the Encoder as a sequence, where the self- attention mechanism captures dependencies between image patches. Finally, the [CLS] token aggregates global features, which are then used by the classification head for image classification tasks.

The idea of converting EEG signals into images before processing them with the original ViT has been widely applied to various tasks of brain disease monitoring [25], sleep staging [26], [27], and trust recognition [28], where image conversion techniques include short- time Fourier transform (STFT) [25], [29], multimodal processing [26], and pseudo- images from raw signals [28], [30]. More specifically, in [31], the signals are divided into small segments according to the image patch segmentation method, while each channel of the signal in [28] is treated as a patch and input into the ViT. Besides, ViT has also been used as part of ensemble learning [32], and as an independent classifier [33]. In 2023, Li et al. [34] proposed an automatic transformer neural architectures search framework to optimize ViT parameters for EEG- based emotion recognition. In 2024, a domain discrimination model based on ViT to extract common features from different datasets was designed [35], showing effectiveness in sensitivity, false alarm rate, and area under the receiver operating characteristic curve (AUC).

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/6c2f42c2fddad673177fea60ecc0c6bf2786970f6fbbc191546f83ddb741ac61.jpg)  
Fig. 2 General ViT backbone structure within EEG decoding.

In conclusion, most of discussed methods rely on straightforward architectures for specific tasks, with limited emphasis on the generalization and multi- dimensional feature extraction. Therefore, researchers are combining Transformer with other models and developing advanced variants. These achievements of a hybrid and customized models are detailed below.

# III. HYBRID MODELS

Although the Transformer model excels in extracting global dependencies in EEG, it has limitations in dealing with more detailed local features, especially short- term fluctuations and rapid changes. These local features, however, are critical for understanding the dynamics of brain activity. Moreover, the analysis of spatial features, for instance, to get connectivity patterns between different brain regions, also requires more fine- grained processing, while the basic transformer model may not adequately capture such information. Consequently, hybrid models of Transformer in conjunction with other DLM models have been widely investigated, which are concluded as follows.

# A. CNN-Transformer

Before adoption of Transformers, CNNs were the dominant models in the field of deep learning, and they remain widely used to date. However, CNNs have inherent limitations in handling sequential data [36], particularly for biological signals such as EEG. Such limitations stem from the convolution operation, which focuses mainly on local features

and is not sufficiently sensitive to long- range dependencies within sequences. These issues, however, can be effectively addressed by Transformers, thus providing new perspectives. For a hybrid CNN- Transformer, CNNs are often exploited to perform the tokenization and embedding of EEG data, while Transformer extracts the entailed global features from the embedded representations. As shown in Fig. 3, the relevant research advances are elaborated according to the CNNs design style and role in feature extraction, which are categorized as the dimension- and structure- based convolution models.

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/e425b207ac9ddf9b1abc60441dbb9b6606590d5856dd0d3c1a306de60bc47ce5.jpg)  
Fig. 3 General CNN-Transformer sequential architecture.

1) CNN Dimension-based Hybrid Model: One-dimensional convolution (Conv1D) is widely applied in sequential data analysis. The early hybrid CNN-Transformer model often used Conv1D first to extract temporal features locally, while the subsequent Transformer module integrated these features for more detailed analysis[7], [15], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46]. The fixed kernel sizes of standard Conv1D limits its ability to capture both short- and long-term features. To overcome this, multi-resolution Conv1D was proposed to apply various kernel sizes in parallel within a single layer, where smaller kernels capture high-frequency, short-term details, and larger ones focus on the others. These extracted features are then fused via concatenation [47], [48], weighted fusion [49], [50], [51], [52], [53] or Transformer [54], [55], [56], [57], [58], to form a comprehensive multi-scale feature representation. As for tasks requiring strict temporal consistency, causal convolution offers an alternative by ensuring each output depends only on current and past inputs, preventing information leakage from future time steps [50], [57].

Except for the temporal features extracted by Transformer, Conv1D of the existing hybrid CNN- Transformer has also been effectively adapted for capturing additional EEG features, such as the spatio- frequency relationships between EEG channels [59], integrated temporal, spatial, and frequency features within single- channel data [60], and fused spatio- temporal information [15]. Conv1D also facilitates multi- branch fusion within multimodal applications [61], [62]. By leveraging Conv1D's feature extraction capability, self- supervised learning further encodes data into segment embeddings for Transformers to predict missing information across spatiotemporal and Fourier domains [40], and improves EEG representation learning in generative [37] and contrastive [39] frameworks. Building on these strengths, Conv1D plays a critical role in

Foundation Models for large- scale EEG datasets [45], [63], [64], [65].

To capture the spatial relationships between electrodes more efficiently, studies begin to adopt two- dimensional convolution (Conv2D). However, the direct Conv2D may yield limited results due to the inherently low spatial resolution [66]. Therefore, scholars convert EEG into image- like formats first, which are then processed using Conv2D in raw, frequency, and time- frequency domains, with help of techniques of differential entropy (DE) [67], [68], wavelet transforms [70], and power spectral density [69], [70]. Typically, a novel Conv2D based approach of [71] involves integrating temporal, frequency, and time- frequency features into an image- like form for processing. For Conv2D operation, factorized convolution decomposes the square convolutions into two directional operations, which helps to improve computational efficiency. On the basis of such module, some research extracted temporal features alone, with Transformers [72] or graph- based embeddings [73] to capture spatial relationships. Examples of the independently applied factorized convolution for the spatiotemporal feature extraction can be referred from [74], [75], [76], [77], [78], [79], [80], [81]. Parallel factorized convolutions are attempted to extract temporal and spatial features while minimizing interference with original data [15], [82], [83]. Another exemplified study of EEG- Net combines the factorized and depthwise separable convolutions to extract spatio- temporal- frequency features from single- channel 2D EEG data. Some studies also enhance EEG- Net by adding self- attention modules to capture global dependencies [84], [85], and EEG Conformer [86] combined factorized convolutions with self- attention mechanisms.

Additionally, Conv2D also supports multi- resolution analysis, enabling the extraction of features across temporal and spatial scales [87], [88], [89], [90]. For example, ADFCNN in [76] used dual factorized branches to capture multiscale temporal and spatial features, while Ahn et al. [87] integrated multiscale temporal, spatial convolution modules, and Transformer to create spatio- temporal- spectral features for EEG classification.

In summary, benefit from its adaptability and compatibility, Conv2D has become a cornerstone technique of the hybrid CNN- Transformers in handling both raw and transformed EEG data. However, the complexity of EEG's high- dimensional features calls for even more advanced convolution operations for the feature extractor. Hence, three- dimensional convolution (Conv3D) appears to capture spatiotemporal and frequency features [91], [92]. In some cases, EEG data is even represented by four- dimensional formats of first three common dimension being the time dynamics, two- dimensional spatial distribution of electrodes, and another dimension being the channels [93], frequency characteristics over time [42], or the dynamic time- evolving features [94]. These representations, though computationally intensive, enrich Transformer by leveraging EEG's multidimensional features and offer a promising mean.

2) CNN Structure-based Hybrid Model: According to the embedded CNN structure by the hybrid network, the developed architectures can be divided into the linear, multi-branch, residual, and flexible hybrid ones.

Linear Structure: As shown in Fig. 4, linear convolutional pathways often consist of straightforward, sequentially stacked convolutional layers, where each layer depends solely on the output of its predecessor. For EEG, linear structures typically rely on shallow Conv1D and Conv2D layers, optimized first to capture basic temporal and spatial features, and then be fed into Transformer to capture global dependencies of spatiotemporal dimensions [15], [42], [95]. Such integration provides a simple and effective framework for the efficient feature extraction and representation, particularly for tasks requiring minimal architectural complexity. As decoding tasks grow more complex, more advanced CNN architectures emerged.

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/bfb4348ada7eec3db54c1d5911441f49da952133a520df424d5d88dbc1d9b07d.jpg)  
Fig. 4 General CNN-Transformer backbone with linear sequential structure.

Multi- Branch Structure: Multi- branch (parallel) convolution, popularized by GoogLeNet's Inception module, addresses the limitations of single- scale kernels in linear structures. As a result, recent studies have introduced branched structures (e.g., parallel filters of varying sizes) to extract multi- scale features and use Transformer models for the feature fusion. In EEG decoding, to capture temporal features of different frequency bands, multi- branch designs are widely used by the two [44], [76], [89], [90], three [52], [58], [87], [96], four [48], [49], [88], and five [47], [54]. As an example, a five- branch model [54], as illustrated in Fig. 5, isolates EEG bands using distinct filters, which are fused with a Transformer for feature integration.

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/b5b5ee30919c478eaae9af66fb5a438aa0a497b3d3a13418d63a792145409525.jpg)  
Fig. 5 One example of multi-branch structure-based CNN-Transformers.

Beyond above multi- scale extraction, parallel convolutions also capture multi- dimensional features [43], [52], [82], [83], [87]. Some studies arrange sequential factorized convolutions in parallel to combine features from the temporal, spatial, and frequency domains [43], [82]. These designs provide diverse feature inputs, enhancing CNNs' effectiveness and complementing Transformer for robust EEG feature representation.

Residual Structure: Residual structures like ResNet address vanishing gradients that allow inputs to skip directly over one or more convolutional layers, and to learn difference between the input and output. As shown in Fig. 6, the input x is transformed by convolutional layers to produce  $F(x)$ , which is then added to x via a shortcut connection. This structure enables the model to build very deep network architectures without performance degradation. Specifically for EEG decoding, such the module is adapted by hybrid CNN- Transformer to meet the unique demands [43], [48], [50], [51], [97]. For example, [97] employs ResNet- 50 as an encoder to extract high- dimensional local features, which are then optimized by the self- attention module of Transformer, enhancing both feature precision and overall model performance. In summary, the structures mitigate gradient and degradation issues in deep networks, promoting information flow and stability of hybrid CNN- Transformers.

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/1eaf5febee159e7c330fdc650710979436f1c6403c5b119808f7e4dcccb0d1fb.jpg)  
Fig. 6 A Res block that commonly used in hybrid CNN-Transformer.

Flexible Hybrid Architectures: Beyond above sequences, recent studies have expanded to more flexible designs, where the developed model in [74] extracts global temporal features first, refining spatial details with separable convolutions. Study [98] uses asymmetric branches where CNN and Transformer components target different feature scales. A Transformer for EEG and convolutional branch for EOG processing is fused via self- attention in [62] for enhanced multimodal analysis.

To conclude, the different mentioned CNN structures in the hybrids have enhanced the feature integration across temporal, spatial, and frequency domains. Further, to finish the specific tasks, latest hybrid models of Transformer incorporating other advanced architectures are gaining attraction.

# B. GAN-Transformer

Generative adversarial networks (GANs) have found broad applications in data generation, image reconstruction, and domain adaptation, for which the ability to capture long- term dependencies of EEG can be enhanced by combining with Transformers. Sartipi and Cetin proposed a related method for emotional recognition (ER) task in a cross- subject manner [99], where Transformer was integrated with adversarial discrimi- native domain Adaptation (ADDA) to improve EEG spatial features and minimize inter- subject differences. Results on the public DEAP dataset demonstrated the classification improve- ments. Yu et al. [100] designed a Transformer- based GAN to address class imbalance in sleep monitoring, and the results shown its potential for time series data generation tasks. Duan et al. [101] used Transformers to extract features from EEG, and corresponding images were generated using conditional GAN (CGAN). Also in the field of image reconstruction, Zhao et al. [102] introduced a Dual AxistGAN method, which used the self- attention mechanism to replace traditional CNNs and enhance the model's ability to capture long- range EEG dependencies. Combining GANs with Transformers holds great potential in EEG for improving feature extraction, synthetic data generation, and domain adaptation.

# C. Diffusion-Transformer

Also emerging for generative tasks, diffusion models have known for their ability to reconstruct data through iterative denoising, producing stable and high- quality outputs, which has been integrated with the Transformer like DiT [103]. In EEG field, diffusion- Transformer have also enabled significant progress. DiffMDD in [104] combined diffusion models with Transformers to isolate noise- independent information for effective data augmentation. STADMs in [105] improved EEG data quality by reconstructing low- resolution signals into

super- resolution formats, and [106] leveraged a DiT- based approach to generate high- quality synthetic EEG, addressing data scarcity challenges. A Transformer- based EEG encoder was used in a diffusion model in [107] to enable EEG- to- image synthesis, paving the way for novel BCI applications. By integrating diffusion models with Transformers, EEG processing has gained another effective tool for robust data generation, super- resolution, and multimodal synthesis.

# D. GNN-Transformer

Mapping EEG electrodes as nodes is one way to extract the complex EEG spatial features. Graph neural networks (GNNs) can model EEG signals as graphs, capturing the complex interactions between different brain regions to establish spatial features, which aids in a more comprehensive understanding of brain activity. For such innovative research ideas, Sun et al. [108] introduced DBGC- ATFFNet- AFTL (DANet) framework, which combines dual- branch graph convolution, adaptive Transformer feature fusion, and adapter fine- tuning transfer learning. The method significantly improved the cross- subject emotion classification in accuracy and parameter efficiency. Similarly, EmoGT [109], used for ER task, integrates graph convolutional networks (GCN) with Transformer to optimize spatial relationship extraction and sequential information processing. This model serves as a core module for handling multimodal inputs and enhances recognition performance.

Kim et al. [110] proposed the DGTM model, which includes a graph encoding module and a graph transformer module. This model was validated to accurately capture low- dimensional manifolds in EEG and the relationships between feature maps, improving data classification accuracy. Wang et al. [111] developed a model that uses a multi- branch feature extractor to extract temporal, spatial, and frequency features. It employs multi- GCN to learn deep graph structures and utilizes a channel- weighted Transformer for feature fusion. In addition, SAG- CET of [112] comprises a scale- aware adaptive GCN and a cross- EEG Transformer to capture multi- scale features and correlations, while CAW- MASA- STST (C- M- S) proposed in [113] uses multilevel adaptive spectral aggregation (MASA) to capture the dynamic spectrogram of EEG by aggregating the spectral features of different sub- bands through graph convolution. In summary, through representing EEG electrodes as nodes in a graph, the hybrid GNN- Transformer framework facilitates a more comprehensive analysis of brain activity and enhances the extraction of spatial features.

# E. RNN/LSTM-Transformer

RNNs/LSTMs are effective for handling sequential data, while struggling with capturing long- term dependencies. Some studies reduce the receptive field of RNNs/LSTMs, using them as local feature extractors while using Transformers as global one [114]. EEGAlzheimer'sNet developed in [115] combines CNN, RNN, Transformer, and LSTM. Multi- scale dilated CNN and RNN extract spatial and temporal features, while an optimized Transformer- LSTM detects Alzheimer's disease using weights obtained through the EWGLO algorithm. As a result, the developed model attained a satisfactory accuracy of  $96\%$  and the Matthews Correlation Coefficient of  $98\%$

Pham et al. proposed a similar combined CNN- Transformer- LSTM model in [116], and Zhou et al. [117] designed the temporal self- attention regression module in EmoTVR. In TBEEG [118], the relationships between different temporal dimensions were acquired by RNN- Transformer module with ViT combined to process frequency domain information, thus enhancing the model's overall performance. Plus, BAFNet [119] includes an LSTM- Trans model that replaces the linear mapping in the Transformer encoder with LSTM. Despite these advances, given the computational complexity and potential model redundancy, it must overcome the high computation burden and potential efficiency issues in practice of combining RNN/LSTM and Transformer for EEG processing.

# F. SNN-Transformer

Spiking neural networks (SNNs) mimic signal transmission mechanism of biological neurons [120], providing new insights into brain computational mechanisms. Study [121] proposed the Spiking Conformer model, with the spiking convolution module processes information in both temporal and spatial dimensions, and spiking Transformer encoder utilizes spiking self- attention to capture the signal's overall characteristics. There have also attempts to combine SNN with Transformer [122], which inherits the dynamic response characteristics of biological neurons and incorporates the global information processing advantages of Transformer.

# G. Capsule Network-Transformer

Capsule network is a network architecture initially designed for processing image data and aims to address challenges faced by traditional CNNs [123]. In EEG analysis, capsule networks capture the complex spatial relationships of EEG features with limited data. TC- Net [124] encodes features with capsules and captures spatial relationships between multiple EEG features by a dynamic routing mechanism, while integrating the Transformer module to extract global information. MESCTNet proposed in [125] further enhances spatial attention, improving emotion state recognition accuracy through multidimensional feature input and various attention mechanisms. However, the application of capsule networks in EEG decoding tasks is limited by their complex structure, high computational cost, and need for improved robustness.

# IV. CUSTOMIZED TRANSFORMERS

While the hybrid approaches are well- established, ongoing research continues to explore more effective architectures. This section gives customized Multi/modified- encoder architectures, Pyramid, and Reconstructed Transformers.

# A. Multi Encoders

The single encoder architecture often struggles to capture the complex features in EEG signals. To address this, researchers have developed multi- encoder architectures, where multiple Transformer encoders, as illustrated in Fig. 7, process different feature branches in parallel, followed by a fusion

mechanism. Based on the fusion approach, such the mentioned model can be divided into two categories.

1) Basic Multi-Encoders: With straightforward fusion means of concatenation, weighted fusion, and attention mechanisms, the structures integrate the EEG features from diverse data sources or modalities of varied types, such as time-frequency and spatio-temporal characteristics from raw EEG [126], multi-scale temporal [39], [56], spatial features [90], different data augmentation features [127], EEG and language features [128], transformed features from PSD [56], wavelet analysis [69], [129], and linear-nonlinear features [130]. Cross-modal features from datasets can also being integrated, such as EEG with EOG data [131], [132], various types of sleep data [61], and audio spectrogram [44].

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/47601508ee2e22e944004296674a9ba70f68f49f7c883350e10098d3492c13e4.jpg)  
Fig. 7 General multi-encoder backbone structure of EEG decoding.

2) TNT & TNT-like Model Application: Theoretically, TNT [133] extends the ViT with a dual-layer structure that captures both micro and macro image features. Similarly, the TNT-like model used for EEG decoding has been depicted in Fig. 8, where multiple Transformer encoders are used to process local features and one global encoder integrates global features, thus making the model more effective (see examples of HSLT [134], SleepTransformer [135], VSTTN [136], EEG2Text [137]). Kim et al. proposed SleepTNT [138], which captures both local features within different time periods and global features across the time periods, whereas the latter SleepConvTNT in [139] further introduced a one-dimensional convolution Transformer module for automatic sleep staging.

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/4c8893d8e2baf3f5188e10a520ee7c45de45f10505460430d4642f644b46cafa.jpg)  
Fig. 8 General TNT-like backbone structure of EEG decoding.

Influenced by [135], similar models of SPDTransNet [140], hierarchical Transformer [141], and others [142] have been proposed recently. Among them, the hierarchical Transformer consists of a high- level transformer (HLT) and a low- level transformer (LLT). The LLT extracts information within short time intervals, while the HLT captures global information across time intervals. MultiChannelSleep- Net in [143] uses an encoder to extract information within time- frequency data converted from single- channel EEG. A unique TNT structure of B2- ViT captures generalized spatio- temporal correlations across encoder layers via a breadth Transformer layer [144].

Although these above multi encoder structures enhance EEG decoding via parallel processing and fusion, currently, their computational cost is still being relatively high. Hence, an idea to adjust the intrinsic encoder structure for customized Trans- formers, which we called the modified encoders has aroused.

# B. Modified Encoders

A generalized framework of MetaFormer from prior research [145] redefines the multi- head attention as a "Token Mixer", emphasizing that the effectiveness of Transformers lies in their structural design rather than specific mechanisms. Expanding on this principle, as shown in Fig. 9, current studies with four modified encoder structures have been concluded as follows.

1) Token Mixer-based Encoders: Such implementations generally fall into two categories, where the first focuses on enhancing the Multi-Head Attention by introducing new mechanisms to improve performance or replacing MHA with alternative modules to reduce computational and memory demands, particularly for long-sequence EEG. Examples are Retention [146], Sparse Attention [147], [148], One-Way Self Attention [149], Linear Self Attention [150], 2D AvgPoolinging [151], and Spiking Self Attention [121]. For these methods, techniques of low-rank approximations, sparse connections, directional constraints, and biologically inspired designs have been leveraged. Composite attention of recent studies also enhances feature representation by integrating multiple attention mechanisms. Study [152] developed shifted channel attention for the global and local feature extraction. ESSleep [153] stacks two encoders to extract features at epoch and sequence levels, and [154] adopts a parallel approach, running two encoders simultaneously for spatiotemporal feature extraction. Moreover, diverse attention types within a single encoder has been attempted in [94], [155]. Typically, [156] combines intra- and inter-MHA for multi-scale channel dependencies. These mechanisms effectively capturing EEG's complex multidimensional dependencies.

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/1d4db1fedc43ec9767ff40e5ff0e8d2748392f2c713e87048ffbad51668f7d62.jpg)  
Fig. 9 Evolution of modified encoder frameworks based on MetaFormer, proposed in this work, to enhance adaptability for EEG decoding tasks. From the left to right are structures of the Token Mixer, Token Enhancer, Extended MetaFormer (EM), and EMA frameworks (ie., EM with Affiliate Modules).

The second category, on the other hand, stresses multimodal and multi- scale feature extraction. Specifically, features at different scales are targeted by Channel Attention (CA) [157] and Shifted Channel Attention [152], and mechanisms like TcT Attention [158] and TSA/SSA [159] specialize in extracting spatio- temporal relationships. In multi- modal scenarios, TACO Cross Attention supports seamless integration of diverse data modalities [160]. These Token Mixer based encoders has effectively enhance the flexibility of Transformers, enabling robust processing of long- sequence and multi- modal EEG data.

2) Token Enhancer-based Encoders: Compared to MHA, the multilayer perceptron (MLP) is responsible for feature expan-sion and transformation, and can be adapted to better capture multi-scale patterns in EEG data. A multi-branch feedforward network (FFN) structure processes wavelet features across frequency bands, facilitating knowledge transfer between raw and transformed EEG data [161]. Similarly, the Convolutional Feature Expansion (CFE) module substitutes MLP with multi-scale convolutions, extracting complex temporal and spatial dependencies [89]. These enhancements to the Token Enhancer can improve EEG decoding performance while preserving the core self-attention mechanism.

3) Extended MetaFormer (EM)-based Encoders: Such the customized framework simultaneously replaces both the MHA and MLP, offering greater flexibility for EEG decoding. More specifically, MHA adaptations primarily aim to enhance the computational efficiency and scalability, such as the Separable MHA [55], Prob-Sparse Attention [162], and Squeezed MSA [163], [164], which utilize sparse connections and low-rank approximations. CNN-based Token Mixers [165], as employed in SSVEPFormer, enable channel-specific feature extraction, showcasing adaptability for EEG tasks. The MLP substitutions focus on improving feature extraction and reducing computa-tional costs. Convolution-based FFNs of BFFN [55], RFFN [163], [164], are common approaches. Besides, Multibranch expert structures [162] and Channel MLP [165] further enhance multi-scale and inter-channel feature learning.

4) EM with Affiliate Modules (EMA): Affiliate Modules are additional components integrated into Transformer structure to expand its functionality, including gating mechanisms [166], positional feedforward mechanisms [167]. Convolutions remain a key support module, for which causal convolution accelerates time-series processing [50], [57], and standard convolution extracts local feature relationships [55], [77], [164], [167]. Some modules also combine convolution with pooling layers [77] to form distillation modules [147] that preserve key features. Together, Affiliate Modules within EM architecture enhance the decoding adaptability and flexibility.

Some representative examples fall in above categories are concluded in Table A- I (see supplements), with tasks, model characteristics, and compared evaluations results in detail.

# C. Pyramid Transformer

![](https://cdn-mineru.openxlab.org.cn/extract/543ef323-37a1-4297-87b2-841ffa5ec812/831b2c8aef84c9967417308722f8207e78d7cbad2957539ccead570dda70519d.jpg)  
Fig. 10 General structure of Pyramid Transformer for EEG decoding.

To reduce computational complexity while ensuring the high performance, pyramid Transformer was inspired by CNN feature pyramids to build a hierarchical framework by applying different techniques for down- sampling and up- sampling the activations within the Transformer (see Fig. 10). The structures are showcasing their potential for advanced EEG processing tasks, and can be further categorized in two classes below.

1) PVT-like Structure: Pyramid Vision Transformer (PVT) [168] employs spatial reduction attention to efficiently handle large-scale inputs by combining reduced resolutions with higher feature dimensions. PVT-like Transformers mainly process diverse input formats, such as raw signals [77], [163], pseudo-images [152], etc. Specifically, ScatterFormer [169] uses frequency-aware attention to extract features across frequency layers from wavelet-transformed data. JDAT [170] integrates spatial-spectral-projection blocks with transformer encoders, enabling automatic multi-dimensional focusing in multispectral images. These examples demonstrate the adaptability of PVT-like architectures in EEG decoding.

2) Swin Transformer Structure: Building on PVT, the Swin Transformer [171] uses shifted window attention, staggering local windows across layers to expand the receptive field while optimizing computational efficiency. For EEG processing, a channel attention mechanism has been incorporated to extract intrinsic correlations between channels [172], while [173] constructed four-dimensional data features at the preprocessing phase and integrated spatiotemporal attention modules into Swin Transformer backbone. In conclusion, these Swin-based models represent a significant advancement in Transformer, demonstrating their ability to effectively manage the complex spatiotemporal relationships inherent in long-sequence EEG.

# D. Reconstructed Transformer Architecture

Some models mimic the Transformer structure and undergo reconstruction to extract specific EEG features. Table III lists the related studies. More specifically, MSDTTs [70] combine multi- domain spatial branch with dynamic temporal branch of Transformers to extract and weight corresponding features. Multimodal models of [96] utilize a multi- scale temporal asymmetric learning module to learn brain characteristics from raw signals, and use another branch to extract facial emotion images related to EEG via a multi- view attention- enhanced convolutional learning module. Further, a cross- attention Transformer is applied to fuse multi- modal information, and extract EEG temporal- spectral- spatial features. GAT [83] uses self- attention mechanisms and parallel convolution layers to capture spatiotemporal information, primarily designed for EEG decoding. Similarly, models like TSformer- SA [174] adopt domain adaptation strategies to handle multimodal data.

ECO- FET [175] is another multimodal model that enhances learning from eye movement data by combining functional brain connectivity with spectral- spatial- temporal domains. Studies [176], [177], also use EOG- EEG data. BNMTrans [178] extracts features from sequential brain networks using a self- attention mechanism and leverages geometric correlations within the Riemannian manifold to guide feature extraction, optimizing cognitive impairment monitoring. TSP [58] uses spatial masking autoencoders and temporal contrastive predictive coding to extract diverse information from EEG. BIOT in [150] focuses on converting different biological signals into a unified representation. [53] combines a residual variational architecture with a slice mode attention mechanism to construct Transformer- based encoder- decoder models to remove EEG

TABLE IOVERVIEW OF RECONSTRUCTED ARCHITECTURE TRANSFORMER [BD: BRAIN DISORDERS; SP: SEIZURE PREDICTION; SSC: SLEEP STAGECLASSIFICATION; ACC: ACCURACY; RRMSE: RELATIVE ROOT MEAN SQUARED ERROR; CC: CORRELATION COEFFICIENT;  $\kappa$  COHEN KAPPA; AUC: AREA UNDER THE CURVE; SEN: SENSITIVITY; MF1: MACRO-AVERAGE F1-SCORE; TPR: TRUE POSITIVE RATE; FPR: FALSE PREDICTION RATE; CID: COGNITIVE IMPAIRMENT DETECTION]  

<table><tr><td>Year</td><td>Models</td><td>Ref</td><td>Tasks</td><td>Model Characteristics</td><td>Structure</td><td>Dataset</td><td>Evaluation</td></tr><tr><td rowspan="3">MSDTT</td><td rowspan="3">2022</td><td rowspan="3">[70]</td><td rowspan="3">ER</td><td rowspan="3">Extract spatial features and temporal features simultaneously</td><td rowspan="3">Multi-spatial Transformer &amp;amp; Dynamic Temporal Transformer</td><td>SEED</td><td>Acc 97.52%</td></tr><tr><td>SEED-IV</td><td>Acc 96.70%</td></tr><tr><td>DEAP</td><td>Acc 98.91%</td></tr><tr><td rowspan="2">Denosiefor rmer</td><td rowspan="2">2023</td><td rowspan="2">[53]</td><td rowspan="2">Seq2seq</td><td rowspan="2">Excellent local and long-term feature representation capabilities</td><td rowspan="2">Residual Information Enhanced Encoder-Decoder Structure</td><td>EEG-EGO</td><td>REMAR 0.278, CC 0.951</td></tr><tr><td>EEG-EMG</td><td>REMSE 0.389, CC 0.894</td></tr><tr><td rowspan="2">GAT</td><td rowspan="2">2023</td><td rowspan="2">[83]</td><td rowspan="2">MI</td><td rowspan="2">Cross Subject, Domain Adaptation</td><td rowspan="2">Global Adapter, Temporal Attention</td><td>BCI-IV 2a</td><td>Acc 76.58%, κ 0.6877</td></tr><tr><td>BCI-IV 2b</td><td>Acc 84.44%, κ 0.6889</td></tr><tr><td>M-d-C</td><td>2023</td><td>[111]</td><td>BD(SP)</td><td>Graph Conv, Multi-Branch/View</td><td>MR-Feature Extractor, dMGCN, CWTFFNet</td><td>CHP-MET</td><td>AUG 93.5%, SEN 97.8%, FPR 0.059</td></tr><tr><td rowspan="4">C-M-S</td><td rowspan="4">2023</td><td rowspan="4">[113]</td><td rowspan="4">EEG Visual</td><td rowspan="4">Dual-Branch, GCN</td><td rowspan="4">CAW, MASA, STST</td><td>Xunwu PD</td><td>AUC 98.4%, SEN 100%, FPR 0.079</td></tr><tr><td>EEG72</td><td>Acc 54.82%/29.98% of 6/72-class</td></tr><tr><td>EEG200</td><td>Acc 66.48%/31.37% of 2/10-class</td></tr><tr><td>CHB-MIT</td><td>Acc 66.40%, AUC-PR 0.26, AUROC 0.86</td></tr><tr><td rowspan="5">BIOT</td><td rowspan="5">2023</td><td rowspan="5">[150]</td><td rowspan="5">Seq2seq</td><td rowspan="5">Bio-signal to “bio-signal sentences”</td><td rowspan="5">Bio-signal Tokenization, Linear Transformer Encoding</td><td>IIC Seizure</td><td>Acc 57.62%, κ 0.4932, Weighted F1 0.577</td></tr><tr><td>TUAB</td><td>Acc 79.25%, AUC-PR 0.87, AUROC 0.87</td></tr><tr><td>TTUE</td><td>Acc 48.82%, κ 0.4482, Weighted F1 0.709</td></tr><tr><td>PTB-XL</td><td>Acc 83.15%, AUC-PR 0.90, AUROC 0.75</td></tr><tr><td>HAR</td><td>Acc 94.61%, κ 0.9351, Weighted F1 0.946</td></tr><tr><td rowspan="4">C-T-D</td><td rowspan="4">2024</td><td rowspan="4">[52]</td><td rowspan="4">RSVP</td><td rowspan="4">Domain-Rectified framework</td><td rowspan="4">CST, TVA, DRTL frame</td><td>Tsinghua</td><td>Acc 92.56%, TPR 89.62%, AUC 0.9415,</td></tr><tr><td>RSVP</td><td>Acc 72.02%, TPR 75.13%, AUC 0.7281</td></tr><tr><td>PhysioNet</td><td>Acc 92.56%, TPR 89.62%, AUC 0.9415,</td></tr><tr><td>RSVP</td><td>Acc 72.02%, TPR 75.13%, AUC 0.7281</td></tr><tr><td rowspan="2">TSP</td><td rowspan="2">2024</td><td rowspan="2">[58]</td><td rowspan="2">ER</td><td rowspan="2">Temporal &amp;amp; Spatial prediction</td><td rowspan="2">Spatial Masked Autoencoder &amp;amp; BIOT</td><td>SEED</td><td>Acc 66.53%, κ 0.4498</td></tr><tr><td>SEED-IV</td><td>Acc 46.40%, κ 0.2737</td></tr><tr><td rowspan="2">-</td><td rowspan="2">2024</td><td rowspan="2">[96]</td><td rowspan="2">ER</td><td rowspan="2">Dual-Branch, Multi-modal</td><td rowspan="2">Multi-scale TASLM, Multi-view</td><td>TUEV</td><td>Acc 53.37%, κ 0.5261</td></tr><tr><td>SEED</td><td>Acc 89.21%,</td></tr><tr><td rowspan="4">Tsformer-SA</td><td rowspan="4">2024</td><td rowspan="4">[174]</td><td rowspan="4">RSVP</td><td rowspan="4">Multi-view feature fusion</td><td>Cross-view in ECLM</td><td>DEAD</td><td>Acc 93.56%, 93.53%, 93.53%</td></tr><tr><td rowspan="3">Cross-view interaction module, attention-based fusion module, subject-specific adapter</td><td>PD Task 1</td><td>Acc 90.22%, TPR 89.21%, FPR 8.64%</td></tr><tr><td>PD Task 2</td><td>Acc 88.42%, TPR 87.24%, FPR 10.39%</td></tr><tr><td>PD Task 3</td><td>Acc 90.20%, TPR 89.36%, FPR 8.97%</td></tr><tr><td rowspan="2">ECO-FET</td><td rowspan="2">2024</td><td rowspan="2">[175]</td><td rowspan="2">ER</td><td rowspan="2">Multi-modal, critical subnetwork Selection SSL</td><td rowspan="2">Functional Emotion Transformer, functional subnetwork selection, functional brain connectivity</td><td>SEED</td><td>Acc 93.66%</td></tr><tr><td>SEED-IV</td><td>Acc 87.79%</td></tr><tr><td>MCAF-T</td><td>2024</td><td>[176]</td><td>ER</td><td>Multi-modal</td><td>Temporal Transformer, Channel-wise Attention</td><td>SEED</td><td>Acc 89.21%,</td></tr><tr><td rowspan="2">CoRe-Sleep</td><td rowspan="2">2024</td><td rowspan="2">[177]</td><td rowspan="2">SSC</td><td rowspan="2">Multi-modal</td><td rowspan="2">EEG/EOG Encoder, Multimodal EEG/EOG Encoder</td><td>DEAD</td><td>Acc 93.56%, 93.53%,</td></tr><tr><td>SHHS</td><td>Acc 84.0%, κ 0.771, MF1 73.0%</td></tr><tr><td>BMMTrans</td><td>2024</td><td>[178]</td><td>CID</td><td>Dynamic brain network structures in Riemannian space, integrating tangent mapping with self-attention</td><td>Brain Network sequence-driven Manifold-based Transformer</td><td>PD</td><td>Acc 90%, AUROC 0.91, Precision 0.87, Recall 0.92, F1 0.89</td></tr></table>

artifacts. Besides, C- M- S [113] decodes semantic information of visual stimuli from a single EEG signal, and significantly improves decoding accuracy.

To summary, by introducing specific changes to the original Transformer architecture, such as subtle structural adjustments of encoders, or novel modifications to overall networks, the model's ability to capture temporal, spatial, and spectral features has been progressively enhanced.

# V.DISCUSSION

The Transformer architecture has garnered significant interest for its exceptional ability to handle sequential data, for which related models can not only be seamlessly integrated into other deep learning networks but can also be customized with various adaptations in EEG decoding. For these Transformer- based networks, we have released detailed summarization of current available open- source EEG datasets in Table B- I (see in supplementary materials), which includes the data size, tasks, annotations, number of subjects, and related studies. Among them, datasets with different data size have been attempted. For those data- scarce situations, extra auxiliary data are integrated and unsupervised or selfsupervised learning methods are used by leveraging unlabeled data [179]. Moreover, current Transformer- based approaches are nearly all essentially devoted to better synergize the temporal, spatial, and frequency features of EEG.We conclude the specific extracted features of models in supplemental Table CI, from which we summarize some particular strengths of each model. Specifically, since EEG signal possesses good temporal properties, the relevant temporal features have been exploited by most of current studies, except for [15], [25], [28], [34], [68], [148], [165], [169]. Meanwhile, various Transformerbased networks are concentrating on the complex spatial relationships among multiple channels of electrodes. Interestingly, current Trans- formers, no matter in which structures, have limited applications to separately dig into frequency domain features. Instead, most studies converted EEG into time- frequency data or combine them with other features for processing. In particular, the temporal- spatial- spectral representations of EEG have been simultaneously investigated by categorized models of backbone [17], hybrid models (with CNN [40], [85], GAN [101], RNN [118], CapsNet [123], [124]), and customized ones (e.g., multi- Encoder [126], [143], modified Encoder [77], [157], Pyramid [170], [172], reconstructed [52], [83]).

Overall, most of these mentioned Transformer- based models integrate EEG patterns from multi- sources and multi- features well, which enhances the feature extraction capability and

TABLE II PERFORMANCE COMPARISON BETWEEN MODELS ON DIFFERENT TASKS AND DATASETS [STD: STANDARD DEVIATION; SPEC: SPECIFICITY; PR: PRECISION]  

<table><tr><td>Task</td><td>Dataset</td><td>Model</td><td>Model Category</td><td>Performance</td></tr><tr><td rowspan="5">Emotional Recognition</td><td rowspan="5">SEED</td><td>DGCNN</td><td>CNN</td><td>Acc ± Std: 90.40 ± 8.49%</td></tr><tr><td>AMDET[19]</td><td>CNN-Transformer</td><td>Acc ± Std: 97.17±0.93 %</td></tr><tr><td>ACTNN[42]</td><td>CNN-Transformer</td><td>Acc ± Std: 98.47±1.72 %</td></tr><tr><td>DANet[108]</td><td>GNN-Transformer</td><td>Acc ± Std: 98.47±1.72 %</td></tr><tr><td>MV-SSTMA[57]</td><td>Modified Encoders</td><td>Acc ± Std: 98.47±1.72 %</td></tr><tr><td rowspan="6">MI Classification</td><td rowspan="6">BCI IV 2a</td><td>EEGNet</td><td>CNN</td><td>Acc: 74.50 %, κ: 0.6600</td></tr><tr><td>EEG Conformer[86]</td><td>CNN-Transformer</td><td>Acc: 74.50 %, κ: 0.6600</td></tr><tr><td>ADFCNN[76]</td><td>CNN-Transformer</td><td>Acc: 79.39%</td></tr><tr><td>MI-CAT[78]</td><td>Modified Encoders</td><td>Acc: 76.74, % κ: 0.6900</td></tr><tr><td>GAT[83]</td><td>Reconstructed</td><td>Acc: 74.50, % κ: 0.6877</td></tr><tr><td>Dual-TSST[81]</td><td>CNN-Transformer</td><td>Acc: 80.67, % κ: 0.7413</td></tr><tr><td rowspan="6">Sleep Stage Classification</td><td rowspan="6">Sleep-EDFx</td><td>DeepSleepNet</td><td>CNN</td><td>Acc: 77.80 %, Macro F1 score: 0.7180, κ: 0.7000</td></tr><tr><td>AttnSleep[50]</td><td>CNN-Transformer</td><td>Acc: 84.20 %, Macro F1 score: 0.7530, κ: 0.7800</td></tr><tr><td>ResTrans[43]</td><td>CNN-Transformer</td><td>Acc: 80.70 %, Macro F1 score: 0.7410, κ: 0.7400</td></tr><tr><td>TSA-Net[51]</td><td>CNN-Transformer</td><td>Acc: 82.21 %, Macro F1 Score: 0.7351, κ: 0.7501</td></tr><tr><td>SleepTrans[135]</td><td>Reconstructed</td><td>Acc: 84.9 %, Macro F1 score: 0.7880, κ: 0.8280, SPEC: 95.9 %</td></tr><tr><td>FoME[65]</td><td>Foundation Models</td><td>Acc: 99.03 %, Macro F1 score: 0.9910, PR: 08.68%, AUC: 98.68%, SENS: 98.73%, F1 score: 0.9910</td></tr><tr><td rowspan="6">Brain Disorders</td><td rowspan="6">CHB-MIT</td><td>CNN</td><td>CNN</td><td>SENS: 81.20%, FPR: 0.16h</td></tr><tr><td>TTT[17]</td><td>Transformer</td><td>SPEC: 96.23 %, SENS: 96.01%, FPR: 0.047/h</td></tr><tr><td>MdC[111]</td><td>GNN-Transformer</td><td>Acc: 79.39%</td></tr><tr><td>HviT-DUL[55]</td><td>Modified Encoders</td><td>AUC: 93.5 %, SENS: 97.80 %, FPR: 0.059/h</td></tr><tr><td>B2-ViT[144]</td><td>Multi-Encoders</td><td>PR: 92.30 %, SENS: 93.33%, FPR: 0.057/h</td></tr><tr><td>TGCNN[164]</td><td>Modified Encoders</td><td>AUC: 93.5 %, SENS: 91.50 %, FPR: 0.145/h</td></tr></table>

exhibits certain robustness. However, the scarcity of available data, the limitations of generalization ability, computational resources challenges, and weak interpretability continue to constrain their wide application. Here we specifically discuss these challenges and possible directions for future research breakthroughs.

# A. Challenges of the Current Models

1) The Scarcity of Available Data: The first challenge lies in the data deficiency for effective training. Compared to traditional machine learning methods and simple convolutional neural networks, although Transformer-based models have unique advantages in handling sequence tasks, they typically require large amounts of training data. However, compared to the image or text required by foundation models, the collection of brain signals is rather more difficult [180]. Despite the use of non-invasive means can avoid surgical risks, the acquisition of large applicable datasets has been a major challenge in the field of EEG decoding. The main reasons are as follows. To begin with, some existing datasets are not publicly available, creating difficulties in data sharing of the field. Second, the collection of EEG data requires the relevant ethical approval, the recruitment of users/subjects, and obtaining their consent forms, which is a time-consuming and laborious task. In particular, some EEG paradigms also require extensive training or stimulus assistance, during which users may interrupt the experiments due to discomfort. Besides, variations in sampling devices, users' state, environment, time of day, and other factors may make EEG data unsuitable for the given decoding task. All these above pose additional challenges of data preparation. Additionally, the annotation of EEG also influences the datasets quality. For instance, the annotation of some emotion recognition task needs the combination of self-assessments with scales and physiological data. Clinical annotations, such as in sleep monitoring, are expert- driven, time- intensive, and prone to subjectivity. Thus, although there are some automatic annotation software, the biases among experts in the same task (e.g., sleep staging) undermines dataset reliability.

2) The Limitation of Generalization Ability: Despite the flexibility and adaptability of Transformer models allow them to be applied in a variety of ways, it is not yet possible to provide one-size-fits-all solutions to each specific EEG-related issue. As reported in Table III, and Table A-I, many existing models are designed for specific tasks and validated on limited datasets, which restricts their ability to generalize across different tasks.

More specifically, the performance comparisons of specific models, with benchmark of CNNs, are concluded in Table II, and we can summarize that current Transformer- based models exhibit some task preferences, although not particularly pronounced. Overall, models have demonstrated robust generalization capabilities in tasks of emotion recognition on SEED dataset, motor imagery classification on BCIIV 2a dataset, sleep stage classification on Sleep- EDFx dataset, brain disorders diagnosis on CHB- MIT dataset, etc. These findings provide preliminary evidence of their ability to generalize across datasets and tasks. Nevertheless, further systematic evaluations across a broader range of datasets and experimental conditions are needed to substantiate these observations. Meanwhile, from Table II, we see that CNN- Transformer performs well in above mentioned task and datasets, while multi- Encoder architectures, such as TNT, are frequently applied to SSC tasks due to their effectiveness in capturing temporal dependencies [135]. Combined with Section III. B, GNN- Transformer models are more commonly applied in emotion recognition tasks owing to their ability to model inter- channel relationships [108], [111]. In fact, from Section III, we also see that generative models, such as GANs and

diffusion models, are able to improve data generation and augmentation, mainly addressing challenges in those low- data scenarios. As concluded from Table I and Table A- I, various customized Transformer- based architectures are favorable in emotion recognition, classification, and brain disorders diagnosis tasks. These observed trends highlight some model- task alignments, while without implying rigid generalizations, as EEG decoding tasks often allow for considerable flexibility in the model selection.

Moreover, for the model- dataset matching relationship, we conclude, from the previous tables, that many architectures exhibit similarly strong performance on the same dataset, making it difficult to attribute differences to structural features alone. Indeed, the model performance of the designated dataset is largely driven by the specific optimization strategies and task requirements. One should note that the variability in EEG signal patterns often forces decoding research to focus on single task, resulting in models that are specifically designed and validated for narrow tasks on limited datasets. Most models still lack extensive experimental validation for crosssubject generalization [83], [99], [108], and relevant research on task generalization remains limited.

3) Computational Resources Challenges: The architecture of transformers and their extensive parameters necessitate signi-ficant computational resources. To evaluate the computational complexity of a designed Transformer model, the practical metric of the parameter count, reflecting memory needs and hardware compatibility, has been selected. Several representa-tive data with the explicit record has been reported in supplementary Table D-I. As a result, the Transformer-based decoding networks of EEG has covered different parameter scales of range from K to B. Overall, it is evident that these advanced models exhibit higher parameter counts compared to traditional CNN models (e.g., EEGNet that with mere 1K parameters), implying a more complicated structure. One obvious conclusion is verified that some hybrid models, which belongs to foundation models, improve task generalization but increase computational costs with much larger parameters [64], [65]. Apart from this, several CNN-Transformer models balance parameter count by combining shallow CNNs with Transformers [41], [87]. Indeed, due to different effecting mechanisms, even models of the same type, such as hybrid models or customized Transformer models, have significantly varying parameters. For instance, some customized models [89], [144], [173] demand more parameters for higher expressiveness, while another customized model [174] remain compact and effective (i.e.,  $4.86\mathrm{K}$  for focused task-specific applications. In any case, the quest for high-performance models with fewer parameters is always present here.

Beside the parameter size, the computational efficiency of a Transformer model is also coupled with the task requirements and the hardware environment. However, a direct comparison of it across the interested models remains challenging as most studies do not report key metrics, such as the FLOPs, the detailed runtime benchmarks. Thus, we encourage future research to report these additional performance metrics to provide a more comprehensive evaluation.

4) Interpretability Challenges: There is a need for

conti- nuous exploration of the model's interpretability. In medical and clinical applications, models with strong interpretability can provide transparency in their decision- making processes, allowing healthcare professionals to verify and understand the outcomes of the models. However, Transformerbased models, despite their excellent performance in handling complex data through numerous parameters and deep structures, often exhibit a high level of abstraction in interpretability, making it difficult to track and explain their decisions [181].

# B.Future Direction

The above limitations can be generally categorized as challenges specific to the EEG analysis and those inherent to the Transformer models, which points to two general lines of inquiry for future research. Specifically, from the perspective of Transformer model aspects, the model scaling can be further explored for model parameter compression. Brain Foundation Models [180] trained on extensive EEG datasets and finetuned for specific tasks are in their infancy. Therefore, due to the significant demands for computational resources and data scale, small models remain vital in resource- constrained environ- ments. To realize rapid deployment in specific tasks, it is crucial to develop models with fewer parameters and smaller scales [182]. Customization of the transformer model is also necessary to enhance the training efficiency while reducing computational costs. Moreover, knowledge distillation can be attempted to reduce model size while maintaining performance. Second, the fusion of Transformer with other methods should be investigated to benefit each other. An interesting fact is that there are rare attempts to integrate Transformer with traditional machine learning methods [183]. Although such models may require more experimental validation and optimization, the clear implications of their relevant features deserve further attention to enrich the domain- specific knowledge. Current research on related models is mostly concentrated on the integration with other deep learning models, particularly CNNs, RNNs, and GANs. The combination with emerging models such as GNNs and SNNs, or the development of new Transformer architectures based on these models represents a promising research direction.

On the other hand, for the EEG signal analysis, future studies could be conducted in the following aspects. First, to avoid the overfitting risk caused by limited data, the dataset expansion is needed. In particular, standardized open- source large- scale datasets, such as the BCI competition dataset, are essential to addressing current Transformer- based model challenges and require special research attention. Large language models and diffusion models have made significant progress in generating text and images. Thus, one can plan to utilize these models to generate EEG data to augment existing datasets. By integrating diverse datasets, Brain Foundation Models can effectively extract universal EEG features, enhancing model robustness and cross- task adaptability. Moreover, tools like Brain Decoder, MOABB, and TorchEEG streamline data preparation and model deployment, addressing the critical need for accessible and widely applied standard datasets.

Second, the cross- domain transfer learning should be further emphasized. Applying models pre- trained on text, speech data, and other similar bio- signals to EEG datasets, followed by a fine- tuning, will be a promising method to effectively compensate for the scarcity of EEG training data [30], [137]. Third, the multi- modal learning can be further extended. Current studies primarily focuses on the combination of EEG with other biosignals, such as EOG [131], [132], EMG [94], and electrocardiography (ECG) [160]. In the future, one could explore joint learning of EEG with images, text, or other data to gain a more comprehensive understanding of the characteristics under EEG signal variations. Especially, inspired by GPT and LLaMA, future BFMs may support joint multimodal modeling [184], with advancements in alignment and efficiency paving the way for broader applications in neuroscience.

To accelerate the real- time application of the model, several coupled issues should also be investigated, such as the effect of common EEG noise on designed Transformers, the unified optimization of all hyper- parameters. The real- time processing ability to ongoing EEG is also pretty important and relevant hardware acceleration can be studied. In conclusion, future research should focus on improving the models' computational efficiency, accuracy, and generalization capabilities to better meet the demands of complex EEG analysis.

# VI. CONCLUSION

This survey aims to classify and analyze the progress of Transformer- based models in EEG decoding, which adopts a distinguished review methodology that evolves with the model. We classify the related models based on their structural features, specifically including direct backbone models, hybrid models with other DL methods, and customized Transformers. Our analysis focuses on assessing the performance of various Transformer models in processing EEG data, striving to provide the academic community with a comprehensive perspective to foster the development of new technologies. Additionally, it reveals research trends and potential future developments in the field of EEG processing.

# REFERENCES

[1] S. M. Alarcão and M. J. Fonseca, "Emotions recognition using EEG signals: a survey," IEEE Trans. Affect. Comput., vol. 10, no. 3, pp. 374- 393, Jun. 2017. [2] H. Y. Zhu, et al., "A human- centric metaverse enabled by brain- computer interface: a survey," IEEE Commun. Surv. Tutor., vol. 26, no. 3, pp. 2120- 2145, 2024. [3] H. Li, X. Li and J. d. R. Millán, "Noninvasive EEG- based intelligent mobile robots: a systematic review," IEEE Trans. Autom. Sci. Eng., vol. 22, pp. 6291- 6315, 2025. [4] A. Vaswani et al., "Attention is all you need," in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5998- 6008. [5] T. Lin et al., "A survey of transformers," AI Open, vol. 3, pp. 111- 132, Jan. 2022. [6] G. Krishna et al., "EEG based continuous speech recognition using Transformers," 2019, arXiv:2001.00501. [7] J. Pedoeem et al., "TABS: Transformer based seizure detection," in Proc. IEEE Signal Process. Med. Biol. Symp., Jul. 2020, pp. 1- 6. [8] J. Sun, J. Xie and H. Zhou, "EEG classification with transformer- based models," in Proc. IEEE 3rd Global Conf. Life Sci. Technol., Mar. 2021, pp. 92- 93. [9] C. Chen et al., "Understanding the brain with attention: A survey of Transformers in brain science," Brain- X, vol. 1, no. 3, p. e29, 2023.

[10] B. Abibullaev, A. Keutayeva and A. Zollanvari, "Deep learning in EEG- based BCIs: A comprehensive review of transformer models advantages challenges and applications," IEEE Access, vol. 11, pp. 127271- 127301, 2023. [11] A. Keutayeva and B. Abibullaev, "Data constraints and performance optimization for transformer- based models in EEG- based brain- computer interfaces: A survey," IEEE Access, vol. 12, pp. 62628- 62647, 2024. [12] S. Islam et al., "A comprehensive survey on applications of transformers for deep learning tasks," Exp. Syst. Appl., vol. 241, May 2024, Art. no. 122666. [13] Y. Du et al., "EEG temporal- spatial transformer for person identification," Sci. Rep., vol. 12, no. 1, Aug. 2022, Art. no. 14378. [14] M. Zeynali, H. Seyedarabi and R. Afrouzian, "Classification of EEG signals using Transformer based deep learning and ensemble models," Biomed. Signal Process. Control, vol. 86, Sep. 2023, Art. no. 105130. [15] J. Xie et al., "A transformer- based approach combining deep learning network and spatial- temporal information for raw EEG classification," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 30, pp. 2126- 2136, 2022. [16] X. Du et al., "MMPosE: Movie- induced multi- label positive emotion classification through EEG signals," IEEE Trans. Affect. Comput., vol. 14, no. 4, pp. 2925- 2938, 2023. [17] J. Yan et al., "Seizure prediction based on transformer using scalp electroencephalogram," Appl. Sci., vol. 12, no. 9, pp. 4158, 2022. [18] V. Delvigne et al., "Spatio- temporal analysis of transformer based architecture for attention estimation from EEG," in Proc. 26th Int. Conf. Pattern Recognit., Montreal, QC, Canada, 2022, pp. 1076- 1082. [19] Y. Xu et al., "AMDET: Attention based multiple dimensions EEG transformer for emotion recognition," IEEE Trans. Affect. Comput., vol. 15, no. 3, pp. 1067- 1077, 2023. [20] T. Zheng and Z. Guan, "EEG source imaging based on a transformer encoder network," in Proc. 3rd Int. Conf. Neural Networks Inf. Com. mun. Eng., Guangzhou, China, 2023, pp. 208- 212. [21] S. Zheng and D. Wu, "Semi- supervised domain adaptation for EEG- based sleep stage classification," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Seoul, Korea, Republic of, 2024, pp. 1776- 1780. [22] D. Pulver et al., "EEG- based cognitive load classification using feature- masked autorecycling and emotion transfer learning," in Proc. 25th Int. Conf. Multimodal Interact., Paris, France, 2023, pp. 190- 197. [23] P. Wang et al., "Decoding the continuous motion imagery trajectories of upper limb skeleton points for EEG- based brain- computer interface," IEEE Trans. Instrum. Meas., vol. 72, pp. 1- 12, 2023. [24] A. Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale," 2020, arXiv:2010.11929. [25] X. Zhang and H. Li, "Patient- specific seizure prediction from scalp EEG using vision transformer," in Proc. IEEE 6th Inf. Technol. Mechatronics Eng. Conf., Chongqing, China, 2022, pp. 1663- 1667. [26] Z. Yang et al., "Exploring feasibility of truth- involved automatic sleep staging combined with transformer," in Proc. IEEE Int. Conf. Bioinf. Biomed., Houston, TX, USA, 2021, pp. 2920- 2923. [27] Z. Chen et al., "Automated sleep staging via parallel frequency- cut attention," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 1974- 1985, 2023. [28] C. Xu et al., "Trust recognition in human- robot cooperation using EEG," in Proc. IEEE Int. Conf. Robot. Autom., Yokohama, Japan, 2024, pp. 7827- 7833. [29] C. Wang et al., "BrainBERT: Self- supervised representation learning for intracranial recordings," 2023, arXiv:2302.14367. [30] B. Wang et al., "Large transformers are better EEG learners," 2023, arXiv:2308.11654. [31] A. Arjun, A. S. Rajpoot and M. Raveendranatha Panicker, "Introducing attention mechanism for EEG signals: Emotion recognition with vision transformers," in Proc. 43rd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., Mexico, 2021, pp. 5723- 5726. [32] A. Mehtiyev et al., "DeepEnsemble: A novel brain wave classification in MI- BCI using ensemble of deep learners," in Proc. IEEE Int. Conf. Consum. Electron., Las Vegas, NV, USA, 2023, pp. 1- 5. [33] S. V. Bhalerao and R. B. Pachori, "Clustering sparse swarm decomposition for automated recognition of upper limb movements from nonhomogeneous cross- channel EEG signals," IEEE Sensors Lett., vol. 8, no. 1, pp. 1- 4, Jan. 2024. [34] C. Li et al., "EEG- based emotion recognition via transformer neural architecture search," IEEE Trans. Ind. Inform., vol. 19, no. 4, pp. 6016- 6025, Apr. 2023. [35] Z. Zhang et al., "Distilling invariant representations with domain adversarial learning for cross- subject children seizure prediction," IEEE Trans. Cogn. Develop. Syst., vol. 16, no. 1, pp. 202- 211, Feb. 2024.

[36] X. Ding et al., "Scaling up your kernels to 31x31: Revisiting large kernel design in CNNs," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2022, pp. 11953- 11965. [37] Z. Yao and X. Liu, "A CNN- transformer deep learning model for real- time sleep stage classification in an energy- constrained wireless device," in Proc. 11th Int. IEEE/EMBS Conf. Neural Eng., Baltimore, MD, USA, pp. 1- 4, 2023. [38] Q. Zhu et al., "EEG2- rec: Self- supervised electroencephalographic representation learning," 2023, arXiv:2305.13957. [39] H. Zhang et al., "Expert knowledge inspired contrastive learning for sleep staging," in Proc. Int. Joint Conf. Neural Networks, Padua, Italy, 2022, pp. 1- 6. [40] D. Wu et al., "Neuro2vec: Masked fourier spectrum prediction for neurophysiological representation learning," IEEE J. Biomed. Health Inform., 2024, doi: 10.1109/JBHI.2024.3415959. [41] P. Busia et al., "EEGformer: Transformer- based epilepsy detection on raw EEG traces for low- channel- count wearable continuous monitoring devices," in Proc. IEEE Biomed. Circuits Syst. Conf., Taipei, Taiwan, 2022, pp. 640- 644. [42] L. Gong et al., "EEG emotion recognition using attention- based convolutional transformer neural network," Biomed. Signal Process. Control, vol. 84, Jul. 2023, Art. no. 104835. [43] W. Qu et al., "A residual based attention model for EEG based sleep staging," IEEE J. Biomed. Health Inform., vol. 24, no. 10, pp. 2833- 2843, Oct. 2020. [44] A. Qayyum et al., "High- density electroencephalography and speech signal based deep framework for clinical depression diagnosis," IEEE/ACM Trans. Comput. Biol. Bioinf., vol. 20, no. 4, pp. 2587- 2597, Jul. 2023. [45] D. Kostas, S. Aroca- Ouellette and F. Rudzicz, "BENDR: Using transformers and a contrastive self- supervised learning task to learn from massive amounts of EEG data," Front. Hum. Neurosci., vol. 15, pp. 253, Jun. 2021. [46] X. Si et al., "Temporal aware mixed attention- based convolution and transformer network (MACTN) for EEG emotion recognition," 2023, arXiv:2305.18234. [47] Z. Xiao et al., "An efficient temporal network with dual self- distillation for electroencephalography signal classification," in Proc. IEEE Int. Conf. Bioinf. Biomed., Las Vegas, NV, USA, 2022, pp. 1759- 1762. [48] S. Hu et al., "Exploring the applicability of transfer learning and feature engineering in epilepsy prediction using hybrid transformer model," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 1321- 1332, 2023. [49] W. Ding et al., "A novel data augmentation approach using mask encoding for deep learning- based asynchronous SSVEP- BCI," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 875- 886, 2024. [50] Eldele et al., "An attention- based deep learning approach for sleep stage classification with single- channel EEG," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29, pp. 809- 818, 2021. [51] G. Fu et al., "A temporal- spectral fused and attention- based deep model for automatic sleep staging," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 1008- 1018, 2023. [52] J. Luo et al., "A cross- scale transformer and triple- view attention based domain- rectified transfer learning for EEG classification in RSVP tasks," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 672- 683, 2024. [53] J. Chen et al., "Denosimeter: A transformer- based approach for single- channel EEG artifact removal," IEEE Trans. Instrum. Meas., vol. 73, pp. 1- 16, 2024. [54] G. Shi, Z. Chen and R. Zhang, "A transformer- based spatial- temporal sleep staging model through raw EEG," in Proc. Int. Conf. High Perform. Big Data Intell. Syst., Macau, China, 2021, pp. 110- 115. [55] Z. Deng et al., "EEG- based seizure prediction via hybrid vision transformer and data uncertainty learning," Eng. Appl. Artif. Intell., vol. 123, Aug. 2023. [56] M. Abdelmagid et al., "Dual deep learning for sleep classification," in Proc. 11th Int. Japan- Africa Conf. Electron. Commun. Comput. Alexandria, Egypt, 2023, pp. 190- 193. [57] R. Li et al., "A multi- view spectral- spatial- temporal masked autoencoder for decoding emotions with self- supervised learning," in Proc. 30th ACM Int. Conf. Multimedia., Oct. 2022, pp. 6- 14. [58] Z. Li et al., "Temporal- spatial prediction: Pre- training on diverse datasets for EEG classification," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Seoul, Korea, Republic of, 2024, pp. 1806- 1810. [59] J. Liu et al., "Context- aware EEG- based perceived stress recognition based on emotion transition paradigm," in Proc. 11th Int. Conf. Affect. Comput. Intell. Interact. Workshops Demos, Cambridge, MA, USA, 2023, pp. 1- 8.

[60] X. Sun et al., "Conformer: consciousness detection using transformer networks with correntropy- based measures," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 2933- 2943, 2023. [61] S. Koorathota et al., "Multimodal neurophysiological transformer for emotion recognition," in Proc. 44th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., Glasgow, Scotland, United Kingdom, 2022, pp. 3563- 3567. [62] M. Zhang et al., "Multimodal vigilance estimation with modality- pairwise contrast loss," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 71, no. 4, pp. 1139- 1150, Apr. 2024. [63] H.- Y. S. Chien et al., "MaEEG: Masked auto- encoder for EEG representation learning," 2022, arXiv:2211.02625. [64] W. B. Jiang, L. M. Zhao and B. L. Lu, "Large brain model for learning generic representations with tremendous EEG data in BCI," 2024, arXiv:2405.18765. [65] E. Shi, et al., "FoME: A foundation model for EEG using adaptive temporal- lateral attention scaling," 2024, arXiv:2409.12454. [66] R. Khadka et al., "Inducing inductive bias in vision transformer for EEG classification," in Proc. ICASSP IEEE Int. Conf. Acoust. Speech Signal Process., Seoul, Korea, Republic of, 2024, pp. 2096- 2100. [67] H. Cai et al., "AITST—affective EEG- based person identification via interrelated temporal- spatial transformer," Pattern Recognit. Lett., vol. 174, pp. 32- 38, Oct. 2023. [68] X. Zhang and X. Cheng, "A transformer convolutional network with the method of image segmentation for EEG- based emotion recognition," IEEE Signal Process. Lett., vol. 31, pp. 401- 405, 2024. [69] A. Miltiadous et al., "DICE- net: A novel convolution- transformer architecture for Alzheimer detection in EEG signals," IEEE Access, vol. 11, pp. 71840- 71858, 2023. [70] C. Cheng et al., "Multi- domain encoding of spatiotemporal dynamics in EEG for emotion recognition," IEEE J. Biomed. Health Informat., vol. 27, no. 3, pp. 1342- 1353, Mar. 2023. [71] H. Sun et al., "MEEG- transformer: Transformer network based on multi- domain EEG for emotion recognition," in Proc. IEEE Int. Conf. Bioinf. Biomed., Istanbul, Turkiye, 2023, pp. 3332- 3339. [72] Y. Ma, Y. Song and F. Gao, "A novel hybrid CNN- transformer model for EEG motor imagery classification," in Proc. Int. Joint Conf. Neural Networks, Padua, Italy, 2022, pp. 1- 8. [73] Z. Wu, B. Sun and X. Zhu, "Coupling convolution, transformer and graph embedding for motor imagery brain- computer interfaces," in Proc. IEEE Int. Symp. Circuits Syst., Austin, TX, USA, 2022, pp. 404- 408. [74] Q. Li et al., "Transformer- based spatial- temporal feature learning for P300," in Proc. 16th ICME Int. Conf. Complex Med. Eng., Zhongshan, China, 2022, pp. 310- 313. [75] J. Luo et al., "A shallow mirror transformer for subject- independent motor imagery BCI," Comput. Biol. Med., vol. 164, Sep. 2023, Art. no. 107254. [76] W. Tao et al., "ADFCNN: Attention- based dual- scale fusion convolutional neural network for motor imagery brain—computer interface," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 154- 165, 2024. [77] Y. Ding et al., "EEG- deformer: A dense convolutional transformer for brain- computer interfaces," IEEE J. Biomed. Health Informat., vol. 29, no. 3, pp. 1909- 1918, Mar. 2025. [78] D. Zhang, H. Li and J. Xie, "MI- CAT: A transformer- based domain adaptation network for motor imagery classification," Neural Netw., vol. 165, pp. 451- 462, Aug. 2023. [79] W. Cui et al., "Neuro- gpt: Developing a foundation model for EEG," 2023, arXiv:2311.03764. [80] Y. Song, "Decoding natural images from EEG for object recognition," in Proc. 12th Int. Conf. Learn. Representant, 2024. pp. 1- 6. [81] H. Li, H. Zhang and Y. Chen, "Dual- TSST: A dual- branch temporal- spectral- spatial Transformer model for EEG decoding," IEEE J. Biomed. Health Inform., 2025, doi: 10.1109/JBHI.2025.3577611. [82] G. Ren et al., "A transformer encoder and convolutional neural network combined method for classification of error- related potentials," in Proc. IEEE Biomed. Circuits Syst. Conf., Toronto, ON, Canada, 2023, pp. 1- 5. [83] Y. Song et al., "Global adaptive transformer for cross- subject enhanced EEG classification," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 2767- 2777, 2023. [84] X. Tan "Transformer- based network with optimization for cross- subject motor imagery identification," Bioengineering, vol. 10, no. 5, pp. 609, May 2023.

[85] Y. E. Lee and S. H. Lee, "EEG- transformer: Self- attention from transformer architecture for decoding EEG of imagined speech," in Proc. 10th Int. Winter Conf. Brain- Computer Interface, Gangwon- do, Korea, Republic of, 2022, pp. 1- 4. [86] Y. Song et al., "EEG conformer: Convolutional transformer for EEG decoding and visualization," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 710- 719, 2023. [87] H. J. Ahn et al., "Multiscale convolutional transformer for EEG classification of mental imagery in different modalities," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 646- 656, 2023. [88] Z. Li et al., "MST- net: A multi- scale swin transformer network for EEG- based cognitive load assessment," Brain Res. Bull., vol. 206, 2024, Art. no. 110834. [89] S. Bagchi and D. R. Bathula, "EEG- ConvTransformer for single- trial EEG- based visual stimulus classification," Pattern Recognit., vol. 129, Sep. 2022, Art. no. 100257. [90] M. Pang et al., "Multi- scale masked autoencoders for cross- session emotion recognition," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 1637- 1646, 2024. [91] H. Chen, O. Gaoxiang and X. Li, "Extracting temporal- spectral- spatial representation of EEG using self- supervised learning for the identification of children with ASD," in Proc. IEEE 13th Int. Conf. on Cyber Technol. Automat., Control, Intelli. Syst., (CYBER), Qinhuangdao, China, 2023, pp. 1263- 1266. [92] J. Su et al., "Transformer- based multiscale 3- D convolutional network for motor imagery classification," IEEE Sensors J., vol. 25, no. 5, pp. 8621- 8630, 2025. [93] G. Cao et al., "BiCCT: A compact convolutional transformer for EEG emotion recognition," in Proc. IEEE Int. Conf. Bioinf. Biomed., Istanbul, Turkiye, pp. 4792- 4799, 2023. [94] J. Liu et al., "Spatial- temporal transformers for EEG emotion recognition," in Proc. Int. Conf. Adv. Artif. Intell., 2022, pp. 116- 120. [95] P. L. Lee et al., "Continuing learning of a transformer- based deep learning classifier using an initial model from action observation EEG data to online motor imagery classification," Bioengineering, vol. 10, no. 2, pp. 186, Feb. 2023. [96] Y. Zhang, W. Chen and C. Cheng, "Multi- modal network based on spatial- temporal and attention for attention recognition," in Proc. IEEE Int. Conf. Bioinf. Biomedicine, Istanbul, Turkiye, 2023, pp. 2364- 2367. [97] S. Lee, T. H. Pham and P. Zhang, "DREAM: Domain invariant and contrastive representation for sleep dynamics," in Proc. IEEE Int. Conf. Data Mining, Orlando, IL, USA, 2022, pp. 1029- 1034. [98] Q. Shen et al., "LGSleepNet: An automatic sleep staging model based on local and global representation learning," IEEE Trans. Instrum. Meas., vol. 72, pp. 1- 14, 2023. [99] S. Sartipi and M. Cetin, "Adversarial discriminative domain adaptation and transformers for EEG- based cross- subject emotion recognition," in Proc. 11th Int. IEEE/EMBS Conf. Neural Engineer., Baltimore, MD, USA, 2023, pp. 1- 4. [100] L. Yu et al., "Denoise enhanced neural network with efficient data generation for automatic sleep stage classification of class imbalance," in Proc. Int. Joint Conf. Neural Networks, Gold Coast, Australia, 2023, pp. 1- 8. [101] Y. Duan et al., "Multi- class image generation from EEG features with conditional generative adversarial networks," in Proc. Int. Conf. Wireless Commun. Signal Process., Hangzhou, China, 2023, pp. 534- 539. [102] X. Zhao and Y. Guo, "Dual AxAtGAN: A feature integrate BCI model for image reconstruction," in Proc. 13th Int. Conf. Int. Technol. Med. Educ., Wuyishan, China, 2023, pp. 135- 139. [103] W. Peebles and S. Xie, "Scalable diffusion models with Transformers," in Proc. IEEE/CVF Int. Conf. on Comput. Vis. (ICCV), Oct. 2023, pp. 4172- 4182. [104] Y. Wang, et al., "DiffMDD: A diffusion- based deep learning framework for MDD diagnosis using EEG," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 728- 738, 2024. [105] S. Wang, et al., "Generative AI enables EEG super- resolution via spatio- temporal adaptive diffusion learning," IEEE Trans. Consum. Electron., vol. 71, no. 1, pp. 1034- 1045, Feb. 2025. [106] M. Chen, et al., "Improving EEG classification through randomly reassembling original and generated data with transformer- based diffusion models," 2024, arXiv:2407.20253. [107] D. Li, et al., "Visual decoding and reconstruction via EEG embeddings with guided diffusion," 2024, arXiv:2403.07721. [108] M. Sun et al., "A dual- branch dynamic graph convolution based adaptive transformer feature fusion network for EEG emotion recognition," IEEE Trans. Affect. Comput., vol. 13, no. 4, pp. 2218- 2228, 2022.

[109] W. B. Jiang et al., "Elastic graph transformer networks for EEG- based emotion recognition," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Rhodes Island, Greece, 2023, pp. 1- 5. [110] H. Kim and Y. S. Choi, "Semi- supervised EEG emotion recognition with discriminative graph transformer model," in Proc. IEEE Int. Conf. Consum. Electronics- Asia, Busan, Korea, Republic of, 2023, pp. 1- 4. [111] Y. Wang et al., "Dynamic multi- graph convolution- based channel- weighted transformer feature fusion network for epileptic seizure prediction," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 4266- 4277, 2023. [112] Y. Guo et al., "A comprehensive interaction in multiscale multichannel EEG signals for emotion recognition," Mathematics, vol. 12, no. 8, p. 1180, Apr. 2024. [113] J. Luo et al., "A dual- branch spatio- temporal- spectral transformer feature fusion network for EEG- based visual recognition," IEEE Trans. Ind. Inform., vol. 20, no. 2, pp. 1721- 1731, Feb. 2024. [114] Z. Wang et al., "R- transformer: Recurrent neural network enhanced transformer," 2019, arXiv:1907.05722. [115] D. K. Ravikanti and S. Saravanan, "EEGAlzheimer'sNet: Development of transformer- based attention long short term memory network for detecting Alzheimer disease using EEG signal," Biomed. Signal Process. Control, vol. 86, Sep. 2023. Art. no. 105318. [116] D. T. Pham and R. Mouček, "Automatic sleep stage classification by CNN- transformer- LSTM using single- channel EEG signal," in Proc. IEEE Int. Conf. Bioinf. Biomed., Istanbul, Turkiye, 2023, pp. 2559- 2563. [117] X. Zhou et al., "EmoTVR: A hybrid model to estimate continuous- time and continuous- level emotion from electroencephalography," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Seoul, Korea, Republic of, 2024, pp. 2021- 2025. [118] Y. Qin, W. Zhang and X. Tao, "TBEEG: A two- branch manifold domain enhanced transformer algorithm for learning EEG decoding," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 1466- 1476, 2024. [119] Y. Wang, L. Zhang and Y. Zhang, "Band- level adaptive fusion network for cross- subject EEG emotion recognition," IEEE Trans. Instrum. Meas., vol. 74, pp. 1- 12, 2025. [120] A. Tavanaei et al., "Deep learning in spiking neural networks," Neural Networks, vol. 111, pp. 47- 63, 2019. [121] Q. Chen et al., "Epilepsy seizure detection and prediction using an approximate spiking convolutional transformer," 2024, arXiv:2402.09424. [122] Z. Zhou et al., "Spikformer: When spiking neural network meets transformer," 2022, arXiv:2209.15425. [123] M. K. Patrick et al., "Capsule networks- a survey," J. King Saud Univ. Comput. Inf. Sci., vol. 34, no. 1, pp. 1295- 1310, 2022. [124] Y. Wei et al., "TC- Net: A transformer capsule network for EEG- based emotion recognition," Comput. Biol. Med., vol. 152, Jan. 2023. Art. no. 106463. [125] Y. Du et al., "MES- CTNet: A novel capsule transformer network base on a multi- domain feature map for electroencephalogram- based emotion recognition," Brain Sci., vol. 14, no. 4, p. 344, 2024. [126] W. Lu, T. P. Tan and H. Ma, "Bi- branch vision transformer network for EEG emotion recognition," IEEE Access, vol. 11, pp. 36233- 36243, 2023. [127] E. Eldele et al., "Time- series representation learning via temporal and contextual contrasting," 2021, arXiv:2100.14112. [128] J. Qiu, et al., "Can brain signals reveal inner alignment with human languages?" in Proc. Findings Assoc. Comput. Linguistics: EMNLP, 2023, pp. 1789- 2804. [129] R. Peng et al., "WAVELET2VEC: A filter- bank masked autoencoder for EEG- based seizure subtype classification," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Rhodes Island, Greece, 2023, pp. 1- 5. [130] B. Fu, X. Yu and Y. Liu, "Linear- nonlinear feature reconstruction network for emotion recognition from brain functional connectivity," IEEE Trans. Instrum. Meas., vol. 74, pp. 1- 10, 2025. [131] Y. Wang et al., "Emotion transformer fusion: complementary representation properties of EEG and eye movements on recognizing anger and surprise," in Proc. IEEE Int. Conf. Bioinf. Biomed., Houston, TX, USA, 2021, pp. 1575- 1578. [132] J. Sun et al., "START: Automatic sleep staging with attention- based cross- modal learning transformer," in Proc. IEEE Int. Conf. Bioinf. Biomed., Istanbul, Turkiye, 2023, pp. 1421- 1428. [133] K. Han et al., "Transformer in transformer," in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 15908- 15919. [134] Z. Wang et al., "Transformers for EEG- based emotion recognition: A hierarchical spatial information learning model," IEEE Sensors J., vol. 22, no. 5, pp. 4359- 4368, Mar. 2022.

[135] H. Phan et al., "SleepTransformer: Automatic sleep staging with interpretability and uncertainty quantification," IEEE Trans. Biomed. Eng., vol. 69, no. 8, pp. 2456- 2467, Aug. 2022. [136] H. Yao et al., "A spatial- temporal transformer architecture using multichannel signals for sleep stage classification," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 3353- 3362, 2023. [137] H. Liu et al., "EEG2TEXT: Open vocabulary EEG- to- text decoding with EEG pre- Training and multi- view transformer," 2024 arXiv:2405.02165. [138] M. Kim, K. Jung and W. Chung, "Automatic sleep stage classification method based on transformer- in- transformer," in Proc. 11th Int. Winter Conf. Brain- Computer Interface, Gangwon, Korea, Republic of, 2023, pp. 1- 5. [139] M. Kim and W. Chung, "Convolutional transformer- in- transformer for automatic sleep stage classification," in Proc. 12th Int. Winter Conf. Brain- Computer Interface, Gangwon, Korea, Republic of, 2024, pp. 1- 5. [140] M. Seraphim et al., "Structure- preserving transformers for sequences of SPD matrices," 2023, arXiv:2309.07579. [141] P. Deny and K. W. Choi, "Hierarchical transformer for brain computer interface," in Proc. 11th Int. Winter Conf. Brain- Computer Interface, Gangwon, Korea, Republic of, 2023, pp. 1- 5. [142] J. Fan et al., "Automatic sleep staging method based on multichannel fusion," in Proc. 3rd Int. Conf. on Electron. Inf. Eng. Comput. Commun., Wuhan, China, 2023, pp. 1- 5. [143] Y. Dai et al., "MultiChannelSleepNet: A transformer- based model for automatic sleep stage classification with PSG," IEEE J. Biomed. Health Inform., vol. 27, no. 9, pp. 4204- 4215, Sept. 2023. [144] S. Shi and W. Liu, "B2- ViT net: Broad vision transformer network with broad attention for seizure prediction," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 178- 188, 2024. [145] W. Yu, et al., "Metaformer is actually what you need for vision," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2022, pp. 10819- 10829. [146] B. Wang, D. Fei and P. Jiang, "EEGDiR: Electroencephalogram denoising network for temporal information storage and global modeling through Retentive Network," Comput. Biol. Med., vol. 177, 2024, Art. no. 108626. [147] Z. He et al., "Unified convolutional sparse transformer for disease diagnosis, monitoring, drug development, and therapeutic effect prediction from EEG raw data," Biology, vol. 13, no. 4, 2024. [148] C. Kalantarpour et al., "Clinical grade prediction of therapeutic dosage for electroconvulsive therapy (ECT) based on patient's pre- ictal EEG using fuzzy causal transformers," in Proc. 3rd Int. Conf. Elect. Comput. Commun. Mechatronics Eng., Tenerife, Canary Islands, Spain, 2023, pp. 1- 6. [149] W. Weng et al., "An efficient spatial- temporal representation method for EEG emotion recognition," in Proc. IEEE Smartworld Ubiquitous Intell. Comput., Scalable Comput., Haikou, China, 2022, pp. 458- 467. [150] C. Yang, M. B. Westover and J. Sun, "BIOT: Cross- data biosignal learning in the wild," 2023, arXiv:2305.10351. [151] T. Fukushima and R. Miyamoto, "Spatiotemporal pooling on appropriate topological maps represented as two- dimensional images for EEG classification," 2024, arXiv:2403.04353. [152] Z. Bai et al., "SECT: A method of shifted EEG channel transformer for emotion recognition," IEEE J. Biomed. Health Inform., vol. 27, no. 10, pp. 4758- 4767, Oct. 2023. [153] K. Jung, M. Kim and W. Chung, "Epoch- level and sequence- level multi- head self- attention- based sleep stage classification," in Proc. 11th Int. Winter Conf. Brain- Computer Interface, Gangwon, Korea, Republic of, 2023, pp. 1- 6. [154] S. Liu et al., "A spatial- temporal transformer based on domain generalization for motor imagery classification," in Proc. IEEE Int. Conf. Syst. Man Cybern., Honolulu, Hawaii, USA, 2023, pp. 3789- 3794. [155] E. Shi et al., "MEET: A multi- band EEG transformer for brain states decoding," IEEE Trans. Biomed. Eng., vol. 71, no. 5, pp. 1442- 1453, May 2024. [156] S. J. Kim et al., "Toward domain- free transformer for generalized EEG pre- training," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 482- 492, 2024. [157] X. Wang et al., "Improving generalized zero- shot learning SSVEP classification performance from data- efficient perspective," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 4135- 4145, 2023. [158] Y. Liu, Y. Zhou and D. Zhang, "TcT: Temporal and channel transformer for EEG- based emotion recognition," in Proc. IEEE 35th Int. Symp. Computer- Based Med. Syst., Shenzhen, China, 2022, pp. 366- 371.

[159] D. Li et al., "ESTformer: Transformer utilizing spatiotemporal dependencies for EEG super- resolution," 2023, arXiv:2312.10052. [160] X. Li, "TACOformer: Token- channel compounded cross attention for multimodal emotion recognition," 2023, arXiv:2306.13592. [161] R. Peng et al., "Multi- branch mutual- distillation transformer for EEG- based seizure subtypes classification," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 831- 839, 2024. [162] M. Liu et al., "EMPT: A sparsity transformer for EEG- based motor imagery recognition," Front. Neurosci., vol. 18, 2024, Art. no. 1366294. [163] M. Yang et al., "IEEG- HCT: A hierarchical CNN- transformer combined network for intracranial EEG signal identification," IEEE Sensors Lett., vol. 8, no. 2, pp. 1- 4, Feb. 2024. [164] C. Li et al., "EEG- based seizure prediction via transformer guided CNN," Measurement, vol. 203, Nov. 2022, Art. no. 111948. [165] J. Chen et al., "A transformer- based deep neural network model for SSVTE classification," Neural Netw., vol. 164, pp. 521- 534, Jul. 2023. [166] Y. Tao et al., "Gated transformer for decoding human brain EEG signals," in Proc. 13rd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., Mexico, 2021, pp. 425- 130. [167] Liu, X. Zhou and Y. Liu, "EENED: End- to- end neural epilepsy detection based on convolutional transformer," in Proc. IEEE Conf. Artif. Intell., Santa Clara, CA, USA, 2023, pp. 368- 371. [168] W. Wang, et al., "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions," in Proc. IEEE/CVF Int. Conf. on Comput. Vis. (ICCV), 2021, pp. 568- 578. [169] R. Zheng et al., "ScatterFormer: Locally- invariant scattering transformer for patient- independent multispectral detection of epileptiform discharges," Proc. AAAI Conf. Artif. Intell., vol. 37, pp. 148- 158, 2023. [170] Z. Wang et al., "JDAT: Joint- dimension- aware transformer with strong flexibility for EEG emotion recognition," in Int. Conf. Electr. Electron. Inf. Eng. (EEI), vol. 13512, pp. 42- 52, Jan. 2025. [171] Z. Liu et al., "Swin transformer: Hierarchical vision transformer using shifted windows," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 10012- 10022. [172] H. Wang et al., "A novel algorithmic structure of EEG channel attention combined with swin transformer for motor patterns classification," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 3132- 3141, 2023. [173] Z. Chen, J. Jin and J. Pan, "Spatiotemporal swin transformer based 4- D EEG emotion recognition," in Proc. IEEE Int. Conf. Bioinf. Biomed., Istanbul, Turkiye, 2023, pp. 1850- 1855. [174] X. Li et al., "A temporal- spectral fusion transformer with subject- specific adapter for enhancing RSVP- BCI decoding," 2024, arXiv:2401.06340. [175] W. B. Jiang et al., "Functional emotion transformer for EEG- assisted cross- modal emotion recognition," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Seoul, Korea, Republic of, 2024, pp. 1841- 1845. [176] J. Yin et al., "Research on multimodal emotion recognition based on fusion of electroencephalogram and electrooculography," IEEE Trans. Instrum. Meas., vol. 73, pp. 1- 12, 2024. [177] K. Kontras et al., "CoRe- sleep: A multimodal fusion framework for time series robust to imperfect modalities," IEEE Trans. Neural Syst. Rehabil. Eng., vol. 32, pp. 840- 849, 2024. [178] R. Qin et al., "BNMTrans: A brain network sequence- driven manifold- based transformer for cognitive impairment detection using EEG," in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Seoul, Korea, Republic of, 2024, pp. 2016- 2020. [179] W. Weng et al., "Self- supervised learning for electroencephalogram: A systematic survey," 2024, arXiv:2401.05446. [180] X. Zhou et al., "Brain foundation models: A survey on advancements in neural signal processing and brain discovery," 2025, arXiv:2503.00580. [181] P. Fantozzi and M. Naldi. "The explainability of transformers: current status and directions," Computers, vol. 13, no. 4, p. 92, 2024. [182] S. Zhao et al., "A resource- efficient algorithm- hardware co- design toward semi- supervised neurological symptoms prediction," IEEE Trans. Instrum. Meas., vol. 74, pp. 1- 12, 2025. [183] R. Jiang et al., "Application of transformer with auto- encoder in motor imagery EEG signals," in Proc. 14th Int. Conf. Wireless Commun. Signal Process., Nanjing, China, 2022, pp. 1- 7. [184] S. Shukla et al., "A survey on bridging EEG signals and generative AI: from image and text to beyond," 2025, arXiv:2502.12048.

# Supplementary Material for "Transformer Based EEG Decoding: A Survey"

Haodong Zhang, Hongqi Li*, Member, IEEE

# Part A - Examples of customized Transformers with modified encoders

Some representative examples that fall into the category of customized Transformers with modified encoders are summarized in Table A- I, where the tasks, model characteristics, and comparative evaluation results are reported in detail.

# Part B - Commonly used EEG standard public datasets based on Transformer decoding

Public datasets play a crucial role in the training and evaluation of Transformer models, providing a unified benchmark for algorithm validation and performance comparison. Table B- I summarizes commonly used public EEG datasets in the field of decoding, detailing key attributes such as data size, annotation methods, number of subjects, number of channels, sampling rates, task types, label formats, auxiliary data (e.g., EOG, EMG), and references to relevant research studies. Additionally, each dataset is linked to its corresponding webpage, allowing readers to easily access detailed information and download the datasets for further research and experimentation.

These standardized datasets are not only tailored to specific tasks (e.g., motor imagery classification, sleep staging, emotion recognition) but also support the investigation of model generalization across tasks and subjects. Furthermore, the diversity and high- quality annotations of these datasets provide a solid foundation for large- scale pretraining models, such as Brain Foundation Models, driving advancements in EEG decoding technologies.

# Part C - The extracted EEG features by Transformer-based models

The decoding characteristics of EEG are primarily reflected in three dimensions: temporal, spatial, and spectral features. Transformer- based EEG decoding models effectively leverage these features by capturing the dynamic variations of EEG signals, modeling complex spatial interactions across brain regions, and extracting key patterns from spectral information, thereby significantly enhancing decoding flexibility and accuracy.

In temporal feature modeling, Transformer- based methods capitalize on their robust sequence processing capabilities to capture both short- term and long- term dependencies in EEG signals. For spatial feature modeling, these models exploit the multi- channel nature of

EEG to uncover the spatial distribution of brain activity. Spectral features, on the other hand, are processed through a combination of spectral transformations and feature extraction techniques, improving the handling of frequency- domain information. The flexible integration of these features provides strong support for addressing more complex and diverse decoding tasks.

To systematically summarize the progress of Transformer- based EEG decoding research, Table C- I categorizes models into Backbone Models, Hybrid Models, and Customized Trans- formers. It outlines how each model type utilizes temporal, spatial, and spectral features, and highlights representative studies. The table offers a clear view of different focuses in feature selection across models, such as temporal modeling, spatial interaction, or spectral analysis, and demonstrates how the integration of multidimensional features optimizes decoding performance. This comprehensive summary provides a clear direction and reference framework for future research.

Specifically, as we can see from the Table, since EEG signal possesses the good temporal properties, the relevant temporal features have been exploited by most of current studies, except for [15], [25], [28], [34], [68], [148], [165], [169]. Meanwhile, various Transformer- based networks are concentrating on the complex spatial relationships among multiple channels of electrodes. Interestingly, current Transformers, no matter in which structures, have limited applications to separately dig into frequency domain features. Instead, most studies converted EEG signals into time- frequency data or combine them with other features for processing. In particular, the temporal- spatial- spectral representations of EEG have been simultaneously investigated by categorized models, such as backbone [17], hybrid DL- Transformer (with CNN [40], [76], [85], GAN [101], RNN [117], [118], CapsNet [123], [124]), and customized Transformers (e.g., multi- Encoder [126], [143], modified Encoder [77], [157], Pyramid [170], [172], reconstructed Transformers [52], [83]). These observations suggest that there is still a great challenge in generalizing the model to all EEG characteristics, especially for those specialized transformer networks in frequency features.

# Part D- Evaluating EEG Model Complexity through Parameter Count

Generally, the computational complexity of EEG decoding models is influenced by architecture, parameter

size, and task requirements. While FLOPs are often used as a measure of complexity, their relevance in EEG decoding is limited due to the relatively small datasets and hardware constraints. Instead, parameter count provides a more practical metric, reflecting memory usage and deployment feasibility.

Based on the parameter count, EEG decoding models can be categorized into small (1K- 100K), medium (100K- 10M), and large models (10M and above). This classification reflects a trade- off between computational efficiency and task complexity. Table D- I provides a detailed comparison of parameter sizes across different model categories. It is evident that Transformer- based models generally exhibit higher parameter counts compared to traditional CNN models such as EEGNet and ConvNet.

# A. Small Parameter Models (1K-100K)

As a representative CNN model, EEGNet performs exceptionally well in resource- constrained environments due to its minimal parameter count (1K). However, its limited expressive power makes it unsuitable for complex EEG decoding tasks. An example of customized Transformer- based architectures of TSFormer- SA [174], having a slightly higher parameter count (4.86K), maintain relatively low computational overhead and are well- suited for lightweight tasks. Additionally, one typical CNN- Transformer of EEGFormer [41], specifically designed for wearable devices, also possesses the small parameter count of  $50.6\mathrm{K}$  which is able to achieve efficient operation on low- end hardware. Such small- scale models are ideal for rapid deployment and low- complexity tasks.

# B. Medium Parameter Models (100K-10M)

Medium- scale models are related to Backbone- based and CNN- Transformer architectures. Backbone- based models (e.g., [19], [34]) feature increased parameter sizes to capture richer temporal, spatial, and frequency features, significantly improving decoding performance. On the other hand, Hybrid models (e.g., [49]) combine shallow CNN or other structures with Transformer layers, achieving a balance between computational complexity and task performance. These models are well- suited for diverse EEG tasks, such as Motor Imagery (MI) and Sleep Stage Classification (SSC).

# C. High Parameter Models and Foundation Models  $(10M + )$

High- parameter models are typically associated with ViT and its variants, such as Swin Transformer [173], and Pyramid Transformer [169]. These models rely on converting EEG signals into image representations, leveraging larger parameter sizes (13M- 42M) to enhance feature express-iveness. However, this comes with significant computational costs. Some hybrid model of Brain

Foundation Models (BFMs) represent the upper bound of parameter counts (i.e., 369M- 1.69B of [64], [65]). By utilizing large- scale pretraining and task- specific finetuning, BFMs demonstrate exceptional generalization across various tasks. Nevertheless, their dependence on high- quality data and extensive computational resources limits their feasibility in resource- constrained scenarios.

The parameter count of EEG decoding models correlates positively with task complexity: small parameter models are suitable for resource- limited applications, medium parameter models achieve a balance between performance and computational complexity, and high parameter models, including BFMs, excel in complex tasks and generalization but come at a higher computational cost.

Overall, the Transformer- based decoding networks of EEG has covered different parameter scales of range from K to B. It is evident that these advanced models exhibit higher parameter counts compared to traditional CNN models (e.g., EEGNet that with mere 1K parameters), implying a more complicated structure. Due to different effecting mechanisms, even models of the same type, such as hybrid models or customized Transformer models, have significantly varying parameters. In any case, the quest for high- performance models with fewer parameters is always present here. Future research should focus on optimizing the trade- off between performance and resource consumption, particularly for multi- task and cross- platform applications.

TABLE III OVERVIEW OF REPRESENTATIVE MODIFIED ENCODER BASED TRANSFORMER ARCHITECTURE STUDIES [ACC: ACCURACY; MF1: MACRO-AVERAGE F1-SCORE; SNR: SIGNAL NOISE RATIO; PCC: PEARSON CORRELATION COEFFICIENT; ITR: INFORMATION TRANSFER RATE;  $\kappa$  COHEN KAPPA; RRSE: RELATIVE ROOT MEAN SQUARED ERROR; CC: CORRELATION COEFFICIENT; PR: PRECISION; BCA: BALANCED ACC; AUPRC: AREA UNDER THE PRECISION-RECALL CURVE]  

<table><tr><td>Year</td><td>Models</td><td>Ref</td><td>Tasks</td><td>Model Characteristics</td><td>Dataset</td><td>Evaluation</td></tr><tr><td>EeT</td><td>2021</td><td>[94]</td><td>ER</td><td>Spatial &amp;amp; Temporal attention are to learn the topological structure information and time-varying EEG characteristics</td><td>DEAP
Valence 92.86%, F1 0.9196
SEED
SEED-IV</td><td>Acc: Arousal 93.34%, F1 0.9326</td></tr><tr><td>MV-SSTMA</td><td>2022</td><td>[57]</td><td>ER</td><td>Multi-view casual Conv &amp;amp; Spatial Attention</td><td>SEED
SEED-IV</td><td>Acc: 96.28%,
Acc: 96.28%,
Acc: 92.82%</td></tr><tr><td>ASTR</td><td>2022</td><td>[149]</td><td>ER</td><td>Cascade Attention &amp;amp; One-way SA</td><td>SEED
DEAP</td><td>Acc: 98.52%, Mix Acc 90.21%,
Acc: 92.18%, Mix Acc 80.49%</td></tr><tr><td>Gated-T</td><td>2022</td><td>[166]</td><td>EEG-Vision /MI</td><td>Introduce Gating Mechanism to Transformer for EEG decoding</td><td>PhysioNet Dataset
Brain Visual Dataset</td><td>Acc: 55.40%,
Acc: high gamma 47.53%,
AUC 93.70%, model size 7.75 MB, etc.</td></tr><tr><td>HVIT-DUL</td><td>2023</td><td>[55]</td><td>BD(SP)</td><td>Using SMHA to replace MHA</td><td>CHB-MIT
AES</td><td>AUC 88.90%, model size 5.06 MB, etc.</td></tr><tr><td>MI-CAT</td><td>2023</td><td>[78]</td><td>MI</td><td>Cross-Transformer block (CTB) to capture domain-dependent information</td><td>BCI IV 2a
BCI IV 2b</td><td>Acc: 76.74%, κ 0.690
Acc: 85.26%, κ 0.706</td></tr><tr><td>EEG-ConvT</td><td>2023</td><td>[89]</td><td>EEG Visual</td><td>MHA &amp;amp; CFE</td><td>Stanford</td><td>Acc: 52.33% for 6 class</td></tr><tr><td>SECT</td><td>2023</td><td>[152]</td><td>ER</td><td>Band Attention Block &amp;amp; Channel Transformer &amp;amp; Shifted Channel Transformer</td><td>SEED
DEAP
HIECD(PD)</td><td>Acc: 85.43%,
Acc: 66.83%, Arousal 65.31%,
Acc: 84.76%</td></tr><tr><td>ESSleep</td><td>2023</td><td>[153]</td><td>SSC</td><td>Epoch Encoder &amp;amp; ES block</td><td>SleepEDFX-78</td><td>Acc: 81.60%, MF1 74.6%,
Acc: 57.705%, AUC 0.823</td></tr><tr><td>ST-DG</td><td>2023</td><td>[78]</td><td>MI</td><td>Spatial-Temporal Transformer based on Domain Generalization</td><td>BCI IV 2b
BCI IV 2b</td><td>Acc: 75.080%, AUC 0.834</td></tr><tr><td>SECT</td><td>2023</td><td>[152]</td><td>ER</td><td>Band Attention Block &amp;amp; Channel Transformer</td><td>SEED
DEAP
HIECD(PD)</td><td>Acc: 86.83.43%,
Acc: 84.76%</td></tr><tr><td>MEET</td><td>2023</td><td>[155]</td><td>ER &amp;amp; Working Memory</td><td>Band Attention Block &amp;amp; Channel Transformer &amp;amp; Shifted Channel Transformer</td><td>SEED
SEED-IV</td><td>Acc ± Std 99.18 ± 0.34%,
Acc ± Std 95.18 ± 1.92%,
Acc ± Std 98.14 ± 1.97%</td></tr><tr><td>-</td><td>2023</td><td>[157]</td><td>SSVEP</td><td>MHA &amp;amp; CA, introduced a dynamic sampling starting time</td><td>SSVEP Bench
BETA
USCD</td><td>Acc about 96%, ITR 165 bit/min
Acc about 80%, ITR 150 bit/min</td></tr><tr><td>TcT</td><td>2023</td><td>[158]</td><td>ER</td><td>Temporal self-attention &amp;amp; Channel SA MLP</td><td>DEAP</td><td>Acc: 96.76%, Arousal 97.02%, V-A 95.97</td></tr><tr><td>ESTformer</td><td>2023</td><td>[159]</td><td>EEG SRR PI,ER</td><td>Spatial interpolation &amp;amp; Temporal reconstruction</td><td>PhysioNet MI
SEED</td><td>NMSE 0.17, SNR 7.60, PCC 0.91
NMSE 0.32, SNR 5.63, PCC 0.85</td></tr><tr><td>TACO-former</td><td>2023</td><td>[160]</td><td>ER</td><td>TACO Attention and SMHA as MHA alternatives, supporting multi-modality</td><td>DEAP
SEED</td><td>Acc: 91.59%, Arousal 92.02%,
Acc: 94.58%, MF1 74.6%,
Acc: 57.705%, AUC 0.823</td></tr><tr><td>TGCNN</td><td>2023</td><td>[164]</td><td>ER</td><td>Local Infor, Layer &amp;amp; Squeezed MHA &amp;amp; Residual FFN</td><td>CHB-MIT
Kaggle</td><td>AUC 93.50%,
AUC 83.50%</td></tr><tr><td>SSVEP Former</td><td>2023</td><td>[165]</td><td>SSVEP</td><td>CNN &amp;amp; Channel MLP Module</td><td>Nakanishi Dataset
Wang Dataset</td><td>Acc 88.43%, ITR 99.05 bit/min
Acc 83.98%, ITR 142.05 bit/min</td></tr><tr><td>EENED</td><td>2023</td><td>[167]</td><td>BD</td><td>Introducing the Conv module into Transformer encoder</td><td>Epileptic Seizure Recognition dataset</td><td>Acc 98.20%, F1 score 0.989</td></tr><tr><td>EEG Deformer</td><td>2024</td><td>[77]</td><td>Cognitive Detection</td><td>Hierarchical Coarse-to-Fine Transformer</td><td>SEED
SEED-IV</td><td>Acc ± Std 99.18 ± 0.34%,
Acc ± Std 95.18 ± 1.92%,
Acc ± Std 98.14 ± 1.97%</td></tr><tr><td>EEGDIR</td><td>2024</td><td>[146]</td><td>Seq2seq</td><td>Multi-Scale Retention to replace MHA</td><td>EEG-EOG
EEG-EMG
SS2016 EOG
MDD Diagnosis</td><td>RRMSE: 0.3279, 0.3614, CC 0.9329
RRMSE: 0.5322, 0.5004, CC 0.8072
RRMSE: 0.3146, 0.3613, CC 0.9488</td></tr><tr><td>UCST</td><td>2024</td><td>[147]</td><td>BD/EEG Drug Discovery</td><td>Sparse Attention to replace MHA</td><td>Helsinki Dataset
Pharmaco-EEG Dataset</td><td>Acc 99.60%, PR 99.4%, Recall 99.7%, F1 0.996
Acc 95.70%, PR 88.9%, Recall 90.1%, F1 0.895
Acc 98.20%, PR 100%, Recall 97.1%, F1 0.985</td></tr><tr><td>EEG-Poolformer</td><td>2024</td><td>[151]</td><td>MI</td><td>2D Average Pooling Layer.</td><td>PhysioNet</td><td>Acc: 88.57%, 80.65%, 70.17% for 2, 3, 4 class</td></tr><tr><td>DFformer</td><td>2024</td><td>[156]</td><td>SSC MI</td><td>Intra-MHA &amp;amp; Inter-Channel MHA</td><td>BCI IV 2a
BCI IV 2b
Sleep-EDF
SHHS
TUSZ
CHSZ
PD</td><td>Acc 58.41%, κ 0.4455, F1 0.5837
Acc 76.18%, κ 0.5208, F1 0.7552
Acc 83.70%, κ 0.7778, F1 0.7809
Acc 83.89%, κ 0.7739, F1 0.7620
Acc 65.00%, BCA 68.4%, Weighted F1 0.667
Acc 74.60%, BCA 66.6%, Weighted F1 0.739
Acc 95.24%, PR 96.38%, Recall 94.88%</td></tr><tr><td>MBMD-T</td><td>2024</td><td>[161]</td><td>BD</td><td>Multi-branch FFN to replace FFN</td><td>CSZ</td><td>Acc 93.39%</td></tr><tr><td>EMPT</td><td>2024</td><td>[162]</td><td>MI</td><td>ProbSparse Attention to replace MHA, MoE to replace FFN</td><td>BFNS</td><td>AUROC 0.93, AUPRC 0.76</td></tr><tr><td>IEEG-HCT</td><td>2024</td><td>[163]</td><td>BD</td><td>Seperable Convolution &amp;amp; Squeezed MHA</td><td>FNUSA</td><td>AUROC 0.93, AUPRC 0.87</td></tr></table>

T-1 TAEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEETT TEEE  

<table><tr><td>Dataset</td><td>Data Size</td><td>Annimation</td><td>Subject</td><td>Channels</td><td>Sampling Rates</td><td>Task</td><td>Label</td><td>Auxiliary Data</td><td>Research</td></tr><tr><td>BCIComp. IV 2a</td><td>848 MB</td><td>Automatic (Experiment)</td><td>9</td><td>22</td><td>250Hz</td><td>MH</td><td>LH/RH/600/1000 Tongue</td><td>EOG</td><td>[72], [68], [67], [76], [78], [79], [81], [83], [84], [80], [81], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499], [500], [501], [502], [503], [504], [505], [506], [507], [508], [509], [510], [511], [512], [513], [514], [515], [516], [517], [518], [519], [520], [521], [522], [523], [524], [525], [526], [527], [528], [529], [530], [531], [532], [533], [534], [535], [536], [537], [538], [539], [540], [541], [542], [543], [544], [545], [546], [547], [548], [549], [550], [551], [552], [553], [554], [555], [556], [557], [558], [559], [560], [561], [562], [563], [564], [565], [566], [567], [568], [569], [570], [571], [572], [573], [574], [575], [576], [577], [578], [579], [580], [581], [582], [583], [584], [585], [586], [587], [588], [589], [590], [591], [592], [593], [594], [595], [596], [597], [598], [599], [600], [601], [602], [603], [604], [605], [606], [607], [608], [609], [610], [611], [612], [613], [614], [615], [616], [617], [618], [619], [620], [621], [622], [623], [624], [625], [626], [627], [628], [629], [630], [631], [632], [633], [634], [635], [636], [637], [638], [639], [640], [641], [642], [643], [644], [645], [646], [647], [648], [649], [650], [651], [652], [653], [654], [655], [656], [657], [658], [659], [660], [661], [662], [663], [664], [665], [666], [667], [668], [669], [670], [671], [672], [673], [674], [675], [676], [677], [678], [679], [680], [681], [682], [683], [684], [685], [686], [687], [688], [689], [690], [691], [692], [693], [694], [695], [696], [697], [698], [699], [700], [701], [702], [703], [704], [705], [706], [707], [708], [709], [710], [711], [712], [713], [714], [715], [716], [717], [718], [719], [720], [721], [722], [723], [724], [725], [726], [727], [728], [729], [730], [731], [732], [733], [734], [735], [736], [737], [738], [739], [740], [741], [742], [743], [744], [745], [746], [747], [748], [749], [750], [751], [752], [753], [754], [755], [756], [757], [758], [759], [760], [761], [762], [763], [764], [765], [766], [767], [768], [769], [770], [771], [772], [773], [774], [775], [776], [777], [778], [779], [780], [781], [782], [783], [784], [785], [786], [787], [788], [789], [790], [791], [792], [793], [794], [795], [796], [797], [798], [799], [800], [801], [802], [803], [804], [805], [806], [807], [808], [809], [810], [811], [812], [813], [814], [815], [816], [817], [818], [819], [820], [821], [822], [823], [824], [825], [826], [827], [828], [829], [830], [831], [832], [833], [834], [835], [836], [837], [838], [839], [840], [841], [842], [843], [844], [845], [846], [847], [848], [849], [850], [851], [852], [853], [854], [855], [856], [857], [858], [859], [860], [861], [862], [863], [864], [865], [866], [867], [868], [869], [870], [871], [872], [873], [874], [875], [876], [877], [878], [879], [880], [881], [882], [883], [884], [885], [886], [887], [888], [889], [890], [891], [892], [893], [894], [895], [896], [897], [898], [899], [900], [901], [902], [903], [904], [905], [906], [907], [908], [909], [910], [911], [912], [913], [914], [915], [916], [917], [918], [919], [920], [921], [922], [923], [924], [925], [926], [927], [928], [929], [930], [931], [932], [933], [934], [935], [936], [937], [938], [939], [940], [941], [942], [943], [944], [945], [946], [947], [948], [949], [950], [951], [952], [953], [954], [955], [956], [957], [958], [959], [960], [961], [962], [963], [964], [965], [966], [967], [968], [969], [970], [971], [972], [973], [974], [975], [976], [977], [978], [979], [980], [981], [982], [983], [984], [985], [986], [987], [988], [989], [990], [991], [992], [993], [994], [995], [996], [997], [998], [999], [1000], [1001], [1002], [1003], [1004], [1005], [1006], [1007], [1008], [1009], [1010], [1011], [1012], [1013], [1014], [1015], [1016], [1017], [1018], [1019], [1020], [1021], [1022], [1023], [1024], [1025], [1026], [1027], [1028], [1029], [1030], [1031], [1032], [1033], [1034], [1035], [1036], [1037], [1038], [1039], [1040], [1041], [1042], [1043], [1044], [1045], [1046], [1047], [1048], [1049], [1050], [1051], [1052], [1053], [1054], [1055], [1056], [1057], [1058], [1059], [1060], [1061], [1062], [1063], [1064], [1065], [1066], [1067], [1068], [1069], [1070], [1071], [1072], [1073], [1074], [1075], [1076], [1077], [1078], [1079], [1080], [1081], [1082], [1083], [1084], [1085], [1086], [1087], [1088], [1089], [1090], [1091], [1092], [1093], [1094], [1095], [1096], [1097], [1098], [1099], [1100], [1101], [1102], [1103], [1104], [1105], [1106], [1107], [1108], [1109], [1110], [1111], [1112], [1113], [1114], [1115], [1116], [1117], [1118], [1119], [1120], [1121], [1122], [1123], [1124], [1125], [1126], [1127], [1128], [1129], [1130], [1131], [1132], [1133], [1134], [1135], [1136], [1137], [1138], [1139], [1140], [1141], [1142], [1143], [1144], [1145], [1146], [1147], [1148], [1149], [1150], [1151], [1152], [1153], [1154], [1155], [1156], [1157], [1158], [1159], [1160], [1161], [1162], [1163], [1164], [1165], [1166], [1167], [1168], [1169], [1170], [1171], [1172], [1173], [1174], [1175], [1176], [1177], [1178], [1179], [1180], [1181], [1182], [1183], [1184], [1185], [1186], [1187], [1188], [1189], [1190], [1191], [1192], [1193], [1194], [1195], [1196], [1197], [1198], [1199], [1200], [1201], [1202], [1203], [1204], [1205], [1206], [1207], [1208], [1209], [1210], [1211], [1212], [1213], [1214], [1215], [1216], [1217], [1218], [1219], [1220], [1221], [1222], [1223], [1224], [1225], [1226], [1227], [1228], [1229], [1230], [1231], [1232], [1233], [1234], [1235], [1236], [1237], [1238], [1239], [1240], [1241], [1242], [1243], [1244], [1245], [1246], [1247], [1248], [1249], [1250], [1251], [1252], [1253], [1254], [1255], [1256], [1257], [1258], [1259], [1260], [1261], [1262], [1263], [1264], [1265], [1266], [1267], [1268], [1269], [1270], [1271], [1272], [1273], [1274], [1275], [1276], [1277], [1278], [1279], [1280], [1281], [1282], [1283], [1284], [1285], [1286], [1287], [1288], [1289], [1290], [1291], [1292], [1293], [1294], [1295], [1296], [1297], [1298], [1299], [1300], [1301], [1302], [1303], [1304], [1305], [1306], [1307], [1308], [1309], [1310], [1311], [1312], [1313], [1314], [1315], [1316], [1317], [1318], [1319], [1320], [1321], [1322], [1323], [1324], [1325], [1326], [1327], [1328], [1329], [1330], [1331], [1332], [1333], [1334], [1335], [1336], [1337], [1338], [1339], [1340], [1341], [1342], [1343], [1344], [1345], [1346], [1347], [1348], [1349], [1350], [1351], [1352], [1353], [1354], [1355], [1356], [1357], [1358], [1359], [1360], [1361], [1362], [1363], [1364], [1365], [1366], [1367], [1368], [1369], [1370], [1371], [1372], [1373], [1374], [1375], [1376], [1377], [1378], [1379], [1380], [1381], [1382], [1383], [1384], [1385], [1386], [1387], [1388], [1389], [1390], [1391], [1392], [1393], [1394], [1395], [1396], [1397], [1398], [1399], [1400], [1401], [1402], [1403], [1404], [1405], [1406], [1407], [1408], [1409], [1410], [1411], [1412], [1413], [1414], [1415], [1416], [1417], [1418], [1419], [1420], [1421], [1422], [1423], [1424], [1425], [1426], [1427], [1428], [1429], [1430], [1431], [1432], [1433], [1434], [1435], [1436], [1437], [1438], [1439], [1440], [1441], [1442], [1443], [1444], [1445], [1446], [1447], [1448], [1449], [1450], [1451], [1452], [1453], [1454], [1455], [1456], [1457], [1458], [1459], [1460], [1461], [1462], [1463], [1464], [1465], [1466], [1467], [1468], [1469], [1470], [1471], [1472], [1473], [1474], [1475], [1476], [1477], [1478], [1479], [1480], [1481], [1482], [1483], [1484], [1485], [1486], [1487], [1488], [1489], [1490], [1491], [1492], [1493], [1494], [1495], [1496], [1497], [1498], [1499], [1500], [1501], [1502], [1503], [1504], [1505], [1506], [1507], [1508], [1509], [1510], [1511], [1512], [1513], [1514], [1515], [1516], [1517], [1518], [1519], [1520], [1521], [1522], [1523], [1524], [1525], [1526], [1527], [1528], [1529], [1530], [1531], [1532], [1533], [1534], [1535], [1536], [1537], [1538], [1539], [1540], [1541], [1542], [1543], [1544], [1545], [1546], [1547], [1548], [1549], [1550], [1551], [1552], [1553], [1554], [1555], [1556], [1557], [1558], [1559], [1560], [1561], [1562], [1563], [1564], [1565], [1566], [1567], [1568], [1569], [1570], [1571], [1572], [1573], [1574], [1575], [1576], [1577], [1578], [1579], [1580], [1581], [1582], [1583], [1584], [1585], [1586], [1587], [1588], [1589], [1600], [1601], [1602], [1603], [1604], [1605], [1606], [1607], [1608], [1609], [1610], [1611], [1612], [1613], [1614], [1615], [1616], [1617], [1618], [1619], [1620], [1621], [1622], [1623], [1624], [1625], [1626], [1627], [1628], [1629], [1630], [1631], [1632], [1633], [1634], [1635], [1636], [1637], [1638], [1639], [1640], [1641], [1642], [1643], [1644], [1645], [1646], [1647], [1648], [1649], [1650], [1651], [1652], [1653], [1654], [1655], [1656], [1657], [1658], [1659], [1660], [1661], [1662], [1663], [1664], [1665], [1666], [1667], [1668], [1669], [1670], [1671], [1672], [1673], [1674], [1675], [1676], [1677], [1678], [1679], [1680], [1681], [1682], [1683], [1684], [1685], [1686], [1687], [1688], [1689], [1690], [1691], [1692], [1693], [1694], [1695], [1696], [1697], [1698], [1699], [1700], [1701], [1702], [1703], [1704], [1705], [1706], [1707], [1708], [1709], [1710], [1711], [1712], [1713], [1714], [1715], [1716], [1717], [1718], [1719], [1720], [1721], [1722], [1723], [1724], [1725], [1726], [1727], [1728], [1729], [1730], [1731], [1732], [1733], [1734], [1735], [1736], [1737], [1738], [1739], [1740], [1741], [1742], [1743], [1744], [1745], [1746], [1747], [1748], [1749], [1750], [1751], [1752], [1753], [1754], [1755], [1756], [1757], [1758], [1759], [1760], [1761], [1762], [1763], [1764], [1765], [1766], [1767], [1768], [1769], [1770], [1771], [1772], [1773], [1774], [1775], [1776], [1777], [1778], [1779], [1780], [1781], [1782], [1783], [1784], [1785], [1786], [1787], [1788], [1789], [1790], [1791], [1792], [1793], [1794], [1795], [1796], [1797], [1798], [1799], [1800], [1801], [1802], [1803], [1804], [1805], [1806], [1807], [1808], [1809], [1810], [1811], [1812], [1813], [1814], [1815], [1816], [1817], [1818], [1819], [1820], [1821], [1822], [1823], [1824], [1825], [1826], [1827], [1828], [1829], [1830], [1831], [1832], [1833], [1834], [1835], [1836], [1837], [1838], [1839], [1840], [1841], [1842], [1843], [1844], [1845], [1846], [1847], [1848], [1849], [1850], [1851], [1852], [1853], [1854], [1855], [1856], [1857], [1858], [1859], [1860], [1861], [1862], [1863], [1864], [1865], [1866], [1867], [1868], [1869], [1870], [1871], [1872], [1873], [1874], [1875], [1876], [1877], [1878], [1879], [1880], [1881], [1882], [1883], [1884], [1885], [1886], [1887], [1888], [1889], [1890], [1891], [1892], [1893], [1894], [1895], [1896], [1897], [1898], [1899], [1900], [1901], [1902], [1903], [1904], [1905], [1906], [1907], [1908], [1909], [1910], [1911], [1912], [1913], [1914], [1915], [1916], [1917], [1918], [1919], [1920], [1921], [1922], [1923], [1924], [1925], [1926], [1927], [1928], [1929], [1930], [1931], [1932], [1933], [1934], [1935], [1936], [1937], [1938], [1939], [1940], [1941], [1942], [1943], [1944], [1945], [1946], [1947], [1948], [1949], [1950], [1951], [1952], [1953], [1954], [1955], [1956], [1957], [1958], [1959], [1960], [1961], [1962], [1963], [1964], [1965], [1966], [1967], [1968], [1969], [1970], [1971], [1972], [1973], [1974], [1975], [1976], [1977], [1978], [1979], [1980], [1981], [1982], [1983], [1984], [1985], [1986], [1987], [1988], [1989], [1990], [1991], [1992], [1993], [1994], [1995], [1996], [1997], [1998], [1999], [2000], [2001], [2002], [2003], [2004], [2005], [2006], [2007], [2008], [2009], [2010], [2011], [2012], [2013], [2014], [2015], [2016], [2017], [2018], [2019], [2020], [2021], [2022], [2023], [2024], [2025], [2026], [2027], [2028], [2029], [2030], [2031], [2032], [2033], [2034], [2035], [2036], [2037], [2038], [2039], [2040], [2041], [2042], [2043], [2044], [2045], [2046], [2047], [2048], [2049], [2050], [2051], [2052], [2053], [2054], [2055], [2056], [2057], [2058], [2059], [2060], [2061], [2062], [2063], [2064], [2065], [2066], [2067], [2068], [2069], [2070], [2071], [2072], [2073], [2074], [2075], [2076], [2077], [2078], [2079], [2080], [2081], [2082], [2083], [2084], [2085], [2086], [2087], [2088], [2089], [2090], [2091], [2092], [2093], [2094], [2095], [2096], [2097], [2098], [2099], [2010], [2011], [2012], [2013], [2014], [2015], [2016], [2017], [2018], [2019], [2020], [2021], [2022], [2023], [2024], [2025], [</td></tr></table>

<table><tr><td colspan="35"></td><td>Cognitive
n (Vision)</td><td>Face/Human/Animal). Natural Object, Man-made Object</td><td>NIRS data</td><td>[77]</td></tr><tr><td>EEG-NIRS</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>5.67CB+</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Cognitive
n (Vision)</td><td>Face/Human/Animal). Natural Object</td><td>NIRS data</td><td>[77]</td><td></td></tr><tr><td>760MB</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1000ns</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ENUSA</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HUHNICU</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>EEG-EEG</td><td></td><td></td><td></td><td></td></tr><tr><td>Image-EEG</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>[106]</td><td></td><td></td><td></td><td></td></tr><tr><td>ISRUC-SLEEP</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>[137]</td><td></td><td></td><td></td><td></td></tr><tr><td>Kaggle (AES)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>[136]</td><td></td><td></td><td></td><td></td></tr><tr><td>Kernakon</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>[128]</td><td></td><td></td><td></td><td></td></tr><tr><td>69.9 GB</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>[134]</td><td></td><td></td><td></td><td></td></tr><tr><td>Locitize-MI</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>[145]</td><td></td><td></td><td></td><td></td></tr><tr><td>MAVLO</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MAHNOB-HCI</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MAHNOB-HCI</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>[146]</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MDMA</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt;ecel&gt;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>

<table><tr><td colspan="101"></td><td colspan="34">18,63,151,159,1166</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="225">OpenBMI</td><td colspan="56">15,151,159,1159</td></tr></table>

e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e

ee e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e ee  

<table><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>TUSL</td><td>20.59 hrs.</td><td>Manual</td><td>38</td><td>23</td><td>256Hz</td><td>BD (SD)</td><td>Background/Seizure/Slowing</td><td>[64]</td></tr><tr><td>TUH EEG Corpus</td><td>1.8</td><td>Unannotated</td><td>15001</td><td>23-41</td><td>250/256/400/512Hz</td><td>BD</td><td>BD</td><td>[45], [79]</td></tr><tr><td>TUSZ</td><td>504 hrs.</td><td>Hybrid</td><td>315</td><td>23</td><td>256Hz</td><td>MI</td><td>Different Seizure Types</td><td>-</td></tr><tr><td>Weibo</td><td>4.0 GB</td><td>Automatic (Expe runtime)</td><td>10</td><td>64</td><td>1000Hz</td><td>MI</td><td>LH / RH / Feet</td><td>-</td></tr><tr><td>ZuCg/ZuCg 2.0</td><td>61.7 GB</td><td>Hybrid</td><td>12/18</td><td>128</td><td>500Hz</td><td>EEG/CTX</td><td>Text Sentiment Label (Positive/Neutral/Negative), Semantic Relations (Score), Text Relation Labels (Yes/No)</td><td>Text Eye Tracking Data</td></tr></table>

TABLE C-I DIFFERENT FEATURE EXTRACTED ON EEG BY TRANSFORMER MODELS  

<table><tr><td colspan="2">Methods</td><td>Temporal</td><td>Spatial</td><td>Spectral</td><td>Research</td></tr><tr><td rowspan="7">Backbone Models</td><td rowspan="3">Transformer</td><td>✓</td><td>×</td><td>×</td><td>[20], [21]</td></tr><tr><td>✓</td><td>✓</td><td>×</td><td>[13], [15], [16], [23], [14], [22]</td></tr><tr><td>✓</td><td>×</td><td>✓</td><td>[17], [18], [19]</td></tr><tr><td rowspan="4">Basic VIT</td><td>✓</td><td>×</td><td>×</td><td>[31]</td></tr><tr><td>×</td><td>✓</td><td>×</td><td>[28], [24]</td></tr><tr><td>×</td><td>×</td><td>✓</td><td>[25]</td></tr><tr><td>✓</td><td>×</td><td>✓</td><td>[26], [31], [32], [33], [35]</td></tr><tr><td rowspan="15">Hybrid Transformer</td><td rowspan="4">CNN-Transformer</td><td>✓</td><td>×</td><td>×</td><td>[7], [15], [37], [38], [39], [46], [61], [15]</td></tr><tr><td>×</td><td>×</td><td>✓</td><td>[68]</td></tr><tr><td>✓</td><td>✓</td><td>×</td><td>[15], [41], [54], [66], [67], [73], [74], [86], [75], [82], [84], [87], [93]</td></tr><tr><td>✓</td><td>×</td><td>✓</td><td>[27], [43], [47], [50], [51], [69], [71], [97], [40], [42], [59], [60], [72], [76], [85], [91], [94], [95]</td></tr><tr><td rowspan="3">GAN-Transformer</td><td>✓</td><td>×</td><td>×</td><td>[100]</td></tr><tr><td>✓</td><td>✓</td><td>×</td><td>[99], [102]</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>[101]</td></tr><tr><td rowspan="2">Diffusion Transformer</td><td>✓</td><td>×</td><td>✓</td><td>[106]</td></tr><tr><td>✓</td><td>✓</td><td>×</td><td>[104], [105], [107]</td></tr><tr><td rowspan="2">GNN-Transformer</td><td>✓</td><td>✓</td><td>×</td><td>[112]</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>[108], [109], [110], [111], [113]</td></tr><tr><td rowspan="2">RNN/LSTM-Transformer</td><td>✓</td><td>✓</td><td>×</td><td>[116]</td></tr><tr><td>✓</td><td>×</td><td>×</td><td>[117], [118]</td></tr><tr><td>SNN-Transformer</td><td>✓</td><td>✓</td><td>×</td><td>[122]</td></tr><tr><td>CapsNet-Transformer</td><td>✓</td><td>✓</td><td>✓</td><td>[123], [124]</td></tr><tr><td rowspan="15">Customized Transformer</td><td rowspan="3">Multi-Encoder</td><td>✓</td><td>×</td><td>×</td><td>[134], [135], [136], [138], [139], [143]</td></tr><tr><td>✓</td><td>×</td><td>✓</td><td>[128], [140]</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>[126], [143]</td></tr><tr><td rowspan="5">Modified Encoders</td><td>✓</td><td>×</td><td>×</td><td>[127], [146], [156], [166], [167]</td></tr><tr><td>✓</td><td>✓</td><td>×</td><td>[78], [89], [147], [149], [151], [154], [158], [159], [160]</td></tr><tr><td>✓</td><td>×</td><td>✓</td><td>[50], [153], [162], [164]</td></tr><tr><td>×</td><td>✓</td><td>✓</td><td>[165]</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>[57], [77], [94], [157]</td></tr><tr><td rowspan="4">Pyramid Transformer</td><td>✓</td><td>×</td><td>×</td><td>[163]</td></tr><tr><td>×</td><td>×</td><td>✓</td><td>[169]</td></tr><tr><td>✓</td><td>✓</td><td>×</td><td>[77]</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>[88], [152], [170], [172], [173]</td></tr><tr><td rowspan="3">Reconstruction</td><td>✓</td><td>×</td><td>×</td><td>[53], [177]</td></tr><tr><td>✓</td><td>✓</td><td>×</td><td>[58], [156], [156], [178]</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>[52], [70], [83], [96], [99], [113], [114], [175]</td></tr></table>

TABLE D-I COMPUTATIONAL COMPLEXITY BETWEEN VARIOUS RESEARCHES  

<table><tr><td>Research</td><td>Section</td><td>Categories</td><td>Parameters Count</td></tr><tr><td>EEGNet</td><td>-</td><td>CNN</td><td>1K</td></tr><tr><td>DeepConvNet</td><td>-</td><td>CNN</td><td>100K</td></tr><tr><td>AMDET [19]</td><td>Backbone</td><td>Transformer</td><td>300K</td></tr><tr><td>[27]</td><td>Backbone</td><td>Vision Transformer</td><td>13K</td></tr><tr><td>[30]</td><td>Backbone</td><td>Direct Vision Transformers</td><td>22M-87M</td></tr><tr><td>MOEA-TNAS [34]</td><td>Backbone</td><td>Vision Transformer</td><td>2.50M</td></tr><tr><td>[37]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>300K</td></tr><tr><td>Neuro2vec [40]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>798K</td></tr><tr><td>EEGFormer [41]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>50.6K</td></tr><tr><td>CNN-Former [49]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>1.7M</td></tr><tr><td>LaBraM [64]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>369M</td></tr><tr><td>FoME [65]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>745M</td></tr><tr><td>BSVT [66]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>750K</td></tr><tr><td>CIT-NET [68]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>1.20M</td></tr><tr><td>DICE-NET [69]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>357.6M</td></tr><tr><td>TransEEG [73]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>280K</td></tr><tr><td>Tsformer [87]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>32K</td></tr><tr><td>BiCCT [93]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>170K</td></tr><tr><td>LSleep [98]</td><td>Hybrid Models</td><td>CNN-Transformer</td><td>650K</td></tr><tr><td>ATM [107]</td><td>Hybrid Models</td><td>Diffusion-Transformer</td><td>3.18M</td></tr><tr><td>DANet [108]</td><td>Hybrid Models</td><td>GNN-Transformer</td><td>240K</td></tr><tr><td>C-M-S [113]</td><td>Hybrid Models</td><td>GNN-Transformer</td><td>318K</td></tr><tr><td>Spilking Conformer [121]</td><td>Hybrid Models</td><td>SNN-Transformer</td><td>9.9K / 40.3K</td></tr><tr><td>B2-ViT [144]</td><td>Customized Transformers</td><td>Multi-Encoders (TNT)</td><td>19.07M</td></tr><tr><td>EEGConvTransformer [89]</td><td>Customized Transformers</td><td>Modified Encoder</td><td>4.56M, 11.52M, 23.55M</td></tr><tr><td>MEET [155]</td><td>Customized Transformers</td><td>Modified Encoder</td><td>30M, 61M, 215M</td></tr><tr><td>SSVEPformer [165]</td><td>Customized Transformers</td><td>Modified Encoder</td><td>2.83M, 9.26M</td></tr><tr><td>Swin Transformer [173]</td><td>Customized Transformers</td><td>Pyramid Transformer (Swin Transformer)</td><td>13M</td></tr><tr><td>EEG-Deformer [77]</td><td>Customized Transformers</td><td>Pyramid Transformer (PVT)</td><td>1.98M, 542.93k, 2.05M</td></tr><tr><td>Scatterformer [169]</td><td>Customized Transformers</td><td>Pyramid Transformer (PVT)</td><td>42M</td></tr><tr><td>JDAT [170]</td><td>Customized Transformers</td><td>Pyramid Transformer (PVT)</td><td>1.32M</td></tr><tr><td>Dinoformer [53]</td><td>Customized Transformers</td><td>Reconstructed Transformer</td><td>8.06M</td></tr><tr><td>Tsformer-SA [174]</td><td>Customized Transformers</td><td>Reconstructed Transformer</td><td>4.86K</td></tr></table>