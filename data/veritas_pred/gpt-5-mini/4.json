{
  "question": "\nConsidering the proposed two-stage process of \"AI for Academic Survey,\" how does the evolution of methodologies in \"Related Work Retrieval\"\u2014from semantic and graph-guided approaches to sophisticated LLM-augmented and multi-agent systems\u2014directly enable and shape the increasing complexity and scope of \"Overview Report Generation,\" from initial research roadmap mapping to the autonomous creation of entire document-level surveys?\n",
  "research_plan": "<research_outline>\n- Research Objective and Immediate Plan\n  - Goal: Explain and map how the evolution of \"Related Work Retrieval\" methods (semantic \u2192 graph-guided \u2192 LLM-augmented \u2192 multi-agent) enables and shapes the increasing complexity and scope of \"Overview Report Generation\" (from roadmap mapping to autonomous document-level surveys).\n  - Step-by-step research plan (tools & team members explicitly assigned):\n    - Step A: Scope & Definitions (perform locally)\n      - Define working terms: \"Related Work Retrieval\", \"Overview Report Generation\", \"semantic retrieval\", \"graph-guided retrieval\", \"LLM-augmented retrieval\", \"multi-agent systems\", \"RAG\", \"provenance/verification\".\n      - Deliverable: short glossary and success criteria for later evaluation.\n    - Step B: Harvest primary sources (Tool: web_search_agent)\n      - Use the ORIGINAL TASK string as the agent \"task\" to run focused queries capturing (i) semantic retrieval systems (embeddings/vector search), (ii) graph/citation/knowledge-graph retrieval, (iii) LLM-augmented retrieval and RAG pipelines, (iv) multi-agent orchestration demos/papers.\n      - Expected output: 20\u201340 prioritized sources (papers, reviews, system demos, blogs, repos) with URL + 1-line metadata.\n      - Team member: web_search_agent.\n    - Step C: If gaps are found, deep harvest (Tool: deep_researcher_agent)\n      - Run exhaustive literature gathering (top 50\u2013100 items), harvest abstracts, system descriptions, demo repos, and dates.\n      - Team member: deep_researcher_agent.\n    - Step D: Mechanism-level analysis (Tool: deep_analyzer_agent)\n      - Map timeline/taxonomy of retrieval methods and extract causal links showing how each retrieval class increases generator capabilities.\n      - Produce annotated pipelines, component lists, and 3 representative system archetypes.\n      - Team member: deep_analyzer_agent.\n    - Step E: Draft and verify the research outline (local drafting + web_search_agent verification).\n      - Final verification queries (web_search_agent) for any missing demos or datasets.\n      - Final deliverable produced via final_answer_tool.\n    - Planning & tracking: planning_tool used to create/update/mark plan steps and progress.\n    - Final verification and human-in-the-loop checkpoint required before publication.\n\n- Core research questions to answer (guide literature selection)\n  - For each retrieval generation class: What are its core mechanisms and typical system components? What new inputs/artifacts does it produce (structured attributes, provenance graphs, ranked candidate lists)?\n  - How do those artifacts change what the Overview Generator can do? Which generator functions become possible or materially easier (coverage, taxonomy induction, timeline construction, section synthesis, cross-paper comparisons, automated tables/figures, living surveys)?\n  - What pipeline architectures map retrieval artifacts \u2192 generator tasks (minimal RAG pipeline, graph-augmented RAG, multi-agent orchestrator)?\n  - What verification and provenance mechanisms are required as generator scope grows, and how do retrieval advances interact with them?\n\n- High-level outline structure (what the research deliverable will contain)\n  1) Definitions, scope, and evaluation criteria\n     - Clear definitions and success metrics for retrieval and generation.\n     - Use cases: short related-work paragraphs, research roadmaps, full surveys, living surveys, policy briefs.\n  2) Historical / evolutionary narrative (class-by-class)\n     - Semantic retrieval (dense embeddings & vector search): mechanisms, primary advantages, immediate generation impacts, limitations.\n     - Graph-guided retrieval (citation, co-citation, knowledge and concept graphs): mechanisms, how structural knowledge enables lineage & taxonomy, limitations.\n     - LLM-augmented retrieval (RAG, query expansion, reranking, extraction): mechanisms, enabling of abstractive synthesis and structured extraction, reliance on grounding.\n     - Multi-agent orchestration (specialist agents: retriever, summarizer, critic, verifier, editor): mechanisms, iterative refinement loops, scalability to document-level automation.\n  3) Mechanism-level causal mapping: for each retrieval class list \"What it produces\" \u2192 \"How that enables generator features\" \u2192 \"New generator outputs unlocked\"\n     - Semantic retrieval: candidate pools \u2192 increased recall & topical coverage \u2192 broad scans, initial taxonomies, per-topic summaries.\n     - Graph-guided retrieval: structured topology \u2192 lineage/timelines & community detection \u2192 research roadmaps, seminal-work narratives, visual cluster outputs.\n     - LLM-augmented retrieval: context-aware reranking + structured extraction \u2192 per-paper TL;DRs, method/result tables, claim provenance \u2192 contrastive syntheses, meta-analyses, and improved coherence.\n     - Multi-agent systems: modular roles and iteration \u2192 document-level drafting, critique loops, automatic fact-checking and formatting \u2192 autonomous, multi-section surveys and living documents.\n  4) End-to-end pipeline architectures (detailed bullet lists + suggested diagrams)\n     - Minimal pipeline: seed queries \u2192 semantic retrieval \u2192 LLM summarizer \u2192 single-pass related-work paragraph.\n     - Mid-level pipeline: seed \u2192 semantic retrieval \u2192 graph expansion (citation neighbors) \u2192 rerank (LLM) \u2192 structured extraction (methods, datasets) \u2192 clustering/taxonomy \u2192 writer agent drafts sections \u2192 editor.\n     - Full multi-agent pipeline: seed \u2192 retriever agent(s) [semantic + graph] \u2192 extractor agents (claims, methods, metrics) \u2192 taxonomy & roadmap induction agent \u2192 writer agents (per-section) \u2192 critic/verifier agents (fact-check, provenance) \u2192 editor/formatter \u2192 output + assets + living update loop.\n     - Explicitly assign components to available tool analogues: vector DBs (semantic), graph DBs (citation), RAG stacks (LLM augmentation), orchestrator (multi-agent controller), verifier modules (chain-of-evidence trackers).\n  5) Component checklist & implementation notes (what to build or verify in systems)\n     - Metadata normalization and doc ID resolution (required for graphs).\n     - Embedding model selection and vector DB scalability.\n     - Citation-graph construction and lineage algorithms (temporal ordering, influence scoring).\n     - RAG design choices: chunking, context window management, reranking models, citation tagging.\n     - Agent roles and protocols: input/output schemas for retriever, extractor, writer, critic, verifier.\n     - Provenance recording: persistent pointers to source spans and evidence for every synthesized claim.\n  6) Evaluation framework (metrics & benchmarks)\n     - Retrieval metrics: recall@k (semantic vs graph expansion), precision/recall for claim extraction, coverage across subtopics.\n     - Generation metrics: factuality (fact-check pass rate), attribution correctness (citation precision), coherence/fluency (human rating), section coverage (topic recall), redundancy and contradiction rates.\n     - End-to-end utility metrics: time-to-coverage for a roadmap, human time saved, reproducibility of findings.\n     - Suggested human evaluation protocols and small-scale gold standards (annotated surveys, per-paper gold TL;DRs, claim\u2192source pairs).\n  7) Datasets, benchmarks & experimental designs to validate claims\n     - Candidate corpora: arXiv subsets by field, Semantic Scholar Open Corpus, CORD-19 (historical example), ACL/NeurIPS/ICLR corpus snapshots.\n     - Graph resources: OpenCitations, MAG snapshots, Semantic Scholar citation graph extracts.\n     - Benchmarks: create or adapt a document-level survey benchmark (seed topics, expected section taxonomies, gold bibliography, gold TL;DRs).\n     - Experiments: ablation studies that hold generator fixed while varying retrieval (keyword vs semantic vs semantic+graph vs semantic+graph+RAG vs multi-agent) and measure coverage, factuality, and time-to-complete.\n  8) Representative case studies and system archetypes to collect and analyze\n     - Papers/systems to seek: RAG papers and demos, systems that couple citation graphs with retrieval (citation-based recommenders), recent multi-agent survey or literature-assistant demos (agentic literature review tools), end-to-end demos that produce long documents.\n     - For each case study capture: architecture diagram, retrieval methods, generator design, provenance approach, evaluation results.\n  9) Risks, limitations, and ethical/legal considerations\n     - Hallucination and misattribution risks as generator scope grows.\n     - Copyright and summarizing paywalled content.\n     - Bias amplification when retrieval prioritizes certain venues or languages.\n     - Over-reliance on automated verification\u2014need for human checkpoints for publishable surveys.\n 10) Recommended roadmap of experiments and milestones (practical timeline)\n     - Phase 0 (week 0\u20131): define scope, prepare corpora, assemble baseline retrieval and generator (keyword baseline + simple summarizer).\n     - Phase 1 (week 2\u20134): implement semantic retrieval baseline (embeddings + vector DB); run coverage and topical clustering experiments.\n     - Phase 2 (week 5\u20138): integrate graph-guided expansion (citation neighbors), produce taxonomy and timeline visualizations; evaluate lineage reconstruction quality.\n     - Phase 3 (week 9\u201312): add LLM-augmented reranking + structured extraction (RAG); generate section drafts and per-paper TL;DRs; measure attribution accuracy.\n     - Phase 4 (week 13\u201316): build multi-agent orchestration prototype (retriever, extractor, writer, critic, verifier); run ablations and human evaluation for full survey generation.\n     - Deliverables at each phase: annotated bibliography, pipeline code snippets, evaluation reports, and example generated sections.\n 11) Prioritized reading list & focused search queries (to feed web_search_agent / deep_researcher_agent)\n     - Focused queries to run (exact search phrases):\n       - \"semantic search embeddings literature review dense retrieval academic surveys\"\n       - \"citation graph retrieval citation network literature mapping\" \n       - \"Retrieval-Augmented Generation RAG literature review systems\" \n       - \"multi-agent literature review system survey generator\" \n     - Prioritized targets: RAG foundational papers, OpenAI/GPT RAG demos, Semantic Scholar retrieval/citation papers, OpenCitations/MAG descriptions, agentic system demos (LangChain agent patterns, recent multi-agent publications).\n\n- Final deliverables & verification checklist\n  - Deliverable 1: This structured <research_outline> embedded in XML (current document).\n  - Deliverable 2: Annotated bibliography (URLs + 1-line notes) produced by web_search_agent / deep_researcher_agent.\n  - Deliverable 3: Mechanism-level analysis notes and representative pipeline diagrams (from deep_analyzer_agent).\n  - Verification checklist before publishing: coverage (\u226590% of prioritized sources), at least one concrete system case study for each retrieval class, demonstrable end-to-end ablation results showing progressive capability gains as retrieval evolves, and explicit provenance traces for generated claims.\n\n- Expected conclusions (hypotheses to confirm via research)\n  - Hypothesis 1: Each evolution in retrieval adds specific structured artifacts (candidate pools, topology, structured attributes, iterative agent outputs) that map directly to new generator capabilities; these artifacts are necessary and in practice sufficient to scale output complexity.\n  - Hypothesis 2: The transition from LLM-augmented retrieval to multi-agent orchestration is the inflection point where autonomous document-level survey generation becomes feasible in practice (given robust provenance and verification).\n  - Hypothesis 3: The dominant remaining bottlenecks are provenance fidelity, evaluation metrics for factuality at scale, and legal/ethical constraints on data access.\n\n</research_outline>",
  "references": [
    {
      "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
      "url": "https://arxiv.org/abs/2507.01903"
    },
    {
      "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
      "url": "https://arxiv.org/abs/2507.01903"
    }
  ]
}