import json
import os
import re
import time
from dotenv import load_dotenv
from openai import OpenAI


load_dotenv(override=True)
LLM_MODEL = os.getenv("llm_model")
LLM_API_KEY = os.getenv("llm_api_key")
LLM_BASE_URL = os.getenv("llm_base_url")
llm_client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

def call_llm_with_prompt(prompt, system_prompt=None):
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    max_retries = 3
    delay = 5
    for attempt in range(max_retries):
        try:
            response = llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=messages,
                stream=False,
                temperature=0.6,
                top_p=0.95
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"API è°ƒç”¨å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                print(f"å°†åœ¨ {delay} ç§’åŽé‡è¯•...")
                time.sleep(delay)
            else:
                print("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚")
                return f"<error>API call failed after {max_retries} attempts: {e}</error>"
            
def evaluate_plan_quality(ground_truth_plan, agent_plan):
    prompt_template = """
<role>
You are a meticulous academic reviewer, skilled at conducting detailed comparative analyses of research proposals.
</role>

<task>
You are given two research plans on the same topic: "Plan A" and "Plan B". Your core task is to conduct a **point-by-point** analysis to determine if 'Plan B' comprehensively covers all the research points of 'Plan A'. "Plan A" is the ground truth, and "Plan B" is the plan generated by an AI agent that is being evaluated.
</task>

<requirements>
1.  **Overall Conclusion:** Begin with an overall conclusion about the coverage (e.g., fully covered, partially covered, not covered, or covered from a different perspective).
2.  **Point-by-Point Analysis:** For each point in "Plan A", identify the corresponding content in "Plan B" and provide a rationale. If "Plan B" covers a point from a different angle or in greater depth, explain how. Specifically point out any omissions in Plan B.
3.  **Summary of Core Differences:** Conclude by summarizing the core differences between the two plans in terms of their perspective, focus, or methodology.
</requirements>

<output_format>
Please use the following structure for your response:

**I. Overall Conclusion**
[State your overall judgment here]

**II. Point-by-Point Comparative Analysis**
* **Regarding Point (1) of Plan A:**
    * **Coverage Status:** [e.g., Fully Covered / Partially Covered / Not Covered]
    * **Rationale and Analysis:** [Explain in detail which parts of Plan B correspond to this point and describe the manner and extent of the coverage.]
* **Regarding Point (2) of Plan A:**
    * **Coverage Status:** [...]
    * **Rationale and Analysis:** [...]
* (Continue for every point in Plan A)

**III. Summary of Core Differences**
[Summarize the fundamental differences between the two plans here]
</output_format>

---
**[Plan A: Ground Truth]**
{plan_a}

---
**[Plan B: Generated by DeepResearchAgent]**
{plan_b}
"""
    prompt = prompt_template.format(plan_a=ground_truth_plan, plan_b=agent_plan)
    return call_llm_with_prompt(prompt)

def analyze_coverage_from_report(report_text, ground_truth_plan):    
    coverage_counts = {
        "fully covered": 0,
        "partially covered": 0,
        "not covered": 0,
        "unknown": 0
    }
    
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¥æŸ¥æ‰¾ "Coverage Status: [Status]"æˆ–è€… "**Coverage Status:** [Status]"
    pattern = re.compile(r"(\*\*?Coverage Status:\*\*?\s*)(.*)", re.IGNORECASE)

    # æŸ¥æ‰¾æŠ¥å‘Šä¸­æ‰€æœ‰çš„çŠ¶æ€
    found_statuses = pattern.findall(report_text)
    
    for status in found_statuses:
        if len(status) == 2:
            clean_status = status[1].strip().lower()
        else:
            clean_status = status.strip().lower()
        print(f"Found coverage status: {clean_status}")
        if clean_status in coverage_counts:
            coverage_counts[clean_status] += 1
        elif "fully covered" in clean_status or "mostly covered" in clean_status or "largely covered" in clean_status:
            coverage_counts["fully covered"] += 1
        elif "partially covered" in clean_status:
            coverage_counts["partially covered"] += 1
        elif "not covered" in clean_status:
            coverage_counts["not covered"] += 1
        else:
            coverage_counts["unknown"] += 1
            
    # è®¡ç®—æ€»çš„è¯„ä¼°ç‚¹æ•°
    num_points = len(re.findall(r"^\(\d+\)", ground_truth_plan, re.MULTILINE))
    
    if num_points == 0:
        return None, 0

    print(f"âœ… Coverage analysis complete. Found {len(found_statuses)} items to analyze.")
    return coverage_counts, num_points

def display_coverage(title, counts, total_points):
    content = "="*90 + "\n"
    content += f"Quantitative Coverage Analysis ({title})\n"
    content += "="*90 + "\n"
    
    if total_points > 0:
        content += f"Total points evaluated: {total_points}\n"
        for status, count in counts.items():
            percentage = (count / total_points) * 100
            status_display = status.replace("_", " ").title().ljust(20)
            content += f"{status_display}: {count} / {total_points} ({percentage:.2f}%)\n"
        return content
    else:
        print("Could not determine the total number of points for analysis.")
        return ""


if __name__ == "__main__":
    src_dir = "data/PMC11835574/veritas"
    preds = [
        # "claude-3.7-sonnet",
        # "claude-4-sonnet",
        # "gemini-2.5-deep-research",
        # "gemini-2.5-flash",
        # "gemini-2.5-pro",
        "gpt-5",
        # "gpt-5-mini"
    ]

    for pred in preds:
        pred_dir = f"data/PMC11835574/veritas_pred/{pred}"
        eval_dir = f"data/PMC11835574/veritas_eval_deepseek_reasoner/{pred}"
        os.makedirs(eval_dir, exist_ok=True)

        total_stat = []
        coverage_counts = {
            "fully covered": 0,
            "partially covered": 0,
            "not covered": 0,
            "unknown": 0
        }
        total_points = 0
        costs = []
        with open(os.path.join(eval_dir, "evaluation_report.txt"), "a", encoding="utf-8") as f_eval:
            for filename in os.listdir(src_dir):
                if not filename.endswith(".json"):
                    continue
                if not os.path.exists(os.path.join(pred_dir, filename)):
                    continue

                with open(os.path.join(src_dir, filename), "r") as f:
                    data = json.load(f)
                    ground_truth_plan = data["research_plan"]
                with open(os.path.join(pred_dir, filename), "r") as f:
                    data = json.load(f)
                    agent_generated_plan = data["research_plan"]
                    token_usage = data.get("token_usage", {})
                    if "total_cost" in data.get("token_usage", {}):
                        costs.append(data["token_usage"]["total_cost"])

                print(f"ðŸš€ Starting evaluation of {filename} ({pred})...")
                eval_report = evaluate_plan_quality(ground_truth_plan, agent_generated_plan)
                print("ðŸ“ Evaluation report generated.")
                # print(eval_report)
                print(f"ðŸ” Analyzing coverage rates of {filename} ({pred})...")
                counts, points = analyze_coverage_from_report(eval_report, ground_truth_plan)
                coverage_content = display_coverage(filename, counts, points)
                print(coverage_content)
                stat = {
                    "filename": filename,
                    "counts": counts,
                    "total_points": points,
                    "evaluation_report": eval_report,
                    "token_usage": token_usage
                }
                total_stat.append(stat)

                for status, count in counts.items():
                    coverage_counts[status] += count
                total_points += points

                f_eval.write(f"\n\n{coverage_content}\n")
                f_eval.write(f"{eval_report}\n")

            with open(os.path.join(eval_dir, "coverage.json"), "w", encoding="utf-8") as f:
                json.dump(total_stat, f, ensure_ascii=False, indent=2)

            coverage_content = display_coverage(f"deep research agent powered by {pred}", coverage_counts, total_points)
            print(coverage_content)
            f_eval.write(f"\n\n{coverage_content}\n")
            if len(costs) > 0:
                avg_cost = sum(costs) / len(costs)
                cost_content = f"Average cost per instance: ${avg_cost:.4f}\n"
                print(cost_content)
                f_eval.write(cost_content)
