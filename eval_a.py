import json
import os

            
def evaluate_plan_quality(ground_truth_plan, agent_plan):
    prompt_template = """
<role>
You are a meticulous academic reviewer, skilled at conducting detailed comparative analyses of research proposals.
</role>

<task>
You are given two research plans on the same topic: "Plan A" and "Plan B". Your core task is to conduct a **point-by-point** analysis to determine if 'Plan B' comprehensively covers all the research points of 'Plan A'. "Plan A" is the ground truth, and "Plan B" is the plan generated by an AI agent that is being evaluated.
</task>

<requirements>
1.  **Overall Conclusion:** Begin with an overall conclusion about the coverage (e.g., fully covered, partially covered, not covered, or covered from a different perspective).
2.  **Point-by-Point Analysis:** For each point in "Plan A", identify the corresponding content in "Plan B" and provide a rationale. If "Plan B" covers a point from a different angle or in greater depth, explain how. Specifically point out any omissions in Plan B.
3.  **Summary of Core Differences:** Conclude by summarizing the core differences between the two plans in terms of their perspective, focus, or methodology.
</requirements>

<output_format>
Please use the following structure for your response:

**I. Overall Conclusion**
[State your overall judgment here]

**II. Point-by-Point Comparative Analysis**
* **Regarding Point (1) of Plan A:**
    * **Coverage Status:** [e.g., Fully Covered / Partially Covered / Not Covered]
    * **Rationale and Analysis:** [Explain in detail which parts of Plan B correspond to this point and describe the manner and extent of the coverage.]
* **Regarding Point (2) of Plan A:**
    * **Coverage Status:** [...]
    * **Rationale and Analysis:** [...]
* (Continue for every point in Plan A)

**III. Summary of Core Differences**
[Summarize the fundamental differences between the two plans here]
</output_format>

---
**[Plan A: Ground Truth]**
{plan_a}

---
**[Plan B: Generated by DeepResearchAgent]**
{plan_b}
"""
    return prompt_template.format(plan_a=ground_truth_plan, plan_b=agent_plan)

if __name__ == "__main__":
    src_dir = "data/veritas"
    tgt_dir = "data/veritas_pred/gemini-2.5-deep-research"


    for filename in os.listdir(src_dir):
        if not filename.endswith(".json"):
            continue
        if not os.path.exists(os.path.join(tgt_dir, filename)):
            continue

        with open(os.path.join(src_dir, filename), "r") as f:
            data = json.load(f)
            ground_truth_plan = data["research_plan"]
        with open(os.path.join(tgt_dir, filename), "r") as f:
            data = json.load(f)
            agent_generated_plan = data["research_plan"]

        print(f"ðŸš€ Starting evaluation of {filename}...")
        prompt = evaluate_plan_quality(ground_truth_plan, agent_generated_plan)
        with open(f"data/prompts/{filename}.prompt", "w") as f:
            f.write(prompt)
