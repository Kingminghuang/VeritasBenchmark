import json
import os
import re
import time
from dotenv import load_dotenv
from openai import OpenAI


load_dotenv(override=True)
LLM_MODEL = os.getenv("llm_model")
LLM_API_KEY = os.getenv("llm_api_key")
LLM_BASE_URL = os.getenv("llm_base_url")
llm_client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

def call_llm_with_prompt(prompt, system_prompt=None):
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    max_retries = 3
    delay = 5
    for attempt in range(max_retries):
        try:
            response = llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=messages,
                stream=False,
                temperature=0.6,
                top_p=0.95
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"API Ë∞ÉÁî®Â§±Ë¥•: {e}")
            if attempt < max_retries - 1:
                print(f"Â∞ÜÂú® {delay} ÁßíÂêéÈáçËØï...")
                time.sleep(delay)
            else:
                print("Â∑≤ËææÂà∞ÊúÄÂ§ßÈáçËØïÊ¨°Êï∞„ÄÇ")
                return f"<error>API call failed after {max_retries} attempts: {e}</error>"
            
def evaluate_plan_quality(ground_truth_plan, agent_plan):
    prompt_template = """
<role>
You are a meticulous academic reviewer, skilled at conducting detailed comparative analyses of research proposals.
</role>

<task>
You are given two research plans on the same topic: "Plan A" and "Plan B". Your core task is to conduct a **point-by-point** analysis to determine if 'Plan B' comprehensively covers all the research points of 'Plan A'. "Plan A" is the ground truth, and "Plan B" is the plan generated by an AI agent that is being evaluated.
</task>

<requirements>
1.  **Overall Conclusion:** Begin with an overall conclusion about the coverage (e.g., fully covered, partially covered, not covered, or covered from a different perspective).
2.  **Point-by-Point Analysis:** For each point in "Plan A", identify the corresponding content in "Plan B" and provide a rationale. If "Plan B" covers a point from a different angle or in greater depth, explain how. Specifically point out any omissions in Plan B.
3.  **Summary of Core Differences:** Conclude by summarizing the core differences between the two plans in terms of their perspective, focus, or methodology.
</requirements>

<output_format>
Please use the following structure for your response:

**I. Overall Conclusion**
[State your overall judgment here]

**II. Point-by-Point Comparative Analysis**
* **Regarding Point (1) of Plan A:**
    * **Coverage Status:** [e.g., Fully Covered / Partially Covered / Not Covered]
    * **Rationale and Analysis:** [Explain in detail which parts of Plan B correspond to this point and describe the manner and extent of the coverage.]
* **Regarding Point (2) of Plan A:**
    * **Coverage Status:** [...]
    * **Rationale and Analysis:** [...]
* (Continue for every point in Plan A)

**III. Summary of Core Differences**
[Summarize the fundamental differences between the two plans here]
</output_format>

---
**[Plan A: Ground Truth]**
{plan_a}

---
**[Plan B: Generated by DeepResearchAgent]**
{plan_b}
"""
    prompt = prompt_template.format(plan_a=ground_truth_plan, plan_b=agent_plan)
    return call_llm_with_prompt(prompt)

def analyze_coverage_from_report(report_text, ground_truth_plan):    
    coverage_counts = {
        "fully covered": 0,
        "partially covered": 0,
        "not covered": 0,
        "unknown": 0
    }
    
    # ÂÆö‰πâÊ≠£ÂàôË°®ËææÂºèÊù•Êü•Êâæ "Coverage Status: [Status]"
    pattern = re.compile(r"Coverage Status:\s*(.*)", re.IGNORECASE)
    
    # Êü•ÊâæÊä•Âëä‰∏≠ÊâÄÊúâÁöÑÁä∂ÊÄÅ
    found_statuses = pattern.findall(report_text)
    
    for status in found_statuses:
        clean_status = status.strip().lower()
        print(f"Found coverage status: {clean_status}")
        if clean_status in coverage_counts:
            coverage_counts[clean_status] += 1
        else:
            coverage_counts["unknown"] += 1
            
    # ËÆ°ÁÆóÊÄªÁöÑËØÑ‰º∞ÁÇπÊï∞
    num_points = len(re.findall(r"^\(\d+\)", ground_truth_plan, re.MULTILINE))
    
    if num_points == 0:
        return None, 0

    print(f"‚úÖ Coverage analysis complete. Found {len(found_statuses)} items to analyze.")
    return coverage_counts, num_points

def display_coverage(title, counts, total_points):
    print("\n" + "="*50)
    print(f"    Quantitative Coverage Analysis ({title})")
    print("="*50 + "\n")
    
    if total_points > 0:
        print(f"Total points evaluated: {total_points}\n")
        
        for status, count in counts.items():
            percentage = (count / total_points) * 100
            status_display = status.replace("_", " ").title().ljust(20)
            print(f"{status_display}: {count} / {total_points} ({percentage:.2f}%)")
    else:
        print("Could not determine the total number of points for analysis.")


if __name__ == "__main__":
    src_dir = "data/veritas"
    filename = "10.json"

    with open(os.path.join(src_dir, filename), "r") as f:
        data = json.load(f)
        ground_truth_plan = data["research_plan"]

    eval_report = """
I. Overall Conclusion
Plan B addresses some key themes of Plan A from a higher-level, architectural perspective; however, it only partially covers the detailed points and implementation specifics outlined in Plan A.

II. Point-by-Point Comparative Analysis

Regarding Point (1) of Plan A:

Coverage Status: Partially Covered
Rationale and Analysis: Plan A requires an investigation into seven distinct areas (interdisciplinary models, ethics and safety, collaborative research, explainability, real-time experimentation, and multimodal & multilingual integration). In contrast, Plan B‚Äôs opening focuses on defining a unified system emphasizing interdisciplinary, multimodal, and multilingual capabilities. While ethics is later touched upon in Point (2) of Plan B, the explicit focus on safety, collaborative research, and real-time experimentation is not clearly delineated.
Regarding Point (2) of Plan A:

Coverage Status: Not Covered
Rationale and Analysis: Point (2) demands a detailed survey of current mainstream techniques, including specific models and frameworks for each frontier (e.g., Foundation Models for interdisciplinary AI, Fairness-Aware Training for ethics, Collaborative AI Agents for collaborative research, etc.). Plan B does not provide a comparable listing or discussion of these existing techniques and frameworks.
Regarding Point (3) of Plan A:

Coverage Status: Not Covered
Rationale and Analysis: Plan A calls for a deep dive into the implementation mechanisms behind each technique‚Äîexploring data heterogeneity, debiasing methods, human-AI interaction protocols, and the intricacies of both white-box and black-box approaches. Plan B instead introduces challenges related to causal explainability but does not engage in a detailed exploration of the operational principles or mechanisms for the techniques mentioned in Plan A.
Regarding Point (4) of Plan A:

Coverage Status: Partially Covered
Rationale and Analysis: While Plan A requires comparisons of trade-offs (such as Performance vs. Fairness and Transparency vs. Performance), Plan B‚Äôs analysis of trade-offs is implicit in its discussion of causal explainability and system architectures. However, it does not explicitly address the range of trade-offs or comparisons (like data privacy vs. accessibility or capacity vs. coverage) that are highlighted in Plan A.
Regarding Point (5) of Plan A:

Coverage Status: Partially Covered
Rationale and Analysis: Point (5) in Plan A focuses on evaluating the effectiveness of current frameworks, including issues like negative transfer, plagiarism singularity, and low-latency decision-making in real-time experimentation. Plan B briefly touches on the risk of a "plagiarism singularity" and the limitations of a monolithic system regarding real-time responsiveness, but it does not comprehensively evaluate other critical challenges such as negative transfer or cross-modal data integration difficulties.
Regarding Point (6) of Plan A:

Coverage Status: Partially Covered
Rationale and Analysis: The synthesis in Plan A emphasizes future challenges, including the need for standardized metrics, high-quality datasets, robust human-AI interaction models, and reliable hardware-software integration. Plan B concludes with an architectural comparison and a synthesis of potential futures for scientific AI, but this synthesis is more focused on choosing between architectural models rather than addressing the broader research directions and challenges defined in Plan A.
III. Summary of Core Differences
Plan A is structured to provide a comprehensive, detailed mapping of both current research techniques and their underlying implementations across multiple targeted areas. In contrast, Plan B adopts a higher-level view that prioritizes architectural models and inherent trade-offs, offering a more conceptual discussion rather than an in-depth technical survey. This shift in focus leads Plan B to omit several specific details and evaluative criteria that are central to Plan A‚Äôs ground truth document.
"""
    print(f"üîç Analyzing coverage rates of {filename}...")
    counts, points = analyze_coverage_from_report(eval_report, ground_truth_plan)
    display_coverage(filename, counts, points)
    stat = {
        "filename": filename,
        "counts": counts,
        "total_points": points,
        "evaluation_report": eval_report.replace("\n", "\\n").replace("\"", "\\\"")
    }
    print(json.dumps(stat, indent=2, ensure_ascii=False))
